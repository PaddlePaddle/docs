{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3e772f-5f57-450f-9370-bdc80f6ef241",
   "metadata": {},
   "source": [
    "# 模型组网\n",
    "\n",
    "\n",
    "模型组网是深度学习任务中的重要一环，该环节定义了神经网络的层次结构、数据从输入到输出的计算过程（即前向计算）等。\n",
    "\n",
    "飞桨框架提供了多种模型组网方式，本文介绍如下几种常见用法：\n",
    "* **直接使用内置模型**\n",
    "* **使用 [paddle.nn.Sequential](../../api/paddle/nn/Sequential_cn.html#sequential) 组网**\n",
    "* **使用 [paddle.nn.Layer](../../api/paddle/nn/Layer_cn.html#layer) 组网**\n",
    "\n",
    "另外飞桨框架提供了 [paddle.summary](../../api/paddle/summary_cn.html#summary) 函数方便查看网络结构、每层的输入输出 shape 和参数信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615327e-4ac0-4616-a091-077046fb40f8",
   "metadata": {},
   "source": [
    "## 一、直接使用内置模型\n",
    "\n",
    "飞桨框架目前在 [paddle.vision.models](../../api/paddle/vision/Overview_cn.html#about-models) 下内置了计算机视觉领域的一些经典模型，只需一行代码即可完成网络构建和初始化，适合完成一些简单的深度学习任务，满足深度学习初阶用户感受模型的输入和输出形式、了解模型的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90c961d-6bd8-4d07-9a0f-44c25da7c023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-09T07:06:35.736296Z",
     "iopub.status.busy": "2022-02-09T07:06:35.735856Z",
     "iopub.status.idle": "2022-02-09T07:06:35.743307Z",
     "shell.execute_reply": "2022-02-09T07:06:35.742125Z",
     "shell.execute_reply.started": "2022-02-09T07:06:35.736251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飞桨框架内置模型： ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext50_64x4d', 'resnext101_32x4d', 'resnext101_64x4d', 'resnext152_32x4d', 'resnext152_64x4d', 'wide_resnet50_2', 'wide_resnet101_2', 'VGG', 'vgg11', 'vgg13', 'vgg16', 'vgg19', 'MobileNetV1', 'mobilenet_v1', 'MobileNetV2', 'mobilenet_v2', 'MobileNetV3Small', 'MobileNetV3Large', 'mobilenet_v3_small', 'mobilenet_v3_large', 'LeNet', 'DenseNet', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenet264', 'AlexNet', 'alexnet', 'InceptionV3', 'inception_v3', 'SqueezeNet', 'squeezenet1_0', 'squeezenet1_1', 'GoogLeNet', 'googlenet', 'ShuffleNetV2', 'shufflenet_v2_x0_25', 'shufflenet_v2_x0_33', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0', 'shufflenet_v2_x1_5', 'shufflenet_v2_x2_0', 'shufflenet_v2_swish']\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "\n",
    "print('飞桨框架内置模型：', paddle.vision.models.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79693587-4896-463d-be11-36cc939748d6",
   "metadata": {},
   "source": [
    "以 LeNet 模型为例，可通过如下代码组网："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "883f5395-3b1c-4d58-a70e-1dd7886a7d36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-05T08:23:47.069913Z",
     "iopub.status.busy": "2022-01-05T08:23:47.068838Z",
     "iopub.status.idle": "2022-01-05T08:23:47.101420Z",
     "shell.execute_reply": "2022-01-05T08:23:47.100556Z",
     "shell.execute_reply.started": "2022-01-05T08:23:47.069844Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-1       [[1, 1, 28, 28]]      [1, 6, 28, 28]          60       \n",
      "    ReLU-1        [[1, 6, 28, 28]]      [1, 6, 28, 28]           0       \n",
      "  MaxPool2D-1     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0       \n",
      "   Conv2D-2       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416     \n",
      "    ReLU-2       [[1, 16, 10, 10]]     [1, 16, 10, 10]           0       \n",
      "  MaxPool2D-2    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0       \n",
      "   Linear-1          [[1, 400]]            [1, 120]           48,120     \n",
      "   Linear-2          [[1, 120]]            [1, 84]            10,164     \n",
      "   Linear-3          [[1, 84]]             [1, 10]              850      \n",
      "===========================================================================\n",
      "Total params: 61,610\n",
      "Trainable params: 61,610\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 61610, 'trainable_params': 61610}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型组网并初始化网络\n",
    "lenet = paddle.vision.models.LeNet(num_classes=10)\n",
    "\n",
    "# 可视化模型组网结构和参数\n",
    "paddle.summary(lenet,(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f8ac6",
   "metadata": {},
   "source": [
    "通过 [paddle.summary](../../api/paddle/summary_cn.html#summary) 可清晰地查看神经网络层次结构、每一层的输入数据和输出数据的形状（Shape）、模型的参数量（Params）等信息，方便可视化地了解模型结构、分析数据计算和传递过程。从以上结果可以看出，LeNet 模型包含 2个`Conv2D` 卷积层、2个`ReLU` 激活层、2个`MaxPool2D` 池化层以及3个`Linear` 全连接层，这些层通过堆叠形成了 LeNet 模型，对应网络结构如下图所示。\n",
    "\n",
    "![](images/lenet.png)\n",
    "\n",
    "<div align=\"center\"><b> 图 1：LeNet 网络结构示意图 </b></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5fda8b-b153-49ee-b168-30f5293730ae",
   "metadata": {},
   "source": [
    "## 二、Paddle.nn 介绍\n",
    "\n",
    "经典模型可以满足一些简单深度学习任务的需求，然后更多情况下，需要使用深度学习框架构建一个自己的神经网络，这时可以使用飞桨框架 [paddle.nn](../../api/paddle/nn/Overview_cn.html) 下的 API 构建网络，该目录下定义了丰富的神经网络层和相关函数 API，如卷积网络相关的 Conv1D、Conv2D、Conv3D，循环神经网络相关的 RNN、LSTM、GRU 等，方便组网调用，详细清单可在 [API 文档](../../api/paddle/nn/Overview_cn.html) 中查看。\n",
    "\n",
    "飞桨提供继承类（class）的方式构建网络，并提供了几个基类，如：[paddle.nn.Sequential](../../api/paddle/nn/Sequential_cn.html#sequential)、 \n",
    "[paddle.nn.Layer](../../api/paddle/nn/Layer_cn.html#layer) 等，构建一个继承基类的子类，并在子类中添加层（layer，如卷积层、全连接层等）可实现网络的构建，不同基类对应不同的组网方式，本节介绍如下两种常用方法：\n",
    " \n",
    "* **使用 [paddle.nn.Sequential](../../api/paddle/nn/Sequential_cn.html#sequential) 组网**：构建顺序的线性网络结构（如 LeNet、AlexNet 和 VGG）时，可以选择该方式。相比于 Layer 方式 ，Sequential 方式可以用更少的代码完成线性网络的构建。\n",
    "* **使用 [paddle.nn.Layer](../../api/paddle/nn/Layer_cn.html#layer) 组网（推荐）**：构建一些比较复杂的网络结构时，可以选择该方式。相比于 Sequential 方式，Layer 方式可以更灵活地组建各种网络结构。Sequential 方式搭建的网络也可以作为子网加入 Layer 方式的组网中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab2efd1-2623-4678-809e-9264f14d9c7c",
   "metadata": {},
   "source": [
    "\n",
    "## 三、使用 paddle.nn.Sequential 组网\n",
    "\n",
    "\n",
    "构建顺序的线性网络结构时，可以选择该方式，只需要按模型的结构顺序，一层一层加到 [paddle.nn.Sequential](../../api/paddle/nn/Sequential_cn.html#sequential) 子类中即可。\n",
    "\n",
    "参照前面图 1 所示的 LeNet 模型结构，构建该网络结构的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a86cc3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-09T07:06:41.002891Z",
     "iopub.status.busy": "2022-02-09T07:06:41.002485Z",
     "iopub.status.idle": "2022-02-09T07:06:41.045284Z",
     "shell.execute_reply": "2022-02-09T07:06:41.044712Z",
     "shell.execute_reply.started": "2022-02-09T07:06:41.002860Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-3       [[1, 1, 28, 28]]      [1, 6, 28, 28]          60       \n",
      "    ReLU-3        [[1, 6, 28, 28]]      [1, 6, 28, 28]           0       \n",
      "  MaxPool2D-3     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0       \n",
      "   Conv2D-4       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416     \n",
      "    ReLU-4       [[1, 16, 10, 10]]     [1, 16, 10, 10]           0       \n",
      "  MaxPool2D-4    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0       \n",
      "   Flatten-1      [[1, 16, 5, 5]]          [1, 400]              0       \n",
      "   Linear-4          [[1, 400]]            [1, 120]           48,120     \n",
      "   Linear-5          [[1, 120]]            [1, 84]            10,164     \n",
      "   Linear-6          [[1, 84]]             [1, 10]              850      \n",
      "===========================================================================\n",
      "Total params: 61,610\n",
      "Trainable params: 61,610\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 61610, 'trainable_params': 61610}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddle import nn\n",
    "\n",
    "# 使用 paddle.nn.Sequential 构建 LeNet 模型\n",
    "lenet_Sequential = nn.Sequential(\n",
    "    nn.Conv2D(1, 6, 3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2D(2, 2),\n",
    "    nn.Conv2D(6, 16, 5, stride=1, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2D(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(400, 120),\n",
    "    nn.Linear(120, 84), \n",
    "    nn.Linear(84, 10)\n",
    ")\n",
    "# 可视化模型组网结构和参数\n",
    "paddle.summary(lenet_Sequential,(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd4c4c-9ff4-434e-b06c-32daf8e1ae43",
   "metadata": {},
   "source": [
    "使用 Sequential 组网时，会自动按照层次堆叠顺序完成网络的前向计算过程，简略了定义前向计算函数的代码。由于 Sequential 组网只能完成简单的线性结构模型，所以对于需要进行分支判断的模型需要使用 paddle.nn.Layer 组网方式实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9524d1c",
   "metadata": {},
   "source": [
    "## 四、使用 paddle.nn.Layer 组网\n",
    "\n",
    "构建一些比较复杂的网络结构时，可以选择该方式，组网包括三个步骤：\n",
    "1. 创建一个继承自 [paddle.nn.Layer](../../api/paddle/nn/Layer_cn.html#layer) 的类；\n",
    "1. 在类的构造函数 `__init__` 中定义组网用到的神经网络层（layer）；\n",
    "1. 在类的前向计算函数 `forward` 中使用定义好的 layer 执行前向计算。\n",
    "\n",
    "仍然以 LeNet 模型为例，使用 paddle.nn.Layer 组网的代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf89df53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-09T07:06:44.908285Z",
     "iopub.status.busy": "2022-02-09T07:06:44.907738Z",
     "iopub.status.idle": "2022-02-09T07:06:44.926711Z",
     "shell.execute_reply": "2022-02-09T07:06:44.925815Z",
     "shell.execute_reply.started": "2022-02-09T07:06:44.908250Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-5       [[1, 1, 28, 28]]      [1, 6, 28, 28]          60       \n",
      "    ReLU-5        [[1, 6, 28, 28]]      [1, 6, 28, 28]           0       \n",
      "  MaxPool2D-5     [[1, 6, 28, 28]]      [1, 6, 14, 14]           0       \n",
      "   Conv2D-6       [[1, 6, 14, 14]]     [1, 16, 10, 10]         2,416     \n",
      "    ReLU-6       [[1, 16, 10, 10]]     [1, 16, 10, 10]           0       \n",
      "  MaxPool2D-6    [[1, 16, 10, 10]]      [1, 16, 5, 5]            0       \n",
      "   Linear-7          [[1, 400]]            [1, 120]           48,120     \n",
      "   Linear-8          [[1, 120]]            [1, 84]            10,164     \n",
      "   Linear-9          [[1, 84]]             [1, 10]              850      \n",
      "===========================================================================\n",
      "Total params: 61,610\n",
      "Trainable params: 61,610\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "{'total_params': 61610, 'trainable_params': 61610}\n"
     ]
    }
   ],
   "source": [
    "# 使用 Subclass 方式构建 LeNet 模型\n",
    "class LeNet(nn.Layer):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 构建 features 子网，用于对输入图像进行特征提取\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2D(\n",
    "                1, 6, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(2, 2),\n",
    "            nn.Conv2D(\n",
    "                6, 16, 5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2D(2, 2))\n",
    "        # 构建 linear 子网，用于分类\n",
    "        if num_classes > 0:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(400, 120),\n",
    "                nn.Linear(120, 84), \n",
    "                nn.Linear(84, num_classes)\n",
    "            )\n",
    "    # 执行前向计算\n",
    "    def forward(self, inputs):\n",
    "        x = self.features(inputs)\n",
    "\n",
    "        if self.num_classes > 0:\n",
    "            x = paddle.flatten(x, 1)\n",
    "            x = self.linear(x)\n",
    "        return x\n",
    "lenet_SubClass = LeNet()\n",
    "\n",
    "# 可视化模型组网结构和参数\n",
    "params_info = paddle.summary(lenet_SubClass,(1, 1, 28, 28))\n",
    "print(params_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59606321",
   "metadata": {},
   "source": [
    "在上面的代码中，将 LeNet 分为了 `features` 和 `linear` 两个子网，`features` 用于对输入图像进行特征提取，`linear` 用于输出十个数字的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55565c4-cc47-4654-98cf-bccc8bff1331",
   "metadata": {},
   "source": [
    "## 五、总结\n",
    "\n",
    "本节介绍了飞桨框架中模型组网的几种方式，并且以 LeNet 为例介绍了如何使用这几种组网方式实现，总结模型组网的方法和用到的关键 API 如下图所示。\n",
    "\n",
    "![](images/model.png)\n",
    "\n",
    "<div align=\"center\"><b> 图 2：模型组网方法</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f2617-b7c2-4b3c-9ee6-1dae6c04bd59",
   "metadata": {},
   "source": [
    "## 扩展：模型的层（Layer）\n",
    "\n",
    "模型组网中一个关键组成就是神经网络层，不同的神经网络层组合在一起，从输入的数据样本中习得数据内在规律，最终输出预测结果。每个层从前一层获得输入数据，然后输出结果作为下一层的输入，并且大多数层包含可调的参数，在反向传播梯度时更新参数。\n",
    "\n",
    "在飞桨框架中内置了丰富的神经网络层，用类（class）的方式表示，构建模型时可直接作为实例添加到子类中，只需设置一些必要的参数，并定义前向计算函数即可，反向传播和参数保存由框架自动完成。\n",
    "\n",
    "下面展开介绍几个常用的神经网络层。\n",
    "\n",
    "\n",
    "\n",
    "### Conv2D\n",
    "[Conv2D](../../api/paddle/nn/Conv2D_cn.html#conv2d) （二维卷积层）主要用于对输入的特征图进行卷积操作，广泛用于深度学习网络中。Conv2D 根据输入、卷积核、步长（stride）、填充（padding）、空洞大小（dilations）等参数计算输出特征层大小。输入和输出是 NCHW 或 NHWC 格式，其中 N 是 batchsize 大小，C 是通道数，H 是特征高度，W 是特征宽度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a076b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "x = paddle.uniform((2, 3, 8, 8), dtype='float32', min=-1., max=1.)\n",
    "\n",
    "conv = nn.Conv2D(3, 6, (3, 3), stride=2) # 卷积层输入通道数为3，输出通道数为6，卷积核尺寸为3*3，步长为2\n",
    "y = conv(x) # 输入数据x\n",
    "y = y.numpy()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63121da0",
   "metadata": {},
   "source": [
    "### MaxPool2D\n",
    "\n",
    "[MaxPool2D](../../api/paddle/nn/MaxPool2D_cn.html#maxpool2d) （二维最大池化层）主要用于缩小特征图大小，根据 `kernel_size` 参数指定的窗口大小，对窗口内特征图进行取最大值的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0cacd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "x = paddle.uniform((2, 3, 8, 8), dtype='float32', min=-1., max=1.)\n",
    "\n",
    "pool = nn.MaxPool2D(3, stride=2) # 池化核尺寸为3*3，步长为2\n",
    "y = pool(x) #输入数据x\n",
    "y = y.numpy()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15082b1f",
   "metadata": {},
   "source": [
    "### Linear\n",
    "\n",
    "[Linear](../../api/paddle/nn/Linear_cn.html#linear) （全连接层）中每个神经元与上一层的所有神经元相连，实现对前一层的线性组合和线性变换。在卷积神经网络分类任务中，输出分类结果之前，通常采用全连接层对特征进行处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bacc6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "x = paddle.uniform((2, 6), dtype='float32', min=-1., max=1.)\n",
    "linear = paddle.nn.Linear(6, 4)\n",
    "y = linear(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d6243",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "[ReLU](../../api/paddle/nn/ReLU_cn.html#relu) 是深度学习任务中常用的激活层，主要用于对输入进行非线性变换。ReLU 将输入中小于 0 的部分变为 0，大于 0 的部分保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7261f42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[3], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = paddle.to_tensor([-2., 0., 1.])\n",
    "relu = paddle.nn.ReLU()\n",
    "y = relu(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b24ab6-802e-4b72-a6cf-6d06b459fd93",
   "metadata": {},
   "source": [
    "## 扩展：模型的参数（Parameter）\n",
    "\n",
    "在飞桨框架中，可通过网络的 [parameters()](../../api/paddle/nn/Layer_cn.html#parameters-include-sublayers-true) 和 [named_parameters()](../../api/paddle/nn/Layer_cn.html#named-parameters-prefix-include-sublayers-true) 方法获取网络在训练期间优化的所有参数（权重 weight 和偏置 bias），通过这些方法可以实现对网络更加精细化的控制，如设置某些层的参数不更新。\n",
    "\n",
    "下面这段示例代码，通过 `named_parameters()` 获取了 LeNet 网络所有参数的名字和值，打印出了参数的名字（name）和形状（shape）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bd4185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-09T07:06:53.239755Z",
     "iopub.status.busy": "2022-02-09T07:06:53.239227Z",
     "iopub.status.idle": "2022-02-09T07:06:53.254632Z",
     "shell.execute_reply": "2022-02-09T07:06:53.253562Z",
     "shell.execute_reply.started": "2022-02-09T07:06:53.239722Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: features.0.weight | Size: [6, 1, 3, 3]\n",
      "Layer: features.0.bias | Size: [6]\n",
      "Layer: features.3.weight | Size: [16, 6, 5, 5]\n",
      "Layer: features.3.bias | Size: [16]\n",
      "Layer: fc.0.weight | Size: [400, 120]\n",
      "Layer: fc.0.bias | Size: [120]\n",
      "Layer: fc.1.weight | Size: [120, 84]\n",
      "Layer: fc.1.bias | Size: [84]\n",
      "Layer: fc.2.weight | Size: [84, 10]\n",
      "Layer: fc.2.bias | Size: [10]\n"
     ]
    }
   ],
   "source": [
    "for name, param in lenet.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
