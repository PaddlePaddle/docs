{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Automatic Mixed Precision Training\n",
    "\n",
    "In general, the datatype of training deep learning models is single-precision floating-point format(also called FP32). In 2018, Baidu and NVIDIA jointly published the paper: [MIXED PRECISION TRAINING](https://arxiv.org/pdf/1710.03740.pdf), which proposed mixed precision training. During the process of training, some operators use FP32 and other operators use half precision(also called FP16) in the same time. Its purpose is to speed up training, while compared with the FP32 training model, the same accuracy is maintained. This tutorial will introduce how to use automatic mixed precision training with PaddlePaddle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Half Precision (FP16)\n",
    "\n",
    "First introduce FP16. As shown in Figure 1, FP16 occupies 16 bits (two bytes in modern computers) of computer memory. In the IEEE 754-2008 standard, it is also named binary16. Compared with FP32 and double precision (also called FP64) commonly used, FP16 is more suitable for the usage in scenarios with low precision requirements.\n",
    "\n",
    "<figure align=\"center\">\n",
    "    <img src=\"https://paddleweb-static.bj.bcebos.com/images/fp16.png\" width=\"600\" alt='missing'/>\n",
    "    <figcaption><center>Figure 1. Half precision(FP16) and single precision(FP32)</center></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. FP16 Computing Power of NVIDIA GPU\n",
    "\n",
    "When the same hyperparameters are used, mixed precision training using FP16 and FP32 can achieve the same accuracy as that of pure single precision used, and can accelerate the training speed. It mainly attributes to the features that NVIDIA Volta and NVIDIA Turing use FP16 to calculate:\n",
    "- FP16 can reduce memory bandwidth and storage requirements by half, which allows researchers to use more complex models and larger batch sizes under the same hardware conditions.\n",
    "- FP16 can make full use of Tensor Cores technology provided by NVIDIA Volta and NVIDIA Turing. On the same GPU hardware, the computing throughput of Tensor Cores' FP16 is 8 times bigger than that of FP32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Automatic Mixed Precision Training with PaddlePaddle\n",
    "\n",
    "Using PaddlePaddle's API ``paddle.amp.auto_cast`` and ``paddle.amp.GradScaler`` can realize automatic mixed precision training (AMP), which can automatically choose FP16 or FP32 for different operators' calculation. After the AMP mode is turned on, the operator list calculated by FP16 and FP32 can be found in this [document](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/amp/Overview_cn.html). This is a specific example to understand how to use PaddlePaddle to achieve mixed precision training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1 Auxiliary Function\n",
    "First define the auxiliary function to calculate the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# start time\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    # get start time\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(msg):\n",
    "    # print message and total training time\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + msg)\n",
    "    print(\"total time = {:.3f} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2 A Simple Network\n",
    "\n",
    "Define a simple network to compare the training speed of common methods and mixed precision. The network is composed of three layers of ``Linear``. The first two layers of ``Linear`` are followed by the ``ReLU`` activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Layer):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, output_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(input_size, output_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Set the parameters of training. In order to effectively show the improvement of training speed by mixed precision training, please set the larger values of ``input_size`` and ``output_size``. And in order to use the ``Tensor Core`` provided by GPU, ``batch_size`` needs to be set as a multiple of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "input_size = 4096   # set to a larger value\n",
    "output_size = 4096  # set to a larger value\n",
    "batch_size = 512    # batch_size is a multiple of 8\n",
    "nums_batch = 50\n",
    "\n",
    "train_data = [paddle.randn((batch_size, input_size)) for _ in range(nums_batch)]\n",
    "labels = [paddle.randn((batch_size, output_size)) for _ in range(nums_batch)]\n",
    "\n",
    "mse = paddle.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Training with Default Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [1.24072289])\n",
      "\n",
      "Default time:\n",
      "total time = 2.935 sec\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet(input_size, output_size)  # define model\n",
    "\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())  # define optimizer\n",
    "\n",
    "start_timer() # get the start time of training\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    datas = zip(train_data, labels)\n",
    "    for i, (data, label) in enumerate(datas):\n",
    "\n",
    "        output = model(data)\n",
    "        loss = mse(output, label)\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "print(loss)\n",
    "end_timer_and_print(\"Default time:\") # print massage and total time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.4 Training with AMP\n",
    "\n",
    "Using automatic mixed precision training with PaddlePaddle requires four steps:\n",
    "\n",
    "- Step1: Define ``GradScaler``, which is used to scale the ``loss`` to avoid underflow\n",
    "- Step2: Use ``decorate``, to do nothing in level='O1' mode without using this api, and in level='O2' mode to convert network parameters from FP32 to FP16\n",
    "- Step3: Use ``auto_cast`` to create an AMP context, in which the input datatype(FP16 or FP32) of each oprator will be automatically determined\n",
    "- Step4: Use ``GradScaler`` defined in Step1 to complete the scaling of ``loss``, and use the scaled ``loss`` for backpropagation to complete the training\n",
    "\n",
    "In level=’O1‘ mode："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [1.24848151])\n",
      "\n",
      "AMP time in O1 mode:\n",
      "total time = 1.299 sec\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet(input_size, output_size)  # define model\n",
    "\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())  # define optimizer\n",
    "\n",
    "# Step1：define GradScaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "start_timer() # get start time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    datas = zip(train_data, labels)\n",
    "    for i, (data, label) in enumerate(datas):\n",
    "\n",
    "        # Step2：create AMP context environment\n",
    "        with paddle.amp.auto_cast():\n",
    "            output = model(data)\n",
    "            loss = mse(output, label)\n",
    "\n",
    "        # Step3：use GradScaler complete the loss scaling\n",
    "        scaled = scaler.scale(loss)\n",
    "        scaled.backward()\n",
    "\n",
    "        # update parameters\n",
    "        scaler.minimize(optimizer, scaled)\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "print(loss)\n",
    "end_timer_and_print(\"AMP time in O1 mode:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In level='O2' mode："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ParamBase copy_to func\n",
      "in ParamBase copy_to func\n",
      "in ParamBase copy_to func\n",
      "in ParamBase copy_to func\n",
      "in ParamBase copy_to func\n",
      "in ParamBase copy_to func\n",
      "Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [1.25075114])\n",
      "\n",
      "AMP time in O2 mode:\n",
      "total time = 0.888 sec\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet(input_size, output_size)  # define model\n",
    "\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())  # define optimizer\n",
    "\n",
    "# Step1：define GradScaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "# Step2：in level='O2' mode, convert network parameters from FP32 to FP16\n",
    "model, optimizer = paddle.amp.decorate(models=model, optimizers=optimizer, level='O2', master_weight=None, save_dtype=None)\n",
    "\n",
    "start_timer() # get start time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    datas = zip(train_data, labels)\n",
    "    for i, (data, label) in enumerate(datas):\n",
    "\n",
    "        # Step3：create AMP context environment\n",
    "        with paddle.amp.auto_cast(enable=True, custom_white_list=None, custom_black_list=None, level='O2'):\n",
    "            output = model(data)\n",
    "            loss = mse(output, label)\n",
    "\n",
    "        # Step4：use GradScaler complete the loss scaling\n",
    "        scaled = scaler.scale(loss)\n",
    "        scaled.backward()\n",
    "\n",
    "        # update parameters\n",
    "        scaler.minimize(optimizer, scaled)\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "print(loss)\n",
    "end_timer_and_print(\"AMP time in O2 mode:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Advanced Usage\n",
    "### 4.1 Gradient Accumulation\n",
    "\n",
    "Gradient accumulation means running a configured number of steps without updating the model variables. Until certain steps, use the accumulated gradients to update the variables.\n",
    "\n",
    "In automatic mixed precision training, gradient accumulation is also supported, and the usage is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False,\n",
      "       [1.25853443])\n",
      "\n",
      "AMP time:\n",
      "total time = 1.034 sec\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet(input_size, output_size)  # define model\n",
    "\n",
    "optimizer = paddle.optimizer.SGD(learning_rate=0.0001, parameters=model.parameters())  # define optimizer\n",
    "\n",
    "accumulate_batchs_num = 10 # the batch numbers of gradients accumulation\n",
    "\n",
    "# define GradScaler\n",
    "scaler = paddle.amp.GradScaler(init_loss_scaling=1024)\n",
    "\n",
    "start_timer() # get start time\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    datas = zip(train_data, labels)\n",
    "    for i, (data, label) in enumerate(datas):\n",
    "\n",
    "        # create AMP context environment\n",
    "        with paddle.amp.auto_cast():\n",
    "            output = model(data)\n",
    "            loss = mse(output, label)\n",
    "\n",
    "        # use GradScaler complete the loss scaling\n",
    "        scaled = scaler.scale(loss)\n",
    "        scaled.backward()\n",
    "\n",
    "        #  when the accumulated batch is accumulate_batchs_num, update the model parameters\n",
    "        if (i + 1) % accumulate_batchs_num == 0:\n",
    "\n",
    "            # update parameters\n",
    "            scaler.minimize(optimizer, scaled)\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "print(loss)\n",
    "end_timer_and_print(\"AMP time:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "As can be seen from the above example, using the automatic mixed precision training, in O1 mode the total time is about 1.299s, in O2 mode the total time is about 0.888s, while the ordinary training method takes 2.935s, and the training speed is increased by about 2.4 times in O1 mode and 2.4 times in O2 mode. For more examples of using mixed precision training, please refer to paddlepaddle's models: [paddlepaddle/models](https://github.com/PaddlePaddle/models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
