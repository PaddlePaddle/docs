{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 用飞桨框架2.0造一个会下五子棋的AI模型\n",
    "\n",
    "作者信息：yangguohao (https://github.com/yangguohao/)\n",
    "更新日期：2022 年 12 月 7 日\n",
    "\n",
    "## 1. 简要介绍\n",
    "\n",
    "### AlphaZero\n",
    "\n",
    "* 在 AlphaZero 之前还有过两代版本，分别为 AlphaGo 和 AlphaGoZero 。而 AlphaZero 是 ALphaGoZero 的升级版本。\n",
    "\n",
    "* 作为一个通用模型，AlphaZero 不只针对围棋，而是同时学习了三种棋类，日本将棋、国际象棋以及围棋。从零开始，经过短时间的训练，AlphaZero 完胜各个领域里的最强AI。包括国际象棋的Stockfish，将棋的Elmo，以及围棋的前辈AlphaGo Zero。其核心主要为 MCTS 和 策略价值深度网络。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/abb79e88765242ccb9df65f2cf4877fc83be041546bf47e293dcae1b98e7e8bb)\n",
    "\n",
    "\n",
    "### 本项目简介\n",
    "\n",
    "* 本项目是AlphaZero算法的一个实现（使用PaddlePaddle框架），用于玩简单的棋盘游戏Gomoku（也称为五子棋），使用纯粹的自我博弈的方式开始训练。\n",
    "\n",
    "* Gomoku游戏比围棋或象棋简单得多，因此我们可以专注于AlphaZero的训练，在一台PC机上几个小时内就可以获得一个让你不可忽视的AI模型。\n",
    "\n",
    "* 因为和围棋相比，五子棋的规则较为简单，落子空间也比较小，因此没有用到AlphaGo Zero中大量使用的残差网络，只使用了卷积层和全连接层。本项目的网路简单，无需使用大量的计算就可以进行运行训练。使用的 Paddle 版本为 2.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. 环境配置\n",
    "本教程基于 PaddlePaddle 2.4.0 编写，如果你的环境不是本版本，请先参考官网安装 PaddlePaddle 2.4.0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-03T10:11:28.495020Z",
     "iopub.status.busy": "2022-12-03T10:11:28.494728Z",
     "iopub.status.idle": "2022-12-03T10:11:31.531408Z",
     "shell.execute_reply": "2022-12-03T10:11:31.529933Z",
     "shell.execute_reply.started": "2022-12-03T10:11:28.494991Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\r\n",
      "Requirement already satisfied: pygame in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.1.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.1.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m22.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-03T10:11:31.535307Z",
     "iopub.status.busy": "2022-12-03T10:11:31.534833Z",
     "iopub.status.idle": "2022-12-03T10:11:33.761158Z",
     "shell.execute_reply": "2022-12-03T10:11:33.759200Z",
     "shell.execute_reply.started": "2022-12-03T10:11:31.535262Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.7.4)\r\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\r\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "\n",
    "import paddle.nn as nn \n",
    "import paddle.nn.functional as F\n",
    "\n",
    "import random\n",
    "import copy\n",
    "from operator import itemgetter\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "# 可视化等\n",
    "import pygame\n",
    "from pygame.locals import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 游戏环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 电脑字体的位置\n",
    "FONT_PATH = 'C:/Windows/Fonts/simkai.ttf'\n",
    "\n",
    "class Board(object):\n",
    "    \"\"\"棋盘游戏逻辑控制\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.width = int(kwargs.get('width', 15))  # 棋盘宽度\n",
    "        self.height = int(kwargs.get('height', 15))  # 棋盘高度\n",
    "        self.states = {}    # 棋盘状态为一个字典,键: 移动步数,值: 玩家的棋子类型\n",
    "        self.n_in_row = int(kwargs.get('n_in_row', 5))  # 5个棋子一条线则获胜\n",
    "        self.players = [1, 2]  # 玩家1,2\n",
    "\n",
    "    def init_board(self, start_player=0):\n",
    "        # 初始化棋盘\n",
    "\n",
    "        # 当前棋盘的宽高小于5时,抛出异常(因为是五子棋)\n",
    "        if self.width < self.n_in_row or self.height < self.n_in_row:\n",
    "            raise Exception('棋盘的长宽不能少于{}'.format(self.n_in_row))\n",
    "        self.current_player = self.players[start_player]  # 先手玩家\n",
    "        self.availables = list(range(self.width * self.height)) # 初始化可用的位置列表\n",
    "        self.states = {}  # 初始化棋盘状态\n",
    "        self.last_move = -1  # 初始化最后一次的移动位置\n",
    "\n",
    "    def move_to_location(self, move):\n",
    "        # 根据传入的移动步数返回位置(如:move=2,计算得到坐标为[0,2],即表示在棋盘上左上角横向第三格位置)\n",
    "        h = move // self.width\n",
    "        w = move % self.width\n",
    "        return [h, w]\n",
    "\n",
    "    def location_to_move(self, location):\n",
    "        # 根据传入的位置返回移动值\n",
    "        # 位置信息必须包含2个值[h,w]\n",
    "        if len(location) != 2:\n",
    "            return -1\n",
    "        h = location[0]\n",
    "        w = location[1]\n",
    "        move = h * self.width + w\n",
    "        # 超出棋盘的值不存在\n",
    "        if move not in range(self.width * self.height):\n",
    "            return -1\n",
    "        return move\n",
    "\n",
    "    def current_state(self):\n",
    "        \"\"\"\n",
    "        从当前玩家的角度返回棋盘状态。\n",
    "    状态形式：4 * 宽 * 高\n",
    "        \"\"\"\n",
    "        # 使用4个15x15的二值特征平面来描述当前的局面\n",
    "        # 前两个平面分别表示当前player的棋子位置和对手player的棋子位置，有棋子的位置是1，没棋子的位置是0\n",
    "        # 第三个平面表示对手player最近一步的落子位置，也就是整个平面只有一个位置是1，其余全部是0\n",
    "        # 第四个平面表示的是当前player是不是先手player，如果是先手player则整个平面全部为1，否则全部为0\n",
    "        square_state = np.zeros((4, self.width, self.height))\n",
    "        if self.states:\n",
    "            moves, players = np.array(list(zip(*self.states.items())))\n",
    "            move_curr = moves[players == self.current_player]   # 获取棋盘状态上属于当前玩家的所有移动值\n",
    "            move_oppo = moves[players != self.current_player]   # 获取棋盘状态上属于对方玩家的所有移动值\n",
    "            square_state[0][move_curr // self.width,            # 对第一个特征平面填充值(当前玩家)\n",
    "                            move_curr % self.height] = 1.0\n",
    "            square_state[1][move_oppo // self.width,            # 对第二个特征平面填充值(对方玩家)\n",
    "                            move_oppo % self.height] = 1.0\n",
    "            # 指出最后一个移动位置\n",
    "            square_state[2][self.last_move // self.width,       # 对第三个特征平面填充值(对手最近一次的落子位置)\n",
    "                            self.last_move % self.height] = 1.0\n",
    "        if len(self.states) % 2 == 0:   # 对第四个特征平面填充值,当前玩家是先手,则填充全1,否则为全0\n",
    "            square_state[3][:, :] = 1.0\n",
    "        # 将每个平面棋盘状态按行逆序转换(第一行换到最后一行,第二行换到倒数第二行..)\n",
    "        return square_state[:, ::-1, :]\n",
    "\n",
    "    def do_move(self, move):\n",
    "        # 根据移动的数据更新各参数\n",
    "        self.states[move] = self.current_player  # 将当前的参数存入棋盘状态中\n",
    "        self.availables.remove(move)  # 从可用的棋盘列表移除当前移动的位置\n",
    "        self.current_player = (\n",
    "            self.players[0] if self.current_player == self.players[1]\n",
    "            else self.players[1]\n",
    "        )  # 改变当前玩家\n",
    "        self.last_move = move  # 记录最后一次的移动位置\n",
    "\n",
    "    def has_a_winner(self):\n",
    "        # 是否产生赢家\n",
    "        width = self.width  # 棋盘宽度\n",
    "        height = self.height  # 棋盘高度\n",
    "        states = self.states  # 状态\n",
    "        n = self.n_in_row  # 获胜需要的棋子数量\n",
    "\n",
    "        # 当前棋盘上所有的落子位置\n",
    "        moved = list(set(range(width * height)) - set(self.availables))\n",
    "        if len(moved) < self.n_in_row + 2:\n",
    "            # 当前棋盘落子数在7个以上时会产生赢家,落子数低于7个时,直接返回没有赢家\n",
    "            return False, -1\n",
    "\n",
    "        # 遍历落子数\n",
    "        for m in moved:\n",
    "            h = m // width\n",
    "            w = m % width  # 获得棋子的坐标\n",
    "            player = states[m]  # 根据移动的点确认玩家\n",
    "\n",
    "            # 判断各种赢棋的情况\n",
    "            # 横向5个\n",
    "            if (w in range(width - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 纵向5个\n",
    "            if (h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 左上到右下斜向5个\n",
    "            if (w in range(width - n + 1) and h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 右上到左下斜向5个\n",
    "            if (w in range(n - 1, width) and h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "        # 当前都没有赢家,返回False\n",
    "        return False, -1\n",
    "\n",
    "    def game_end(self):\n",
    "        \"\"\"检查当前棋局是否结束\"\"\"\n",
    "        win, winner = self.has_a_winner()\n",
    "        if win:\n",
    "            return True, winner\n",
    "        elif not len(self.availables):\n",
    "            # 棋局布满,没有赢家\n",
    "            return True, -1\n",
    "        return False, -1\n",
    "\n",
    "    def get_current_player(self):\n",
    "        return self.current_player\n",
    "\n",
    "N = 9\n",
    "\n",
    "IMAGE_PATH = 'UI/'\n",
    "\n",
    "WIDTH = 540 # 棋盘图片宽\n",
    "HEIGHT = 540    # 棋盘图片高\n",
    "MARGIN = 22 # 图片上的棋盘边界有间隔\n",
    "# GRID = (WIDTH - 2 * MARGIN) / (N - 1)   # 设置每个格子的大小\n",
    "GRID = (270-22)/7      #设置每个格子的大小，从棋盘左上方开始下棋\n",
    "PIECE = 32  # 棋子的大小\n",
    "\n",
    "# 加上UI的布局的训练方式\n",
    "class Game_UI(object):\n",
    "    \"\"\"游戏控制区域\"\"\"\n",
    "\n",
    "    def __init__(self, board, is_shown, **kwargs):\n",
    "        self.board = board  # 加载棋盘控制类\n",
    "        self.is_shown = is_shown\n",
    "\n",
    "        # 初始化 pygame\n",
    "        pygame.init()\n",
    "\n",
    "        if is_shown != 0:\n",
    "            self.__screen = pygame.display.set_mode((WIDTH, HEIGHT), 0, 32)\n",
    "            pygame.display.set_caption('五子棋AI')\n",
    "\n",
    "            # UI 资源\n",
    "            self.__ui_chessboard = pygame.image.load(IMAGE_PATH + 'chessboard.jpg').convert()\n",
    "            self.__ui_piece_black = pygame.image.load(IMAGE_PATH + 'piece_black.png').convert_alpha()\n",
    "            self.__ui_piece_white = pygame.image.load(IMAGE_PATH + 'piece_white.png').convert_alpha()\n",
    "\n",
    "    # 将索引转换成坐标\n",
    "    def coordinate_transform_map2pixel(self, i, j):\n",
    "        # 从 逻辑坐标到 UI 上的绘制坐标的转换\n",
    "        return MARGIN + j * GRID - PIECE / 2, MARGIN + i * GRID - PIECE / 2\n",
    "\n",
    "    # 将坐标转换成索引\n",
    "    def coordinate_transform_pixel2map(self, x, y):\n",
    "        # 从 UI 上的绘制坐标到 逻辑坐标的转换\n",
    "        i, j = int(round((y - MARGIN + PIECE / 2) / GRID)), int(round((x - MARGIN + PIECE / 2) / GRID))\n",
    "        # 有MAGIN, 排除边缘位置导致 i,j 越界\n",
    "        if i < 0 or i >= N or j < 0 or j >= N:\n",
    "            return None, None\n",
    "        else:\n",
    "            return i, j\n",
    "\n",
    "    def draw_chess(self):\n",
    "        # 棋盘\n",
    "        self.__screen.blit(self.__ui_chessboard, (0, 0))\n",
    "        # 棋子\n",
    "        for i in range(0, N):\n",
    "            for j in range(0, N):\n",
    "                # 计算移动位置\n",
    "                loc = i * N + j\n",
    "                p = self.board.states.get(loc, -1)\n",
    "\n",
    "                player1, player2 = self.board.players\n",
    "\n",
    "                # 求出落子的坐标\n",
    "                x, y = self.coordinate_transform_map2pixel(i, j)\n",
    "\n",
    "                if p == player1:  # 玩家为1时,将该位置放入黑棋\n",
    "                    self.__screen.blit(self.__ui_piece_black, (x, y))\n",
    "                elif p == player2:  # 玩家为2时,将该位置放入白棋\n",
    "                    self.__screen.blit(self.__ui_piece_white, (x, y))\n",
    "                else:\n",
    "                    pass  # 当前位置无玩家落子时,跳过\n",
    "\n",
    "    def one_step(self):\n",
    "        i, j = None, None\n",
    "        # 鼠标点击\n",
    "        mouse_button = pygame.mouse.get_pressed()\n",
    "        # 左键\n",
    "        if mouse_button[0]:\n",
    "            x, y = pygame.mouse.get_pos()\n",
    "            i, j = self.coordinate_transform_pixel2map(x, y)\n",
    "\n",
    "        if not i is None and not j is None:\n",
    "            loc = i * N + j\n",
    "            p = self.board.states.get(loc, -1)\n",
    "\n",
    "            player1, player2 = self.board.players\n",
    "\n",
    "            if p == player1 or p == player2:\n",
    "                # 当前位置有棋子\n",
    "                return False\n",
    "            else:\n",
    "                cp = self.board.current_player\n",
    "\n",
    "                location = [i,j]\n",
    "                move = self.board.location_to_move(location)\n",
    "                self.board.do_move(move)\n",
    "\n",
    "                if self.is_shown:\n",
    "                    if cp == player1:\n",
    "                        self.__screen.blit(self.__ui_piece_black, (x, y))\n",
    "                    else:\n",
    "                        self.__screen.blit(self.__ui_piece_white, (x, y))\n",
    "\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def draw_result(self, result):\n",
    "        font = pygame.font.Font(FONT_PATH, 50)\n",
    "        tips = u\"本局结束:\"\n",
    "\n",
    "        player1, player2 = self.board.players\n",
    "\n",
    "        if result == player1:\n",
    "            tips = tips + u\"玩家1胜利\"\n",
    "        elif result == player2:\n",
    "            tips = tips + u\"玩家2胜利\"\n",
    "        else:\n",
    "            tips = tips + u\"平局\"\n",
    "        text = font.render(tips, True, (255, 0, 0))\n",
    "        self.__screen.blit(text, (WIDTH / 2 - 200, HEIGHT / 2 - 50))\n",
    "\n",
    "    # 使用鼠标对弈(player1传入为人类玩家,player2为MCTS机器人)\n",
    "    def start_play_mouse(self, player1, player2, start_player=0):\n",
    "        \"\"\"开始一局游戏\"\"\"\n",
    "        if start_player not in (0, 1):\n",
    "            # 如果玩家不在玩家1,玩家2之间,抛出异常\n",
    "            raise Exception('开始的玩家必须为0(玩家1)或1(玩家2)')\n",
    "        self.board.init_board(start_player)  # 初始化棋盘\n",
    "        p1, p2 = self.board.players  # 加载玩家1,玩家2\n",
    "        player1.set_player_ind(p1)  # 设置玩家1\n",
    "        player2.set_player_ind(p2)  # 设置玩家2\n",
    "        players = {p1: player1, p2: player2}\n",
    "\n",
    "        # 如果人类玩家不是先手\n",
    "        if start_player != 0:\n",
    "            current_player = self.board.current_player  # 获取当前玩家\n",
    "            player_in_turn = players[current_player]  # 当前玩家的信息\n",
    "            move = player_in_turn.get_action(self.board)  # 基于MCTS的AI下一步落子\n",
    "            self.board.do_move(move)  # 根据下一步落子的状态更新棋盘各参数\n",
    "\n",
    "        if self.is_shown:\n",
    "            # 绘制棋盘\n",
    "            self.draw_chess()\n",
    "            # 刷新\n",
    "            pygame.display.update()\n",
    "\n",
    "        flag = False\n",
    "        win = None\n",
    "\n",
    "        while True:\n",
    "            # 捕捉pygame事件\n",
    "            for event in pygame.event.get():\n",
    "                # 退出程序\n",
    "                if event.type == QUIT:\n",
    "                    pygame.quit()\n",
    "                    exit()\n",
    "                elif event.type == MOUSEBUTTONDOWN:\n",
    "                    # 成功着棋\n",
    "                    if self.one_step():\n",
    "                        end, winner = self.board.game_end()\n",
    "                    else:\n",
    "                        continue\n",
    "                    # 结束\n",
    "                    if end:\n",
    "                        flag = True\n",
    "                        win = winner\n",
    "                        break\n",
    "\n",
    "                    # 没有结束,则使用MCTS进行下一步落子\n",
    "                    current_player = self.board.current_player  # 获取当前玩家\n",
    "                    player_in_turn = players[current_player]  # 当前玩家的信息\n",
    "\n",
    "                    move = player_in_turn.get_action(self.board)  # 基于MCTS的AI下一步落子\n",
    "                    self.board.do_move(move)  # 根据下一步落子的状态更新棋盘各参数\n",
    "\n",
    "                    if self.is_shown:\n",
    "                        # 展示棋盘\n",
    "                        self.draw_chess()\n",
    "                        # 刷新\n",
    "                        pygame.display.update()\n",
    "\n",
    "                    # 判断当前棋局是否结束\n",
    "                    end, winner = self.board.game_end()\n",
    "                    # 结束\n",
    "                    if end:\n",
    "                        flag = True\n",
    "                        win = winner\n",
    "                        break\n",
    "\n",
    "            if flag and self.is_shown:\n",
    "                self.draw_result(win)\n",
    "                # 刷新\n",
    "                pygame.display.update()\n",
    "                break\n",
    "\n",
    "    def start_play(self, player1, player2, start_player=0):\n",
    "        \"\"\"开始一局游戏\"\"\"\n",
    "        if start_player not in (0, 1):\n",
    "            # 如果玩家不在玩家1,玩家2之间,抛出异常\n",
    "            raise Exception('开始的玩家必须为0(玩家1)或1(玩家2)')\n",
    "        self.board.init_board(start_player)  # 初始化棋盘\n",
    "        p1, p2 = self.board.players  # 加载玩家1,玩家2\n",
    "        player1.set_player_ind(p1)  # 设置玩家1\n",
    "        player2.set_player_ind(p2)  # 设置玩家2\n",
    "        players = {p1: player1, p2: player2}\n",
    "        if self.is_shown:\n",
    "            # 绘制棋盘\n",
    "            self.draw_chess()\n",
    "            # 刷新\n",
    "            pygame.display.update()\n",
    "\n",
    "        while True:\n",
    "            if self.is_shown:\n",
    "                # 捕捉pygame事件\n",
    "                for event in pygame.event.get():\n",
    "                    # 退出程序\n",
    "                    if event.type == QUIT:\n",
    "                        pygame.quit()\n",
    "                        exit()\n",
    "\n",
    "            current_player = self.board.current_player  # 获取当前玩家\n",
    "            player_in_turn = players[current_player]  # 当前玩家的信息\n",
    "            move = player_in_turn.get_action(self.board)  # 基于MCTS的AI下一步落子\n",
    "            self.board.do_move(move)  # 根据下一步落子的状态更新棋盘各参数\n",
    "            if self.is_shown:\n",
    "                # 展示棋盘\n",
    "                self.draw_chess()\n",
    "                # 刷新\n",
    "                pygame.display.update()\n",
    "\n",
    "            # 判断当前棋局是否结束\n",
    "            end, winner = self.board.game_end()\n",
    "            # 结束\n",
    "            if end:\n",
    "                win = winner\n",
    "                break\n",
    "        if self.is_shown:\n",
    "            self.draw_result(win)\n",
    "            # 刷新\n",
    "            pygame.display.update()\n",
    "        return win\n",
    "\n",
    "    def start_self_play(self, player, temp=1e-3):\n",
    "        \"\"\" 使用MCTS玩家开始自己玩游戏,重新使用搜索树并存储自己玩游戏的数据\n",
    "        (state, mcts_probs, z) 提供训练\n",
    "        \"\"\"\n",
    "        self.board.init_board()  # 初始化棋盘\n",
    "        states, mcts_probs, current_players = [], [], []  # 状态,mcts的行为概率,当前玩家\n",
    "\n",
    "        if self.is_shown:\n",
    "            # 绘制棋盘\n",
    "            self.draw_chess()\n",
    "            # 刷新\n",
    "            pygame.display.update()\n",
    "\n",
    "        while True:\n",
    "            if self.is_shown:\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        pygame.quit()\n",
    "                        exit()\n",
    "\n",
    "            # 根据当前棋盘状态返回可能得行为,及行为对应的概率\n",
    "            move, move_probs = player.get_action(self.board,\n",
    "                                                 temp=temp,\n",
    "                                                 return_prob=1)\n",
    "            # 存储数据\n",
    "            states.append(self.board.current_state())  # 存储状态数据\n",
    "            mcts_probs.append(move_probs)  # 存储行为概率数据\n",
    "            current_players.append(self.board.current_player)  # 存储当前玩家\n",
    "            # 执行一个移动\n",
    "            self.board.do_move(move)\n",
    "            if self.is_shown:\n",
    "                # 绘制棋盘\n",
    "                self.draw_chess()\n",
    "                # 刷新\n",
    "                pygame.display.update()\n",
    "\n",
    "            # 判断该局游戏是否终止\n",
    "            end, winner = self.board.game_end()\n",
    "            if end:\n",
    "                # 从每个状态的当时的玩家的角度看待赢家\n",
    "                winners_z = np.zeros(len(current_players))\n",
    "                if winner != -1:\n",
    "                    # 没有赢家时\n",
    "                    winners_z[np.array(current_players) == winner] = 1.0\n",
    "                    winners_z[np.array(current_players) != winner] = -1.0\n",
    "                # 重置MSCT的根节点\n",
    "                player.reset_player()\n",
    "                if self.is_shown:\n",
    "                    self.draw_result(winner)\n",
    "\n",
    "                    # 刷新\n",
    "                    pygame.display.update()\n",
    "                return winner, zip(states, mcts_probs, winners_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. 价值策略网络\n",
    "\n",
    "* 首先，让我们开始定义策略价值网络的结构，网络比较简单，由公共网络层、行动策略网络层和状态价值网络层构成。\n",
    "\n",
    "* 在定义好策略和价值网络的基础上，接下来实现PolicyValueNet类，该类主要定义：policy_value_fn()方法，主要用于蒙特卡洛树搜索时评估叶子节点对应局面评分、该局所有可行动作及对应概率；另一个方法train_step()，主要用于更新自我对弈收集数据上策略价值网络的参数。\n",
    "\n",
    "* 在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合(s,π,z),训练我们神经网络的模型参数。训练的目的是对于每个输入s, 神经网络输出的p,v和我们训练样本中的π,z差距尽可能的少。\n",
    "\n",
    "* 损失函数由三部分组成，第一部分是均方误差损失函数，用于评估神经网络预测的胜负结果和真实结果之间的差异。第二部分是交叉熵损失函数，用于评估神经网络的输出策略和我们MCTS输出的策略的差异。第三部分是L2正则化项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-03T10:11:33.765479Z",
     "iopub.status.busy": "2022-12-03T10:11:33.763613Z",
     "iopub.status.idle": "2022-12-03T10:11:33.792161Z",
     "shell.execute_reply": "2022-12-03T10:11:33.789781Z",
     "shell.execute_reply.started": "2022-12-03T10:11:33.765396Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(paddle.nn.Layer):\n",
    "    def __init__(self,board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        # 公共网络层\n",
    "        self.conv1 = nn.Conv2D(in_channels=4,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2D(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.conv3 = nn.Conv2D(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        # 行动策略网络层\n",
    "        self.act_conv1 = nn.Conv2D(in_channels=128,out_channels=4,kernel_size=1,padding=0)\n",
    "        self.act_fc1 = nn.Linear(4*self.board_width*self.board_height,\n",
    "                                 self.board_width*self.board_height)\n",
    "        self.val_conv1 = nn.Conv2D(in_channels=128,out_channels=2,kernel_size=1,padding=0)\n",
    "        self.val_fc1 = nn.Linear(2*self.board_width*self.board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 公共网络层 \n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 行动策略网络层\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = paddle.reshape(\n",
    "                x_act, [-1, 4 * self.board_height * self.board_width])\n",
    "        \n",
    "        x_act  = F.log_softmax(self.act_fc1(x_act))        \n",
    "        # 状态价值网络层\n",
    "        x_val  = F.relu(self.val_conv1(x))\n",
    "        x_val = paddle.reshape(\n",
    "                x_val, [-1, 2 * self.board_height * self.board_width])\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "\n",
    "        return x_act,x_val\n",
    "\n",
    "class PolicyValueNet():\n",
    "    \"\"\"策略&值网络 \"\"\"\n",
    "    def __init__(self, board_width, board_height,\n",
    "                 model_file=None, use_gpu=True):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-3  # coef of l2 penalty\n",
    "        \n",
    "\n",
    "        self.policy_value_net = Net(self.board_width, self.board_height)        \n",
    "        \n",
    "        self.optimizer  = paddle.optimizer.Adam(learning_rate=0.02,\n",
    "                                parameters=self.policy_value_net.parameters(), weight_decay=self.l2_const)\n",
    "                                     \n",
    "\n",
    "        if model_file:\n",
    "            net_params = paddle.load(model_file)\n",
    "            self.policy_value_net.set_state_dict(net_params)\n",
    "            \n",
    "    def policy_value(self, state_batch):\n",
    "        \"\"\"\n",
    "        input: a batch of states\n",
    "        output: a batch of action probabilities and state values\n",
    "        \"\"\"\n",
    "        # state_batch = paddle.to_tensor(np.ndarray(state_batch))\n",
    "        state_batch = paddle.to_tensor(state_batch)\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        act_probs = np.exp(log_act_probs.numpy())\n",
    "        return act_probs, value.numpy()\n",
    "\n",
    "    def policy_value_fn(self, board):\n",
    "        \"\"\"\n",
    "        input: board\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "        action and the score of the board state\n",
    "        \"\"\"\n",
    "        legal_positions = board.availables\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
    "                -1, 4, self.board_width, self.board_height)).astype(\"float32\")\n",
    "        \n",
    "        # print(current_state.shape)\n",
    "        current_state = paddle.to_tensor(current_state)\n",
    "        log_act_probs, value = self.policy_value_net(current_state)\n",
    "        act_probs = np.exp(log_act_probs.numpy().flatten())\n",
    "        \n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        # value = value.numpy()\n",
    "        return act_probs, value.numpy()\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr=0.002):\n",
    "        \"\"\"perform a training step\"\"\"\n",
    "        # wrap in Variable\n",
    "        state_batch = paddle.to_tensor(state_batch)\n",
    "        mcts_probs = paddle.to_tensor(mcts_probs)\n",
    "        winner_batch = paddle.to_tensor(winner_batch)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.clear_gradients()\n",
    "        # set learning rate\n",
    "        self.optimizer.set_lr(lr)\n",
    "\n",
    "                                     \n",
    "\n",
    "        # forward\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        # Note: the L2 penalty is incorporated in optimizer\n",
    "        value = paddle.reshape(x=value, shape=[-1])\n",
    "        value_loss = F.mse_loss(input=value, label=winner_batch)\n",
    "        policy_loss = -paddle.mean(paddle.sum(mcts_probs*log_act_probs, axis=1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.minimize(loss)\n",
    "        # calc policy entropy, for monitoring only\n",
    "        entropy = -paddle.mean(\n",
    "                paddle.sum(paddle.exp(log_act_probs) * log_act_probs, axis=1)\n",
    "                )\n",
    "        return loss.numpy(), entropy.numpy()[0]    \n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\" save model params to file \"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        paddle.save(net_params, model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. 蒙特卡洛树搜索（MCTS）\n",
    "\n",
    "传统的AI博弈树搜索算法效率都很低，因为这些算法在做出最终选择前需要穷尽每一种走法。这样即使是带有较少分支因子的游戏也会使其博弈搜索空间爆炸增长。分支因子就是所有可能的走法的数量，这个数量会随着游戏的进行不断变化。因此，你可以试着计算一个游戏的平均分支因子数，国际象棋的平均分支因子是35，而围棋则是250。这意味着，在国际象棋中，仅走两步就有1,225（35²）种可能的棋面，而在围棋中，这个数字会变成62,500（250²）。因此，上述的价值策略神经网络将指导并告诉我们哪些博弈路径值得探索，从而避免被许多无用的搜索路径所淹没。而蒙特卡洛搜索树来选择具体要走法。\n",
    "\n",
    "## 棋类游戏的蒙特卡洛树搜索（MCTS）\n",
    "使用MCTS的具体做法是这样的，给定一个棋面，MCTS共进行N次模拟。主要的搜索阶段有4个：选择，扩展，仿真和回溯\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/73384055df364b44a49e7e206a9015790be7b3c0aa1942d0a4e57aa617fad087)\n",
    "\n",
    "* 第一步是选择(Selection):这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用UCT选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点\n",
    "\n",
    "* 第二步是扩展(Expansion)，在这个搜索到的存在未扩展的子节点，加上一个没有历史记录的子节点，初始化子节点\n",
    "\n",
    "* 第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略（Rollout policy）走到底，得到一个胜负结果。快速走子策略一般适合选择走子很快可能不是很精确的策略。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。\n",
    "\n",
    "* 第四步是回溯(backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录。\n",
    "\n",
    "以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。\n",
    "## 基于神经网络的蒙特卡洛树搜索（MCTS）\n",
    "N(s,a) :记录边的访问次数；\n",
    "W(s,a):  合计行动价值；\n",
    "Q(s,a) :平均行动价值；\n",
    "P(s,a) :选择该条边的先验概率；\n",
    "* 首先是选择(Selection):在MCTS内部，出现过的局面，我们会使用UCT选择子分支。最终我们会选择Q+U最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a3edc34d8d554068becbfb21b7f6a5f7fc0b43f804eb45ed9c92a27d38478fdd)\n",
    "\n",
    "* 然后是扩展(Expansion)&&仿真(simulation)，对于叶子节点状态s，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL落子的概率p和对应的价值v,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aed60f9babbb4c208f19d480fd25558b903c4836a2e5438bb858e6ddcaa218c9)\n",
    "\n",
    "\n",
    "* 最后是回溯(backpropagation)， 将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点L依次向根节点回溯，并依次更新上层分支数据结构如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b39e2bd4ab0f42a691e6a239443bc57f0dfac929a03b428b8a06b0416d4340c3)\n",
    "\n",
    "MCTS搜索完毕后，模型就可以在MCTS的根节点s基于以下公式选择行棋的MCTS分支了:\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3a14cd6be857468b9bcbcdee61d6ecdb325a864649284df4a8aa5b1d2b7605a0)\n",
    "\n",
    "τ是用来控制探索的程度，τ的取值介于(0,1]之间，当τ越接近于1时，神经网络的采样越接近于MCTS的原始采样，当τ越接近于0时，神经网络的采样越接近于贪婪策略，即选择最大访问次数N所对应的动作。\n",
    "因为在τ很小的情况下，直接计算访问次数N的τ次方根可能会导致数值异常，为了避免这种情况，在计算行动概率时，先将访问次数N加上一个非常小的数值（本项目是1e-10），取自然对数后乘上1/τ，再用一个简化的softmax函数将输出还原为概率，这和原始公式在数学上基本上是等效的。\n",
    "\n",
    "**关键点是什么？**\n",
    "* 通过每一次模拟，MCTS依靠神经网络， 使用累计价值（Q）、神经网络给出的走法先验概率（P）以及访问对应节点的频率这些数字的组合，沿着最有希望获胜的路径（换句话说，也就是具有最高置信区间上界的路径）进行探索。\n",
    "* 在每一次模拟中，MCTS会尽可能向纵深进行探索直至遇到它从未见过的盘面状态，在这种情况下，它会通过神经网络来评估该盘面状态的优劣\n",
    "* 巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。\n",
    "\n",
    "## 训练算法流程\n",
    "* AlphaZero的算法流程，概括来说就是通过自我对弈收集数据，并用于更新策略价值网络，更新后的策略价值网络又会被用于后续的自我对弈过程中，从而产生高质量的自我对弈数据，这样相互促进、不断迭代，实现稳定的学习和提升。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-03T10:11:33.795852Z",
     "iopub.status.busy": "2022-12-03T10:11:33.795257Z",
     "iopub.status.idle": "2022-12-03T10:11:33.842045Z",
     "shell.execute_reply": "2022-12-03T10:11:33.841071Z",
     "shell.execute_reply.started": "2022-12-03T10:11:33.795799Z"
    },
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs\n",
    "\n",
    "def rollout_policy_fn(board):\n",
    "    \"\"\"在首次展示阶段使用策略方法的粗略,快速的版本.\"\"\"\n",
    "    # 初次展示时使用随机方式\n",
    "    action_probs = np.random.rand(len(board.availables))\n",
    "    return zip(board.availables, action_probs)\n",
    "\n",
    "\n",
    "def policy_value_fn(board):\n",
    "    \"\"\"\n",
    "    接受状态并输出（动作，概率）列表的函数元组和状态的分数\"\"\"\n",
    "    # 返回统一概率和0分的纯MCTS\n",
    "    action_probs = np.ones(len(board.availables)) / len(board.availables)\n",
    "    return zip(board.availables, action_probs), 0\n",
    "\n",
    "\n",
    "class TreeNode(object):\n",
    "    \"\"\"MCTS树中的节点。\n",
    "\n",
    "    每个节点跟踪其自身的值Q，先验概率P及其访问次数调整的先前得分u。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # 从动作到TreeNode的映射\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"通过创建新子项来展开树。\n",
    "     action_priors：一系列动作元组及其先验概率根据策略函数.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(self, prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"在子节点中选择能够提供最大行动价值Q的行动加上奖金u（P）。\n",
    "     return：（action，next_node）的元组\n",
    "        \"\"\"\n",
    "        return max(self._children.items(),\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "\n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"从叶节点评估中更新节点值\n",
    "        leaf_value: 这个子树的评估值来自从当前玩家的视角\n",
    "        \"\"\"\n",
    "        # 统计访问次数\n",
    "        self._n_visits += 1\n",
    "        # 更新Q值,取对于所有访问次数的平均数\n",
    "        self._Q += 1.0*(leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"就像调用update（）一样，但是对所有祖先进行递归应用。\n",
    "        \"\"\"\n",
    "        # 如果它不是根节点，则应首先更新此节点的父节点。\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"计算并返回此节点的值。它是叶评估Q和此节点的先验的组合\n",
    "     调整了访问次数，u。\n",
    "     c_puct：控制相对影响的（0，inf）中的数字，该节点得分的值Q和先验概率P.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"检查叶节点（即没有扩展的节点）。\"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "\n",
    "\n",
    "class MCTS_train(object):\n",
    "    \"\"\"对蒙特卡罗树搜索的一个简单实现\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn：一个接收板状态和输出的函数（动作，概率）元组列表以及[-1,1]中的分数\n",
    "             （即来自当前的最终比赛得分的预期值玩家的观点）对于当前的玩家。\n",
    "    c_puct：（0，inf）中的数字，用于控制探索的速度收敛于最大值政策。 更高的价值意味着\n",
    "             依靠先前的更多。\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "\n",
    "    def _playout(self, state):\n",
    "        \"\"\"从根到叶子运行单个播出，获取值\n",
    "         叶子并通过它的父母传播回来。\n",
    "         State已就地修改，因此必须提供副本。\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while(1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # 贪心算法选择下一步行动\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.do_move(action)\n",
    "\n",
    "        # 使用网络评估叶子，该网络输出（动作，概率）元组p的列表以及当前玩家的[-1,1]中的分数v。\n",
    "        action_probs, leaf_value = self._policy(state)\n",
    "        # 查看游戏是否结束\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        else:\n",
    "            # 对于结束状态,将叶子节点的值换成\"true\"\n",
    "            if winner == -1:  # tie\n",
    "                leaf_value = 0.0\n",
    "            else:\n",
    "                leaf_value = (\n",
    "                    1.0 if winner == state.get_current_player() else -1.0\n",
    "                )\n",
    "\n",
    "        # 在本次遍历中更新节点的值和访问次数\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    def get_move_probs(self, state, temp=1e-3):\n",
    "        \"\"\"按顺序运行所有播出并返回可用的操作及其相应的概率。\n",
    "        state: 当前游戏的状态\n",
    "        temp: 介于(0,1]之间的临时参数控制探索的概率\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "\n",
    "        # 根据根节点处的访问计数来计算移动概率\n",
    "        act_visits = [(act, node._n_visits)\n",
    "                      for act, node in self._root._children.items()]\n",
    "        acts, visits = zip(*act_visits)\n",
    "        act_probs = softmax(1.0/temp * np.log(np.array(visits) + 1e-10))\n",
    "\n",
    "        return acts, act_probs\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"在当前的树上向前一步，保持我们已经知道的关于子树的一切.\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\"\n",
    "\n",
    "\n",
    "class MCTSPlayer(object):\n",
    "    \"\"\"基于MCTS的AI玩家\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_function,\n",
    "                 c_puct=5, n_playout=2000, is_selfplay=0):\n",
    "        self.mcts = MCTS_train(policy_value_function, c_puct, n_playout)\n",
    "        self._is_selfplay = is_selfplay\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, board, temp=1e-3, return_prob=0):\n",
    "        sensible_moves = board.availables\n",
    "        # 像alphaGo Zero论文一样使用MCTS算法返回的pi向量\n",
    "        move_probs = np.zeros(board.width*board.height)\n",
    "        if len(sensible_moves) > 0:\n",
    "            acts, probs = self.mcts.get_move_probs(board, temp)\n",
    "            move_probs[list(acts)] = probs\n",
    "            if self._is_selfplay:\n",
    "                # 添加Dirichlet Noise进行探索（自我训练所需）\n",
    "                move = np.random.choice(\n",
    "                    acts,\n",
    "                    p=0.75*probs + 0.25*np.random.dirichlet(0.3*np.ones(len(probs)))\n",
    "                )\n",
    "                # 更新根节点并重用搜索树\n",
    "                self.mcts.update_with_move(move)\n",
    "            else:\n",
    "                # 使用默认的temp = 1e-3，它几乎相当于选择具有最高概率的移动\n",
    "                move = np.random.choice(acts, p=probs)\n",
    "                # 重置根节点\n",
    "                self.mcts.update_with_move(-1)\n",
    "\n",
    "            if return_prob:\n",
    "                return move, move_probs\n",
    "            else:\n",
    "                return move\n",
    "        else:\n",
    "            print(\"棋盘已满\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)\n",
    "\n",
    "class MCTS_evaluate(object):\n",
    "    \"\"\"对蒙特卡罗树搜索的一个简单实现\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000):\n",
    "        \"\"\"\n",
    "        policy_value_fn：一个接收板状态和输出的函数\n",
    "             （动作，概率）元组列表以及[-1,1]中的分数\n",
    "             （即来自当前的最终比赛得分的预期值\n",
    "             玩家的观点）对于当前的玩家。\n",
    "        c_puct：（0，inf）中的数字，用于控制探索的速度\n",
    "             收敛于最大值政策。 更高的价值意味着\n",
    "             依靠先前的更多。\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "\n",
    "    def _playout(self, state):\n",
    "        \"\"\"\n",
    "        从根到叶子运行单个播出，获取值\n",
    "         叶子并通过它的父母传播回来。\n",
    "         State已就地修改，因此必须提供副本。\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while (1):\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # 贪心算法选择下一步行动\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.do_move(action)\n",
    "\n",
    "        action_probs, _ = self._policy(state)\n",
    "        # 查询游戏是否终结\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        # 通过随机的rollout评估叶子结点\n",
    "        leaf_value = self._evaluate_rollout(state)\n",
    "        # 在本次遍历中更新节点的值和访问次数\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    def _evaluate_rollout(self, state, limit=1000):\n",
    "        \"\"\"使用推出策略直到游戏结束，\n",
    "      如果当前玩家获胜则返回+1，如果对手获胜则返回-1，\n",
    "     如果是平局则为0。\n",
    "        \"\"\"\n",
    "        player = state.get_current_player()\n",
    "        for i in range(limit):\n",
    "            end, winner = state.game_end()\n",
    "            if end:\n",
    "                break\n",
    "            action_probs = rollout_policy_fn(state)\n",
    "            max_action = max(action_probs, key=itemgetter(1))[0]\n",
    "            state.do_move(max_action)\n",
    "        else:\n",
    "            # 如果没有从循环中断，请发出警告。\n",
    "            print(\"WARNING: rollout reached move limit\")\n",
    "        if winner == -1:  # tie\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 if winner == player else -1\n",
    "\n",
    "    def get_move(self, state):\n",
    "        \"\"\"按顺序运行所有播出并返回访问量最大的操作。\n",
    "     state：当前的比赛状态\n",
    "     return ：所选操作\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "        return max(self._root._children.items(),\n",
    "                   key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"保留我们已经知道的关于子树的信息\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\"\n",
    "\n",
    "\n",
    "class MCTS_Pure(object):\n",
    "    \"\"\"基于MCTS的AI玩家\"\"\"\n",
    "\n",
    "    def __init__(self, c_puct=5, n_playout=2000):\n",
    "        self.mcts = MCTS_evaluate(policy_value_fn, c_puct, n_playout)\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, board):\n",
    "        sensible_moves = board.availables\n",
    "        if len(sensible_moves) > 0:\n",
    "            move = self.mcts.get_move(board)\n",
    "            self.mcts.update_with_move(-1)\n",
    "            return move\n",
    "        else:\n",
    "            print(\"棋盘已满\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-12-03T10:11:33.904454Z",
     "iopub.status.busy": "2022-12-03T10:11:33.903969Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:768:(parse_card) cannot find card '0'\r\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\r\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\r\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\r\n",
      "ALSA lib confmisc.c:1251:(snd_func_refer) error evaluating name\r\n",
      "ALSA lib conf.c:4292:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\r\n",
      "ALSA lib conf.c:4771:(snd_config_expand) Evaluate error: No such file or directory\r\n",
      "ALSA lib pcm.c:2266:(snd_pcm_open_noupdate) Unknown PCM default\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch i:133, episode_len:28\r\n",
      "kl:0.02397,lr_multiplier:0.198,loss:[3.8678508],entropy:3.3453786373138428,explained_var_old:0.305,explained_var_new:0.429\r\n",
      "loss :[3.8678508], entropy:3.3453786373138428\r\n",
      "batch i:134, episode_len:18\r\n",
      "kl:0.02196,lr_multiplier:0.198,loss:[3.7762082],entropy:3.294621706008911,explained_var_old:0.369,explained_var_new:0.442\r\n",
      "loss :[3.7762082], entropy:3.294621706008911\r\n",
      "batch i:135, episode_len:19\r\n",
      "kl:0.03064,lr_multiplier:0.198,loss:[3.866716],entropy:3.2450034618377686,explained_var_old:0.332,explained_var_new:0.430\r\n",
      "loss :[3.866716], entropy:3.2450034618377686\r\n",
      "batch i:136, episode_len:40\r\n",
      "kl:0.02187,lr_multiplier:0.198,loss:[3.716823],entropy:3.2354001998901367,explained_var_old:0.405,explained_var_new:0.491\r\n",
      "loss :[3.716823], entropy:3.2354001998901367\r\n",
      "batch i:137, episode_len:17\r\n",
      "kl:0.03527,lr_multiplier:0.198,loss:[3.7296898],entropy:3.249450206756592,explained_var_old:0.409,explained_var_new:0.497\r\n",
      "loss :[3.7296898], entropy:3.249450206756592\r\n",
      "batch i:138, episode_len:34\r\n",
      "kl:0.02256,lr_multiplier:0.198,loss:[3.8259187],entropy:3.2435872554779053,explained_var_old:0.339,explained_var_new:0.425\r\n",
      "loss :[3.8259187], entropy:3.2435872554779053\r\n",
      "batch i:139, episode_len:29\r\n",
      "kl:0.02980,lr_multiplier:0.198,loss:[3.7900295],entropy:3.2262120246887207,explained_var_old:0.381,explained_var_new:0.465\r\n",
      "loss :[3.7900295], entropy:3.2262120246887207\r\n",
      "batch i:140, episode_len:27\r\n",
      "kl:0.01975,lr_multiplier:0.198,loss:[3.8560126],entropy:3.2468602657318115,explained_var_old:0.263,explained_var_new:0.395\r\n",
      "loss :[3.8560126], entropy:3.2468602657318115\r\n",
      "batch i:141, episode_len:17\r\n",
      "kl:0.02307,lr_multiplier:0.198,loss:[3.8686],entropy:3.3325388431549072,explained_var_old:0.315,explained_var_new:0.429\r\n",
      "loss :[3.8686], entropy:3.3325388431549072\r\n",
      "batch i:142, episode_len:18\r\n",
      "kl:0.02522,lr_multiplier:0.198,loss:[3.8259652],entropy:3.2634339332580566,explained_var_old:0.328,explained_var_new:0.421\r\n",
      "loss :[3.8259652], entropy:3.2634339332580566\r\n",
      "batch i:143, episode_len:21\r\n",
      "kl:0.02902,lr_multiplier:0.198,loss:[3.79714],entropy:3.219214677810669,explained_var_old:0.253,explained_var_new:0.365\r\n",
      "loss :[3.79714], entropy:3.219214677810669\r\n",
      "batch i:144, episode_len:35\r\n",
      "kl:0.02981,lr_multiplier:0.198,loss:[3.8165174],entropy:3.240899085998535,explained_var_old:0.353,explained_var_new:0.446\r\n",
      "loss :[3.8165174], entropy:3.240899085998535\r\n",
      "batch i:145, episode_len:22\r\n",
      "kl:0.02844,lr_multiplier:0.198,loss:[3.7672222],entropy:3.2448935508728027,explained_var_old:0.378,explained_var_new:0.458\r\n",
      "loss :[3.7672222], entropy:3.2448935508728027\r\n",
      "batch i:146, episode_len:20\r\n",
      "kl:0.03359,lr_multiplier:0.198,loss:[3.6746285],entropy:3.1835885047912598,explained_var_old:0.374,explained_var_new:0.488\r\n",
      "loss :[3.6746285], entropy:3.1835885047912598\r\n",
      "batch i:147, episode_len:20\r\n",
      "kl:0.04775,lr_multiplier:0.132,loss:[3.6996055],entropy:3.2493324279785156,explained_var_old:0.464,explained_var_new:0.533\r\n",
      "loss :[3.6996055], entropy:3.2493324279785156\r\n",
      "batch i:148, episode_len:24\r\n",
      "kl:0.04400,lr_multiplier:0.088,loss:[3.7624755],entropy:3.1765334606170654,explained_var_old:0.364,explained_var_new:0.447\r\n",
      "loss :[3.7624755], entropy:3.1765334606170654\r\n",
      "batch i:149, episode_len:18\r\n",
      "kl:0.00951,lr_multiplier:0.132,loss:[3.6341715],entropy:3.0975985527038574,explained_var_old:0.397,explained_var_new:0.475\r\n",
      "loss :[3.6341715], entropy:3.0975985527038574\r\n",
      "batch i:150, episode_len:32\r\n",
      "kl:0.02663,lr_multiplier:0.132,loss:[3.7542443],entropy:3.2611100673675537,explained_var_old:0.379,explained_var_new:0.447\r\n",
      "loss :[3.7542443], entropy:3.2611100673675537\r\n",
      "batch i:151, episode_len:38\r\n",
      "kl:0.01967,lr_multiplier:0.132,loss:[3.6389508],entropy:3.160550594329834,explained_var_old:0.406,explained_var_new:0.478\r\n",
      "loss :[3.6389508], entropy:3.160550594329834\r\n",
      "batch i:152, episode_len:9\r\n",
      "kl:0.01212,lr_multiplier:0.132,loss:[3.5424707],entropy:3.1093318462371826,explained_var_old:0.462,explained_var_new:0.521\r\n",
      "loss :[3.5424707], entropy:3.1093318462371826\r\n",
      "batch i:153, episode_len:13\r\n",
      "kl:0.01383,lr_multiplier:0.132,loss:[3.6179972],entropy:3.10671067237854,explained_var_old:0.483,explained_var_new:0.548\r\n",
      "loss :[3.6179972], entropy:3.10671067237854\r\n",
      "batch i:154, episode_len:16\r\n",
      "kl:0.01411,lr_multiplier:0.132,loss:[3.492502],entropy:3.0807080268859863,explained_var_old:0.465,explained_var_new:0.523\r\n",
      "loss :[3.492502], entropy:3.0807080268859863\r\n",
      "batch i:155, episode_len:16\r\n",
      "kl:0.02029,lr_multiplier:0.132,loss:[3.628665],entropy:3.1283938884735107,explained_var_old:0.401,explained_var_new:0.475\r\n",
      "loss :[3.628665], entropy:3.1283938884735107\r\n",
      "batch i:156, episode_len:26\r\n",
      "kl:0.03008,lr_multiplier:0.132,loss:[3.6844373],entropy:3.0784332752227783,explained_var_old:0.386,explained_var_new:0.464\r\n",
      "loss :[3.6844373], entropy:3.0784332752227783\r\n",
      "batch i:157, episode_len:37\r\n",
      "kl:0.02938,lr_multiplier:0.132,loss:[3.6530123],entropy:3.1568052768707275,explained_var_old:0.384,explained_var_new:0.460\r\n",
      "loss :[3.6530123], entropy:3.1568052768707275\r\n",
      "batch i:158, episode_len:23\r\n",
      "kl:0.02330,lr_multiplier:0.132,loss:[3.6233516],entropy:3.089895248413086,explained_var_old:0.397,explained_var_new:0.472\r\n",
      "loss :[3.6233516], entropy:3.089895248413086\r\n",
      "batch i:159, episode_len:22\r\n",
      "kl:0.03228,lr_multiplier:0.132,loss:[3.6718912],entropy:3.1241164207458496,explained_var_old:0.391,explained_var_new:0.445\r\n",
      "loss :[3.6718912], entropy:3.1241164207458496\r\n",
      "batch i:160, episode_len:34\r\n",
      "kl:0.02421,lr_multiplier:0.132,loss:[3.6318402],entropy:3.0671634674072266,explained_var_old:0.356,explained_var_new:0.421\r\n",
      "loss :[3.6318402], entropy:3.0671634674072266\r\n",
      "batch i:161, episode_len:22\r\n",
      "kl:0.00950,lr_multiplier:0.198,loss:[3.6858187],entropy:3.087029457092285,explained_var_old:0.340,explained_var_new:0.411\r\n",
      "loss :[3.6858187], entropy:3.087029457092285\r\n",
      "batch i:162, episode_len:24\r\n",
      "kl:0.03795,lr_multiplier:0.198,loss:[3.6418734],entropy:3.097334384918213,explained_var_old:0.320,explained_var_new:0.431\r\n",
      "loss :[3.6418734], entropy:3.097334384918213\r\n",
      "batch i:163, episode_len:18\r\n",
      "kl:0.01900,lr_multiplier:0.198,loss:[3.5868454],entropy:3.0533437728881836,explained_var_old:0.388,explained_var_new:0.463\r\n",
      "loss :[3.5868454], entropy:3.0533437728881836\r\n",
      "batch i:164, episode_len:9\r\n",
      "kl:0.03153,lr_multiplier:0.198,loss:[3.6301823],entropy:3.101895332336426,explained_var_old:0.339,explained_var_new:0.431\r\n",
      "loss :[3.6301823], entropy:3.101895332336426\r\n",
      "batch i:165, episode_len:12\r\n",
      "kl:0.02020,lr_multiplier:0.198,loss:[3.4729369],entropy:2.9986462593078613,explained_var_old:0.450,explained_var_new:0.543\r\n",
      "loss :[3.4729369], entropy:2.9986462593078613\r\n",
      "batch i:166, episode_len:19\r\n",
      "kl:0.02432,lr_multiplier:0.198,loss:[3.5267396],entropy:3.0203704833984375,explained_var_old:0.417,explained_var_new:0.497\r\n",
      "loss :[3.5267396], entropy:3.0203704833984375\r\n",
      "batch i:167, episode_len:18\r\n",
      "kl:0.02979,lr_multiplier:0.198,loss:[3.569073],entropy:3.060734272003174,explained_var_old:0.416,explained_var_new:0.507\r\n",
      "loss :[3.569073], entropy:3.060734272003174\r\n",
      "batch i:168, episode_len:19\r\n",
      "kl:0.01985,lr_multiplier:0.198,loss:[3.6175795],entropy:3.0591530799865723,explained_var_old:0.362,explained_var_new:0.459\r\n",
      "loss :[3.6175795], entropy:3.0591530799865723\r\n",
      "batch i:169, episode_len:17\r\n",
      "kl:0.02052,lr_multiplier:0.198,loss:[3.5045097],entropy:3.066736936569214,explained_var_old:0.447,explained_var_new:0.528\r\n",
      "loss :[3.5045097], entropy:3.066736936569214\r\n",
      "batch i:170, episode_len:13\r\n",
      "kl:0.01955,lr_multiplier:0.198,loss:[3.5345864],entropy:3.0457279682159424,explained_var_old:0.441,explained_var_new:0.542\r\n",
      "loss :[3.5345864], entropy:3.0457279682159424\r\n",
      "batch i:171, episode_len:32\r\n",
      "kl:0.02803,lr_multiplier:0.198,loss:[3.5380943],entropy:3.029998302459717,explained_var_old:0.377,explained_var_new:0.479\r\n",
      "loss :[3.5380943], entropy:3.029998302459717\r\n",
      "batch i:172, episode_len:21\r\n",
      "kl:0.02169,lr_multiplier:0.198,loss:[3.4728563],entropy:3.0489699840545654,explained_var_old:0.455,explained_var_new:0.569\r\n",
      "loss :[3.4728563], entropy:3.0489699840545654\r\n",
      "batch i:173, episode_len:24\r\n",
      "kl:0.02460,lr_multiplier:0.198,loss:[3.462148],entropy:3.0142664909362793,explained_var_old:0.427,explained_var_new:0.537\r\n",
      "loss :[3.462148], entropy:3.0142664909362793\r\n",
      "batch i:174, episode_len:9\r\n",
      "kl:0.02248,lr_multiplier:0.198,loss:[3.4984417],entropy:3.006854772567749,explained_var_old:0.393,explained_var_new:0.484\r\n",
      "loss :[3.4984417], entropy:3.006854772567749\r\n",
      "batch i:175, episode_len:14\r\n",
      "kl:0.02091,lr_multiplier:0.198,loss:[3.4392805],entropy:3.057138681411743,explained_var_old:0.501,explained_var_new:0.601\r\n",
      "loss :[3.4392805], entropy:3.057138681411743\r\n",
      "batch i:176, episode_len:18\r\n",
      "kl:0.02916,lr_multiplier:0.198,loss:[3.5516922],entropy:3.082890272140503,explained_var_old:0.453,explained_var_new:0.534\r\n",
      "loss :[3.5516922], entropy:3.082890272140503\r\n",
      "batch i:177, episode_len:12\r\n",
      "kl:0.01918,lr_multiplier:0.198,loss:[3.5182009],entropy:3.0334696769714355,explained_var_old:0.445,explained_var_new:0.527\r\n",
      "loss :[3.5182009], entropy:3.0334696769714355\r\n",
      "batch i:178, episode_len:16\r\n",
      "kl:0.03149,lr_multiplier:0.198,loss:[3.5739493],entropy:3.1005470752716064,explained_var_old:0.439,explained_var_new:0.524\r\n",
      "loss :[3.5739493], entropy:3.1005470752716064\r\n",
      "batch i:179, episode_len:28\r\n",
      "kl:0.02555,lr_multiplier:0.198,loss:[3.4715323],entropy:3.0382237434387207,explained_var_old:0.405,explained_var_new:0.504\r\n",
      "loss :[3.4715323], entropy:3.0382237434387207\r\n",
      "batch i:180, episode_len:26\r\n",
      "kl:0.02789,lr_multiplier:0.198,loss:[3.4610827],entropy:3.0083885192871094,explained_var_old:0.506,explained_var_new:0.592\r\n",
      "loss :[3.4610827], entropy:3.0083885192871094\r\n",
      "batch i:181, episode_len:17\r\n",
      "kl:0.02612,lr_multiplier:0.198,loss:[3.529778],entropy:3.0304739475250244,explained_var_old:0.387,explained_var_new:0.480\r\n",
      "loss :[3.529778], entropy:3.0304739475250244\r\n",
      "batch i:182, episode_len:16\r\n",
      "kl:0.03519,lr_multiplier:0.198,loss:[3.4762886],entropy:3.02909255027771,explained_var_old:0.449,explained_var_new:0.530\r\n",
      "loss :[3.4762886], entropy:3.02909255027771\r\n",
      "batch i:183, episode_len:16\r\n",
      "kl:0.03142,lr_multiplier:0.198,loss:[3.4000983],entropy:3.0360419750213623,explained_var_old:0.512,explained_var_new:0.614\r\n",
      "loss :[3.4000983], entropy:3.0360419750213623\r\n",
      "batch i:184, episode_len:14\r\n",
      "kl:0.02403,lr_multiplier:0.198,loss:[3.4568026],entropy:3.039496421813965,explained_var_old:0.457,explained_var_new:0.558\r\n",
      "loss :[3.4568026], entropy:3.039496421813965\r\n",
      "batch i:185, episode_len:44\r\n",
      "kl:0.02734,lr_multiplier:0.198,loss:[3.3997593],entropy:3.006253719329834,explained_var_old:0.501,explained_var_new:0.617\r\n",
      "loss :[3.3997593], entropy:3.006253719329834\r\n",
      "batch i:186, episode_len:23\r\n",
      "kl:0.03104,lr_multiplier:0.198,loss:[3.4256508],entropy:3.029216766357422,explained_var_old:0.482,explained_var_new:0.582\r\n",
      "loss :[3.4256508], entropy:3.029216766357422\r\n",
      "batch i:187, episode_len:20\r\n",
      "kl:0.04272,lr_multiplier:0.132,loss:[3.4752657],entropy:3.11204195022583,explained_var_old:0.501,explained_var_new:0.576\r\n",
      "loss :[3.4752657], entropy:3.11204195022583\r\n",
      "batch i:188, episode_len:18\r\n",
      "kl:0.01829,lr_multiplier:0.132,loss:[3.505046],entropy:3.030243396759033,explained_var_old:0.435,explained_var_new:0.507\r\n",
      "loss :[3.505046], entropy:3.030243396759033\r\n",
      "batch i:189, episode_len:17\r\n",
      "kl:0.02186,lr_multiplier:0.132,loss:[3.4865317],entropy:3.0306601524353027,explained_var_old:0.409,explained_var_new:0.497\r\n",
      "loss :[3.4865317], entropy:3.0306601524353027\r\n",
      "batch i:190, episode_len:18\r\n",
      "kl:0.01790,lr_multiplier:0.132,loss:[3.3579936],entropy:2.9761409759521484,explained_var_old:0.531,explained_var_new:0.578\r\n",
      "loss :[3.3579936], entropy:2.9761409759521484\r\n",
      "batch i:191, episode_len:22\r\n",
      "kl:0.01509,lr_multiplier:0.132,loss:[3.4513395],entropy:2.994032859802246,explained_var_old:0.466,explained_var_new:0.531\r\n",
      "loss :[3.4513395], entropy:2.994032859802246\r\n",
      "batch i:192, episode_len:22\r\n",
      "kl:0.02287,lr_multiplier:0.132,loss:[3.3024356],entropy:2.9719455242156982,explained_var_old:0.542,explained_var_new:0.613\r\n",
      "loss :[3.3024356], entropy:2.9719455242156982\r\n",
      "batch i:193, episode_len:23\r\n",
      "kl:0.02148,lr_multiplier:0.132,loss:[3.4814677],entropy:2.990506649017334,explained_var_old:0.487,explained_var_new:0.549\r\n",
      "loss :[3.4814677], entropy:2.990506649017334\r\n",
      "batch i:194, episode_len:13\r\n",
      "kl:0.01690,lr_multiplier:0.132,loss:[3.4568942],entropy:2.9676809310913086,explained_var_old:0.464,explained_var_new:0.538\r\n",
      "loss :[3.4568942], entropy:2.9676809310913086\r\n",
      "batch i:195, episode_len:24\r\n",
      "kl:0.01274,lr_multiplier:0.132,loss:[3.3911731],entropy:2.983314037322998,explained_var_old:0.423,explained_var_new:0.511\r\n",
      "loss :[3.3911731], entropy:2.983314037322998\r\n",
      "batch i:196, episode_len:15\r\n",
      "kl:0.00901,lr_multiplier:0.198,loss:[3.3893268],entropy:2.9626996517181396,explained_var_old:0.485,explained_var_new:0.559\r\n",
      "loss :[3.3893268], entropy:2.9626996517181396\r\n",
      "batch i:197, episode_len:24\r\n",
      "kl:0.01761,lr_multiplier:0.198,loss:[3.4611666],entropy:2.94964599609375,explained_var_old:0.413,explained_var_new:0.526\r\n",
      "loss :[3.4611666], entropy:2.94964599609375\r\n",
      "batch i:198, episode_len:29\r\n",
      "kl:0.01810,lr_multiplier:0.198,loss:[3.3981323],entropy:2.948513984680176,explained_var_old:0.428,explained_var_new:0.528\r\n",
      "loss :[3.3981323], entropy:2.948513984680176\r\n",
      "batch i:199, episode_len:20\r\n",
      "kl:0.02910,lr_multiplier:0.198,loss:[3.3999171],entropy:2.993915319442749,explained_var_old:0.451,explained_var_new:0.551\r\n",
      "loss :[3.3999171], entropy:2.993915319442749\r\n",
      "batch i:200, episode_len:12\r\n",
      "kl:0.02992,lr_multiplier:0.198,loss:[3.360405],entropy:2.94319486618042,explained_var_old:0.435,explained_var_new:0.535\r\n",
      "loss :[3.360405], entropy:2.94319486618042\r\n",
      "current self-play batch: 200\r\n",
      "num_playouts:1000, win: 6, lose: 4, tie:0\r\n",
      "New best policy!!!!!!!!\r\n",
      "batch i:201, episode_len:13\r\n",
      "kl:0.02370,lr_multiplier:0.198,loss:[3.4064195],entropy:2.9012465476989746,explained_var_old:0.416,explained_var_new:0.521\r\n",
      "loss :[3.4064195], entropy:2.9012465476989746\r\n",
      "batch i:202, episode_len:14\r\n",
      "kl:0.02776,lr_multiplier:0.198,loss:[3.3672585],entropy:2.970543384552002,explained_var_old:0.483,explained_var_new:0.569\r\n",
      "loss :[3.3672585], entropy:2.970543384552002\r\n",
      "batch i:203, episode_len:28\r\n",
      "kl:0.03624,lr_multiplier:0.198,loss:[3.3203487],entropy:2.9696059226989746,explained_var_old:0.496,explained_var_new:0.591\r\n",
      "loss :[3.3203487], entropy:2.9696059226989746\r\n",
      "batch i:204, episode_len:14\r\n",
      "kl:0.02685,lr_multiplier:0.198,loss:[3.2693772],entropy:2.8813118934631348,explained_var_old:0.512,explained_var_new:0.603\r\n",
      "loss :[3.2693772], entropy:2.8813118934631348\r\n",
      "batch i:205, episode_len:22\r\n",
      "kl:0.02461,lr_multiplier:0.198,loss:[3.3797903],entropy:2.8726119995117188,explained_var_old:0.457,explained_var_new:0.541\r\n",
      "loss :[3.3797903], entropy:2.8726119995117188\r\n",
      "batch i:206, episode_len:21\r\n",
      "kl:0.02245,lr_multiplier:0.198,loss:[3.3093376],entropy:2.8849408626556396,explained_var_old:0.451,explained_var_new:0.557\r\n",
      "loss :[3.3093376], entropy:2.8849408626556396\r\n",
      "batch i:207, episode_len:33\r\n",
      "kl:0.02854,lr_multiplier:0.198,loss:[3.3331132],entropy:2.894634246826172,explained_var_old:0.434,explained_var_new:0.533\r\n",
      "loss :[3.3331132], entropy:2.894634246826172\r\n",
      "batch i:208, episode_len:15\r\n",
      "kl:0.02612,lr_multiplier:0.198,loss:[3.3047504],entropy:2.9574146270751953,explained_var_old:0.476,explained_var_new:0.599\r\n",
      "loss :[3.3047504], entropy:2.9574146270751953\r\n",
      "batch i:209, episode_len:18\r\n",
      "kl:0.03813,lr_multiplier:0.198,loss:[3.4674616],entropy:2.897101879119873,explained_var_old:0.324,explained_var_new:0.467\r\n",
      "loss :[3.4674616], entropy:2.897101879119873\r\n",
      "batch i:210, episode_len:18\r\n",
      "kl:0.02962,lr_multiplier:0.198,loss:[3.3651783],entropy:2.9216601848602295,explained_var_old:0.428,explained_var_new:0.530\r\n",
      "loss :[3.3651783], entropy:2.9216601848602295\r\n",
      "batch i:211, episode_len:25\r\n",
      "kl:0.03593,lr_multiplier:0.198,loss:[3.3739026],entropy:2.9083118438720703,explained_var_old:0.364,explained_var_new:0.525\r\n",
      "loss :[3.3739026], entropy:2.9083118438720703\r\n",
      "batch i:212, episode_len:22\r\n",
      "kl:0.01815,lr_multiplier:0.198,loss:[3.3997498],entropy:2.9410812854766846,explained_var_old:0.444,explained_var_new:0.545\r\n",
      "loss :[3.3997498], entropy:2.9410812854766846\r\n",
      "batch i:213, episode_len:22\r\n",
      "kl:0.02944,lr_multiplier:0.198,loss:[3.3729084],entropy:3.0003390312194824,explained_var_old:0.422,explained_var_new:0.541\r\n",
      "loss :[3.3729084], entropy:3.0003390312194824\r\n",
      "batch i:214, episode_len:21\r\n",
      "kl:0.05248,lr_multiplier:0.132,loss:[3.3772163],entropy:2.862567901611328,explained_var_old:0.425,explained_var_new:0.518\r\n",
      "loss :[3.3772163], entropy:2.862567901611328\r\n",
      "batch i:215, episode_len:14\r\n",
      "kl:0.03783,lr_multiplier:0.132,loss:[3.346579],entropy:2.929163694381714,explained_var_old:0.482,explained_var_new:0.537\r\n",
      "loss :[3.346579], entropy:2.929163694381714\r\n",
      "batch i:216, episode_len:12\r\n",
      "kl:0.02112,lr_multiplier:0.132,loss:[3.312234],entropy:2.912292957305908,explained_var_old:0.479,explained_var_new:0.560\r\n",
      "loss :[3.312234], entropy:2.912292957305908\r\n",
      "batch i:217, episode_len:27\r\n",
      "kl:0.01630,lr_multiplier:0.132,loss:[3.4351785],entropy:2.8766660690307617,explained_var_old:0.410,explained_var_new:0.488\r\n",
      "loss :[3.4351785], entropy:2.8766660690307617\r\n",
      "batch i:218, episode_len:13\r\n",
      "kl:0.02230,lr_multiplier:0.132,loss:[3.356534],entropy:2.8806681632995605,explained_var_old:0.406,explained_var_new:0.517\r\n",
      "loss :[3.356534], entropy:2.8806681632995605\r\n",
      "batch i:219, episode_len:15\r\n",
      "kl:0.01690,lr_multiplier:0.132,loss:[3.3824368],entropy:2.9061827659606934,explained_var_old:0.437,explained_var_new:0.531\r\n",
      "loss :[3.3824368], entropy:2.9061827659606934\r\n",
      "batch i:220, episode_len:15\r\n",
      "kl:0.01731,lr_multiplier:0.132,loss:[3.3429556],entropy:2.8690743446350098,explained_var_old:0.426,explained_var_new:0.503\r\n",
      "loss :[3.3429556], entropy:2.8690743446350098\r\n",
      "batch i:221, episode_len:21\r\n",
      "kl:0.01421,lr_multiplier:0.132,loss:[3.351256],entropy:2.9155259132385254,explained_var_old:0.466,explained_var_new:0.532\r\n",
      "loss :[3.351256], entropy:2.9155259132385254\r\n",
      "batch i:222, episode_len:16\r\n",
      "kl:0.01468,lr_multiplier:0.132,loss:[3.3201194],entropy:2.8660051822662354,explained_var_old:0.501,explained_var_new:0.558\r\n",
      "loss :[3.3201194], entropy:2.8660051822662354\r\n",
      "batch i:223, episode_len:22\r\n",
      "kl:0.01505,lr_multiplier:0.132,loss:[3.2796445],entropy:2.8471155166625977,explained_var_old:0.492,explained_var_new:0.565\r\n",
      "loss :[3.2796445], entropy:2.8471155166625977\r\n",
      "batch i:224, episode_len:20\r\n",
      "kl:0.00974,lr_multiplier:0.198,loss:[3.220427],entropy:2.89033579826355,explained_var_old:0.566,explained_var_new:0.625\r\n",
      "loss :[3.220427], entropy:2.89033579826355\r\n",
      "batch i:225, episode_len:10\r\n",
      "kl:0.01792,lr_multiplier:0.198,loss:[3.2722821],entropy:2.8775522708892822,explained_var_old:0.531,explained_var_new:0.618\r\n",
      "loss :[3.2722821], entropy:2.8775522708892822\r\n",
      "batch i:361, episode_len:36\r\n",
      "kl:0.01404,lr_multiplier:0.132,loss:[3.0522912],entropy:2.5903429985046387,explained_var_old:0.440,explained_var_new:0.527\r\n",
      "loss :[3.0522912], entropy:2.5903429985046387\r\n",
      "batch i:362, episode_len:19\r\n",
      "kl:0.01869,lr_multiplier:0.132,loss:[3.0001445],entropy:2.5762081146240234,explained_var_old:0.513,explained_var_new:0.578\r\n",
      "loss :[3.0001445], entropy:2.5762081146240234\r\n",
      "batch i:363, episode_len:40\r\n",
      "kl:0.01649,lr_multiplier:0.132,loss:[2.995224],entropy:2.550962209701538,explained_var_old:0.464,explained_var_new:0.537\r\n",
      "loss :[2.995224], entropy:2.550962209701538\r\n",
      "batch i:364, episode_len:9\r\n",
      "kl:0.01845,lr_multiplier:0.132,loss:[3.0042207],entropy:2.549238681793213,explained_var_old:0.479,explained_var_new:0.543\r\n",
      "loss :[3.0042207], entropy:2.549238681793213\r\n",
      "batch i:365, episode_len:21\r\n",
      "kl:0.02844,lr_multiplier:0.132,loss:[2.8963878],entropy:2.588254451751709,explained_var_old:0.533,explained_var_new:0.624\r\n",
      "loss :[2.8963878], entropy:2.588254451751709\r\n",
      "batch i:366, episode_len:25\r\n",
      "kl:0.02527,lr_multiplier:0.132,loss:[2.9999113],entropy:2.608123302459717,explained_var_old:0.512,explained_var_new:0.590\r\n",
      "loss :[2.9999113], entropy:2.608123302459717\r\n",
      "batch i:367, episode_len:29\r\n",
      "kl:0.01576,lr_multiplier:0.132,loss:[2.9955926],entropy:2.5736465454101562,explained_var_old:0.508,explained_var_new:0.591\r\n",
      "loss :[2.9955926], entropy:2.5736465454101562\r\n",
      "batch i:368, episode_len:17\r\n",
      "kl:0.01805,lr_multiplier:0.132,loss:[2.9636908],entropy:2.5490026473999023,explained_var_old:0.536,explained_var_new:0.578\r\n",
      "loss :[2.9636908], entropy:2.5490026473999023\r\n",
      "batch i:369, episode_len:18\r\n",
      "kl:0.01455,lr_multiplier:0.132,loss:[2.9813752],entropy:2.582477569580078,explained_var_old:0.564,explained_var_new:0.617\r\n",
      "loss :[2.9813752], entropy:2.582477569580078\r\n",
      "batch i:370, episode_len:11\r\n",
      "kl:0.03065,lr_multiplier:0.132,loss:[3.0729709],entropy:2.627500534057617,explained_var_old:0.470,explained_var_new:0.542\r\n",
      "loss :[3.0729709], entropy:2.627500534057617\r\n",
      "batch i:371, episode_len:9\r\n",
      "kl:0.01906,lr_multiplier:0.132,loss:[3.028552],entropy:2.637159824371338,explained_var_old:0.502,explained_var_new:0.587\r\n",
      "loss :[3.028552], entropy:2.637159824371338\r\n",
      "batch i:372, episode_len:37\r\n",
      "kl:0.01907,lr_multiplier:0.132,loss:[2.9818137],entropy:2.5989785194396973,explained_var_old:0.547,explained_var_new:0.617\r\n",
      "loss :[2.9818137], entropy:2.5989785194396973\r\n",
      "batch i:373, episode_len:28\r\n",
      "kl:0.01660,lr_multiplier:0.132,loss:[2.9361038],entropy:2.5874171257019043,explained_var_old:0.561,explained_var_new:0.604\r\n",
      "loss :[2.9361038], entropy:2.5874171257019043\r\n",
      "batch i:374, episode_len:20\r\n",
      "kl:0.01852,lr_multiplier:0.132,loss:[2.9317048],entropy:2.5787906646728516,explained_var_old:0.537,explained_var_new:0.605\r\n",
      "loss :[2.9317048], entropy:2.5787906646728516\r\n",
      "batch i:375, episode_len:12\r\n",
      "kl:0.01214,lr_multiplier:0.132,loss:[2.996931],entropy:2.5766477584838867,explained_var_old:0.498,explained_var_new:0.573\r\n",
      "loss :[2.996931], entropy:2.5766477584838867\r\n",
      "batch i:376, episode_len:22\r\n",
      "kl:0.02184,lr_multiplier:0.132,loss:[2.9893239],entropy:2.589205265045166,explained_var_old:0.519,explained_var_new:0.589\r\n",
      "loss :[2.9893239], entropy:2.589205265045166\r\n",
      "batch i:377, episode_len:11\r\n",
      "kl:0.01964,lr_multiplier:0.132,loss:[2.9774425],entropy:2.5129120349884033,explained_var_old:0.465,explained_var_new:0.541\r\n",
      "loss :[2.9774425], entropy:2.5129120349884033\r\n",
      "batch i:378, episode_len:11\r\n",
      "kl:0.02010,lr_multiplier:0.132,loss:[3.0587485],entropy:2.633042335510254,explained_var_old:0.490,explained_var_new:0.572\r\n",
      "loss :[3.0587485], entropy:2.633042335510254\r\n",
      "batch i:379, episode_len:11\r\n",
      "kl:0.01243,lr_multiplier:0.132,loss:[2.9753275],entropy:2.5304722785949707,explained_var_old:0.492,explained_var_new:0.554\r\n",
      "loss :[2.9753275], entropy:2.5304722785949707\r\n",
      "batch i:380, episode_len:9\r\n",
      "kl:0.01245,lr_multiplier:0.132,loss:[3.0598571],entropy:2.610997200012207,explained_var_old:0.474,explained_var_new:0.538\r\n",
      "loss :[3.0598571], entropy:2.610997200012207\r\n",
      "batch i:381, episode_len:9\r\n",
      "kl:0.01288,lr_multiplier:0.132,loss:[2.9385972],entropy:2.5410819053649902,explained_var_old:0.522,explained_var_new:0.598\r\n",
      "loss :[2.9385972], entropy:2.5410819053649902\r\n",
      "batch i:382, episode_len:19\r\n",
      "kl:0.02280,lr_multiplier:0.132,loss:[2.9953568],entropy:2.5374999046325684,explained_var_old:0.476,explained_var_new:0.542\r\n",
      "loss :[2.9953568], entropy:2.5374999046325684\r\n",
      "batch i:383, episode_len:15\r\n",
      "kl:0.01868,lr_multiplier:0.132,loss:[3.0756936],entropy:2.5771772861480713,explained_var_old:0.454,explained_var_new:0.507\r\n",
      "loss :[3.0756936], entropy:2.5771772861480713\r\n",
      "batch i:384, episode_len:17\r\n",
      "kl:0.02466,lr_multiplier:0.132,loss:[2.9220989],entropy:2.5577392578125,explained_var_old:0.541,explained_var_new:0.622\r\n",
      "loss :[2.9220989], entropy:2.5577392578125\r\n",
      "batch i:385, episode_len:28\r\n",
      "kl:0.01414,lr_multiplier:0.132,loss:[3.0153198],entropy:2.572713851928711,explained_var_old:0.467,explained_var_new:0.527\r\n",
      "loss :[3.0153198], entropy:2.572713851928711\r\n",
      "batch i:386, episode_len:41\r\n",
      "kl:0.02362,lr_multiplier:0.132,loss:[2.9458373],entropy:2.5687975883483887,explained_var_old:0.527,explained_var_new:0.574\r\n",
      "loss :[2.9458373], entropy:2.5687975883483887\r\n",
      "batch i:387, episode_len:24\r\n",
      "kl:0.02079,lr_multiplier:0.132,loss:[3.036737],entropy:2.5952179431915283,explained_var_old:0.471,explained_var_new:0.557\r\n",
      "loss :[3.036737], entropy:2.5952179431915283\r\n",
      "batch i:388, episode_len:23\r\n",
      "kl:0.02200,lr_multiplier:0.132,loss:[2.8494942],entropy:2.4891128540039062,explained_var_old:0.576,explained_var_new:0.635\r\n",
      "loss :[2.8494942], entropy:2.4891128540039062\r\n",
      "batch i:389, episode_len:19\r\n",
      "kl:0.02684,lr_multiplier:0.132,loss:[2.9649134],entropy:2.552816867828369,explained_var_old:0.493,explained_var_new:0.575\r\n",
      "loss :[2.9649134], entropy:2.552816867828369\r\n",
      "batch i:390, episode_len:18\r\n",
      "kl:0.01913,lr_multiplier:0.132,loss:[3.0329297],entropy:2.5536928176879883,explained_var_old:0.445,explained_var_new:0.505\r\n",
      "loss :[3.0329297], entropy:2.5536928176879883\r\n",
      "batch i:391, episode_len:16\r\n",
      "kl:0.02167,lr_multiplier:0.132,loss:[2.9853964],entropy:2.4979119300842285,explained_var_old:0.434,explained_var_new:0.516\r\n",
      "loss :[2.9853964], entropy:2.4979119300842285\r\n",
      "batch i:392, episode_len:13\r\n",
      "kl:0.01876,lr_multiplier:0.132,loss:[2.9940124],entropy:2.5527291297912598,explained_var_old:0.448,explained_var_new:0.516\r\n",
      "loss :[2.9940124], entropy:2.5527291297912598\r\n",
      "batch i:393, episode_len:13\r\n",
      "kl:0.02343,lr_multiplier:0.132,loss:[3.0660663],entropy:2.5536751747131348,explained_var_old:0.409,explained_var_new:0.464\r\n",
      "loss :[3.0660663], entropy:2.5536751747131348\r\n",
      "batch i:394, episode_len:17\r\n",
      "kl:0.02273,lr_multiplier:0.132,loss:[3.0048103],entropy:2.534705877304077,explained_var_old:0.424,explained_var_new:0.480\r\n",
      "loss :[3.0048103], entropy:2.534705877304077\r\n",
      "batch i:395, episode_len:21\r\n",
      "kl:0.01886,lr_multiplier:0.132,loss:[2.9079623],entropy:2.4802024364471436,explained_var_old:0.510,explained_var_new:0.574\r\n",
      "loss :[2.9079623], entropy:2.4802024364471436\r\n",
      "batch i:396, episode_len:42\r\n",
      "kl:0.02034,lr_multiplier:0.132,loss:[2.9316854],entropy:2.509450912475586,explained_var_old:0.505,explained_var_new:0.563\r\n",
      "loss :[2.9316854], entropy:2.509450912475586\r\n",
      "batch i:397, episode_len:21\r\n",
      "kl:0.02239,lr_multiplier:0.132,loss:[3.06504],entropy:2.5504558086395264,explained_var_old:0.401,explained_var_new:0.473\r\n",
      "loss :[3.06504], entropy:2.5504558086395264\r\n",
      "batch i:398, episode_len:24\r\n",
      "kl:0.01955,lr_multiplier:0.132,loss:[3.2410662],entropy:2.607607841491699,explained_var_old:0.308,explained_var_new:0.401\r\n",
      "loss :[3.2410662], entropy:2.607607841491699\r\n",
      "batch i:399, episode_len:22\r\n",
      "kl:0.01763,lr_multiplier:0.132,loss:[3.10983],entropy:2.575453042984009,explained_var_old:0.314,explained_var_new:0.410\r\n",
      "loss :[3.10983], entropy:2.575453042984009\r\n",
      "batch i:400, episode_len:13\r\n",
      "kl:0.02933,lr_multiplier:0.132,loss:[3.0982563],entropy:2.5042574405670166,explained_var_old:0.297,explained_var_new:0.410\r\n",
      "loss :[3.0982563], entropy:2.5042574405670166\r\n",
      "current self-play batch: 400\r\n",
      "num_playouts:1000, win: 6, lose: 4, tie:0\r\n",
      "batch i:401, episode_len:18\r\n",
      "kl:0.02936,lr_multiplier:0.132,loss:[3.078248],entropy:2.582731246948242,explained_var_old:0.380,explained_var_new:0.463\r\n",
      "loss :[3.078248], entropy:2.582731246948242\r\n",
      "batch i:402, episode_len:15\r\n",
      "kl:0.01768,lr_multiplier:0.132,loss:[3.0469813],entropy:2.5350966453552246,explained_var_old:0.369,explained_var_new:0.443\r\n",
      "loss :[3.0469813], entropy:2.5350966453552246\r\n",
      "batch i:403, episode_len:22\r\n",
      "kl:0.02486,lr_multiplier:0.132,loss:[3.0821397],entropy:2.467891216278076,explained_var_old:0.302,explained_var_new:0.365\r\n",
      "loss :[3.0821397], entropy:2.467891216278076\r\n",
      "batch i:404, episode_len:13\r\n",
      "kl:0.02280,lr_multiplier:0.132,loss:[3.0050867],entropy:2.4339723587036133,explained_var_old:0.382,explained_var_new:0.458\r\n",
      "loss :[3.0050867], entropy:2.4339723587036133\r\n",
      "batch i:405, episode_len:32\r\n",
      "kl:0.02650,lr_multiplier:0.132,loss:[3.093439],entropy:2.5215747356414795,explained_var_old:0.364,explained_var_new:0.438\r\n",
      "loss :[3.093439], entropy:2.5215747356414795\r\n",
      "batch i:406, episode_len:31\r\n",
      "kl:0.01470,lr_multiplier:0.132,loss:[3.0887167],entropy:2.5524065494537354,explained_var_old:0.356,explained_var_new:0.443\r\n",
      "loss :[3.0887167], entropy:2.5524065494537354\r\n",
      "batch i:407, episode_len:13\r\n",
      "kl:0.01942,lr_multiplier:0.132,loss:[3.1507936],entropy:2.5602383613586426,explained_var_old:0.301,explained_var_new:0.365\r\n",
      "loss :[3.1507936], entropy:2.5602383613586426\r\n",
      "batch i:408, episode_len:20\r\n",
      "kl:0.01277,lr_multiplier:0.132,loss:[3.1306067],entropy:2.529663562774658,explained_var_old:0.263,explained_var_new:0.342\r\n",
      "loss :[3.1306067], entropy:2.529663562774658\r\n",
      "batch i:409, episode_len:36\r\n",
      "kl:0.02617,lr_multiplier:0.132,loss:[3.268289],entropy:2.492748737335205,explained_var_old:0.181,explained_var_new:0.263\r\n",
      "loss :[3.268289], entropy:2.492748737335205\r\n",
      "batch i:410, episode_len:22\r\n",
      "kl:0.02903,lr_multiplier:0.132,loss:[3.1138072],entropy:2.5468785762786865,explained_var_old:0.298,explained_var_new:0.375\r\n",
      "loss :[3.1138072], entropy:2.5468785762786865\r\n",
      "batch i:411, episode_len:12\r\n",
      "kl:0.02263,lr_multiplier:0.132,loss:[3.140959],entropy:2.497349739074707,explained_var_old:0.278,explained_var_new:0.346\r\n",
      "loss :[3.140959], entropy:2.497349739074707\r\n",
      "batch i:412, episode_len:19\r\n",
      "kl:0.01938,lr_multiplier:0.132,loss:[3.1226237],entropy:2.519125461578369,explained_var_old:0.290,explained_var_new:0.365\r\n",
      "loss :[3.1226237], entropy:2.519125461578369\r\n",
      "batch i:413, episode_len:15\r\n",
      "kl:0.02529,lr_multiplier:0.132,loss:[3.046763],entropy:2.3990063667297363,explained_var_old:0.273,explained_var_new:0.341\r\n",
      "loss :[3.046763], entropy:2.3990063667297363\r\n",
      "batch i:414, episode_len:13\r\n",
      "kl:0.02097,lr_multiplier:0.132,loss:[3.090408],entropy:2.4669158458709717,explained_var_old:0.283,explained_var_new:0.364\r\n",
      "loss :[3.090408], entropy:2.4669158458709717\r\n",
      "batch i:415, episode_len:15\r\n",
      "kl:0.01832,lr_multiplier:0.132,loss:[3.044603],entropy:2.463289737701416,explained_var_old:0.353,explained_var_new:0.427\r\n",
      "loss :[3.044603], entropy:2.463289737701416\r\n",
      "batch i:416, episode_len:13\r\n",
      "kl:0.02282,lr_multiplier:0.132,loss:[3.023778],entropy:2.492452383041382,explained_var_old:0.354,explained_var_new:0.435\r\n",
      "loss :[3.023778], entropy:2.492452383041382\r\n",
      "batch i:417, episode_len:34\r\n",
      "kl:0.02928,lr_multiplier:0.132,loss:[3.0993745],entropy:2.5260605812072754,explained_var_old:0.318,explained_var_new:0.388\r\n",
      "loss :[3.0993745], entropy:2.5260605812072754\r\n",
      "batch i:418, episode_len:15\r\n",
      "kl:0.01903,lr_multiplier:0.132,loss:[3.0425887],entropy:2.5140364170074463,explained_var_old:0.374,explained_var_new:0.452\r\n",
      "loss :[3.0425887], entropy:2.5140364170074463\r\n",
      "batch i:419, episode_len:37\r\n",
      "kl:0.03179,lr_multiplier:0.132,loss:[3.020031],entropy:2.4562625885009766,explained_var_old:0.312,explained_var_new:0.401\r\n",
      "loss :[3.020031], entropy:2.4562625885009766\r\n",
      "batch i:420, episode_len:11\r\n",
      "kl:0.01528,lr_multiplier:0.132,loss:[3.0185204],entropy:2.5149593353271484,explained_var_old:0.384,explained_var_new:0.486\r\n",
      "loss :[3.0185204], entropy:2.5149593353271484\r\n",
      "batch i:421, episode_len:21\r\n",
      "kl:0.01946,lr_multiplier:0.132,loss:[3.0272899],entropy:2.4806671142578125,explained_var_old:0.390,explained_var_new:0.444\r\n",
      "loss :[3.0272899], entropy:2.4806671142578125\r\n",
      "batch i:422, episode_len:19\r\n",
      "kl:0.01921,lr_multiplier:0.132,loss:[3.1233552],entropy:2.5440125465393066,explained_var_old:0.305,explained_var_new:0.397\r\n",
      "loss :[3.1233552], entropy:2.5440125465393066\r\n",
      "batch i:423, episode_len:16\r\n",
      "kl:0.02319,lr_multiplier:0.132,loss:[3.03342],entropy:2.4720962047576904,explained_var_old:0.353,explained_var_new:0.430\r\n",
      "loss :[3.03342], entropy:2.4720962047576904\r\n",
      "batch i:424, episode_len:25\r\n",
      "kl:0.01426,lr_multiplier:0.132,loss:[3.0616817],entropy:2.533317804336548,explained_var_old:0.419,explained_var_new:0.481\r\n",
      "loss :[3.0616817], entropy:2.533317804336548\r\n",
      "batch i:425, episode_len:17\r\n",
      "kl:0.01488,lr_multiplier:0.132,loss:[3.0382342],entropy:2.5415382385253906,explained_var_old:0.382,explained_var_new:0.441\r\n",
      "loss :[3.0382342], entropy:2.5415382385253906\r\n",
      "batch i:426, episode_len:19\r\n",
      "kl:0.01534,lr_multiplier:0.132,loss:[2.971508],entropy:2.4395828247070312,explained_var_old:0.369,explained_var_new:0.452\r\n",
      "loss :[2.971508], entropy:2.4395828247070312\r\n",
      "batch i:427, episode_len:12\r\n",
      "kl:0.02153,lr_multiplier:0.132,loss:[3.0554612],entropy:2.4394636154174805,explained_var_old:0.352,explained_var_new:0.408\r\n",
      "loss :[3.0554612], entropy:2.4394636154174805\r\n",
      "batch i:428, episode_len:17\r\n",
      "kl:0.02999,lr_multiplier:0.132,loss:[2.996073],entropy:2.511186361312866,explained_var_old:0.413,explained_var_new:0.475\r\n",
      "loss :[2.996073], entropy:2.511186361312866\r\n",
      "batch i:429, episode_len:21\r\n",
      "kl:0.02903,lr_multiplier:0.132,loss:[3.030727],entropy:2.5099613666534424,explained_var_old:0.413,explained_var_new:0.462\r\n",
      "loss :[3.030727], entropy:2.5099613666534424\r\n",
      "batch i:430, episode_len:14\r\n",
      "kl:0.02985,lr_multiplier:0.132,loss:[2.9387493],entropy:2.4075050354003906,explained_var_old:0.351,explained_var_new:0.481\r\n",
      "loss :[2.9387493], entropy:2.4075050354003906\r\n",
      "batch i:431, episode_len:13\r\n",
      "kl:0.02053,lr_multiplier:0.132,loss:[3.0364099],entropy:2.4294285774230957,explained_var_old:0.354,explained_var_new:0.437\r\n",
      "loss :[3.0364099], entropy:2.4294285774230957\r\n",
      "batch i:432, episode_len:29\r\n",
      "kl:0.01516,lr_multiplier:0.132,loss:[2.9944193],entropy:2.436310291290283,explained_var_old:0.325,explained_var_new:0.411\r\n",
      "loss :[2.9944193], entropy:2.436310291290283\r\n",
      "batch i:433, episode_len:12\r\n",
      "kl:0.02250,lr_multiplier:0.132,loss:[3.0332668],entropy:2.4714126586914062,explained_var_old:0.366,explained_var_new:0.429\r\n",
      "loss :[3.0332668], entropy:2.4714126586914062\r\n",
      "batch i:434, episode_len:15\r\n",
      "kl:0.02191,lr_multiplier:0.132,loss:[2.9557943],entropy:2.4351181983947754,explained_var_old:0.387,explained_var_new:0.455\r\n",
      "loss :[2.9557943], entropy:2.4351181983947754\r\n",
      "batch i:435, episode_len:35\r\n",
      "kl:0.01275,lr_multiplier:0.132,loss:[3.0493546],entropy:2.4668989181518555,explained_var_old:0.330,explained_var_new:0.421\r\n",
      "loss :[3.0493546], entropy:2.4668989181518555\r\n",
      "batch i:436, episode_len:18\r\n",
      "kl:0.01564,lr_multiplier:0.132,loss:[3.048617],entropy:2.5153555870056152,explained_var_old:0.314,explained_var_new:0.386\r\n",
      "loss :[3.048617], entropy:2.5153555870056152\r\n",
      "batch i:437, episode_len:21\r\n",
      "kl:0.01943,lr_multiplier:0.132,loss:[3.0651407],entropy:2.4430642127990723,explained_var_old:0.318,explained_var_new:0.389\r\n",
      "loss :[3.0651407], entropy:2.4430642127990723\r\n",
      "batch i:438, episode_len:42\r\n",
      "kl:0.01056,lr_multiplier:0.132,loss:[3.0191925],entropy:2.4297738075256348,explained_var_old:0.312,explained_var_new:0.389\r\n",
      "loss :[3.0191925], entropy:2.4297738075256348\r\n",
      "batch i:439, episode_len:11\r\n",
      "kl:0.01594,lr_multiplier:0.132,loss:[3.0540352],entropy:2.4451568126678467,explained_var_old:0.328,explained_var_new:0.398\r\n",
      "loss :[3.0540352], entropy:2.4451568126678467\r\n",
      "batch i:440, episode_len:15\r\n",
      "kl:0.02673,lr_multiplier:0.132,loss:[3.0929928],entropy:2.4169089794158936,explained_var_old:0.281,explained_var_new:0.374\r\n",
      "loss :[3.0929928], entropy:2.4169089794158936\r\n",
      "batch i:441, episode_len:29\r\n",
      "kl:0.03159,lr_multiplier:0.132,loss:[3.0436723],entropy:2.420365333557129,explained_var_old:0.341,explained_var_new:0.400\r\n",
      "loss :[3.0436723], entropy:2.420365333557129\r\n",
      "batch i:442, episode_len:13\r\n",
      "kl:0.03013,lr_multiplier:0.132,loss:[3.0149508],entropy:2.522286891937256,explained_var_old:0.372,explained_var_new:0.448\r\n",
      "loss :[3.0149508], entropy:2.522286891937256\r\n",
      "batch i:443, episode_len:9\r\n",
      "kl:0.02197,lr_multiplier:0.132,loss:[3.0782967],entropy:2.4665260314941406,explained_var_old:0.319,explained_var_new:0.387\r\n",
      "loss :[3.0782967], entropy:2.4665260314941406\r\n",
      "batch i:444, episode_len:32\r\n",
      "kl:0.02416,lr_multiplier:0.132,loss:[2.9563174],entropy:2.508058547973633,explained_var_old:0.435,explained_var_new:0.489\r\n",
      "loss :[2.9563174], entropy:2.508058547973633\r\n",
      "batch i:445, episode_len:22\r\n",
      "kl:0.01755,lr_multiplier:0.132,loss:[3.1357775],entropy:2.4692628383636475,explained_var_old:0.279,explained_var_new:0.354\r\n",
      "loss :[3.1357775], entropy:2.4692628383636475\r\n",
      "batch i:446, episode_len:43\r\n",
      "kl:0.01911,lr_multiplier:0.132,loss:[3.1482644],entropy:2.4863781929016113,explained_var_old:0.258,explained_var_new:0.364\r\n",
      "loss :[3.1482644], entropy:2.4863781929016113\r\n",
      "batch i:447, episode_len:11\r\n",
      "kl:0.01844,lr_multiplier:0.132,loss:[3.0869749],entropy:2.501525402069092,explained_var_old:0.346,explained_var_new:0.412\r\n",
      "loss :[3.0869749], entropy:2.501525402069092\r\n",
      "batch i:448, episode_len:21\r\n",
      "kl:0.02168,lr_multiplier:0.132,loss:[2.9863005],entropy:2.4709725379943848,explained_var_old:0.360,explained_var_new:0.464\r\n",
      "loss :[2.9863005], entropy:2.4709725379943848\r\n",
      "batch i:449, episode_len:22\r\n",
      "kl:0.02705,lr_multiplier:0.132,loss:[3.110519],entropy:2.508591651916504,explained_var_old:0.311,explained_var_new:0.397\r\n",
      "loss :[3.110519], entropy:2.508591651916504\r\n",
      "batch i:450, episode_len:11\r\n",
      "kl:0.02686,lr_multiplier:0.132,loss:[3.091069],entropy:2.446415901184082,explained_var_old:0.263,explained_var_new:0.345\r\n",
      "loss :[3.091069], entropy:2.446415901184082\r\n",
      "batch i:451, episode_len:19\r\n",
      "kl:0.03726,lr_multiplier:0.132,loss:[2.955666],entropy:2.455524444580078,explained_var_old:0.405,explained_var_new:0.478\r\n",
      "loss :[2.955666], entropy:2.455524444580078\r\n",
      "batch i:452, episode_len:16\r\n",
      "kl:0.01503,lr_multiplier:0.132,loss:[2.9695961],entropy:2.4704179763793945,explained_var_old:0.418,explained_var_new:0.494\r\n",
      "loss :[2.9695961], entropy:2.4704179763793945\r\n",
      "batch i:453, episode_len:34\r\n",
      "kl:0.01816,lr_multiplier:0.132,loss:[3.0645456],entropy:2.535371780395508,explained_var_old:0.351,explained_var_new:0.444\r\n",
      "loss :[3.0645456], entropy:2.535371780395508\r\n",
      "batch i:454, episode_len:19\r\n",
      "kl:0.01639,lr_multiplier:0.132,loss:[3.057404],entropy:2.5501832962036133,explained_var_old:0.367,explained_var_new:0.447\r\n",
      "loss :[3.057404], entropy:2.5501832962036133\r\n",
      "batch i:455, episode_len:40\r\n",
      "kl:0.02126,lr_multiplier:0.132,loss:[3.0685902],entropy:2.5430994033813477,explained_var_old:0.377,explained_var_new:0.440\r\n",
      "loss :[3.0685902], entropy:2.5430994033813477\r\n",
      "batch i:456, episode_len:17\r\n",
      "kl:0.02277,lr_multiplier:0.132,loss:[3.007358],entropy:2.4474754333496094,explained_var_old:0.348,explained_var_new:0.419\r\n",
      "loss :[3.007358], entropy:2.4474754333496094\r\n",
      "batch i:457, episode_len:19\r\n",
      "kl:0.02117,lr_multiplier:0.132,loss:[3.0939584],entropy:2.4957218170166016,explained_var_old:0.305,explained_var_new:0.416\r\n",
      "loss :[3.0939584], entropy:2.4957218170166016\r\n",
      "batch i:458, episode_len:37\r\n",
      "kl:0.02394,lr_multiplier:0.132,loss:[3.0726724],entropy:2.5047898292541504,explained_var_old:0.360,explained_var_new:0.436\r\n",
      "loss :[3.0726724], entropy:2.5047898292541504\r\n",
      "batch i:459, episode_len:27\r\n",
      "kl:0.02027,lr_multiplier:0.132,loss:[3.0941749],entropy:2.5414676666259766,explained_var_old:0.332,explained_var_new:0.413\r\n",
      "loss :[3.0941749], entropy:2.5414676666259766\r\n",
      "batch i:460, episode_len:13\r\n",
      "kl:0.01888,lr_multiplier:0.132,loss:[3.0544236],entropy:2.4788761138916016,explained_var_old:0.326,explained_var_new:0.410\r\n",
      "loss :[3.0544236], entropy:2.4788761138916016\r\n",
      "batch i:461, episode_len:18\r\n",
      "kl:0.01566,lr_multiplier:0.132,loss:[3.021997],entropy:2.5223283767700195,explained_var_old:0.311,explained_var_new:0.434\r\n",
      "loss :[3.021997], entropy:2.5223283767700195\r\n",
      "batch i:462, episode_len:17\r\n",
      "kl:0.01849,lr_multiplier:0.132,loss:[3.061675],entropy:2.5001726150512695,explained_var_old:0.369,explained_var_new:0.442\r\n",
      "loss :[3.061675], entropy:2.5001726150512695\r\n",
      "batch i:463, episode_len:12\r\n",
      "kl:0.02250,lr_multiplier:0.132,loss:[3.010841],entropy:2.4848270416259766,explained_var_old:0.415,explained_var_new:0.470\r\n",
      "loss :[3.010841], entropy:2.4848270416259766\r\n",
      "batch i:464, episode_len:25\r\n",
      "kl:0.02537,lr_multiplier:0.132,loss:[3.0425172],entropy:2.450150489807129,explained_var_old:0.404,explained_var_new:0.467\r\n",
      "loss :[3.0425172], entropy:2.450150489807129\r\n",
      "batch i:465, episode_len:16\r\n",
      "kl:0.02196,lr_multiplier:0.132,loss:[2.9997334],entropy:2.50765061378479,explained_var_old:0.424,explained_var_new:0.506\r\n",
      "loss :[2.9997334], entropy:2.50765061378479\r\n",
      "batch i:466, episode_len:26\r\n",
      "kl:0.02494,lr_multiplier:0.132,loss:[2.976676],entropy:2.4501166343688965,explained_var_old:0.404,explained_var_new:0.479\r\n",
      "loss :[2.976676], entropy:2.4501166343688965\r\n",
      "batch i:467, episode_len:12\r\n",
      "kl:0.02499,lr_multiplier:0.132,loss:[2.9956112],entropy:2.463731288909912,explained_var_old:0.370,explained_var_new:0.468\r\n",
      "loss :[2.9956112], entropy:2.463731288909912\r\n",
      "batch i:468, episode_len:22\r\n",
      "kl:0.01173,lr_multiplier:0.132,loss:[2.9905565],entropy:2.4283785820007324,explained_var_old:0.369,explained_var_new:0.457\r\n",
      "loss :[2.9905565], entropy:2.4283785820007324\r\n",
      "batch i:469, episode_len:32\r\n",
      "kl:0.01444,lr_multiplier:0.132,loss:[2.9949365],entropy:2.489748239517212,explained_var_old:0.387,explained_var_new:0.463\r\n",
      "loss :[2.9949365], entropy:2.489748239517212\r\n",
      "batch i:470, episode_len:16\r\n",
      "kl:0.01120,lr_multiplier:0.132,loss:[3.0145094],entropy:2.418280601501465,explained_var_old:0.312,explained_var_new:0.387\r\n",
      "loss :[3.0145094], entropy:2.418280601501465\r\n",
      "batch i:471, episode_len:17\r\n",
      "kl:0.01072,lr_multiplier:0.132,loss:[2.9632165],entropy:2.4140162467956543,explained_var_old:0.367,explained_var_new:0.443\r\n",
      "loss :[2.9632165], entropy:2.4140162467956543\r\n",
      "batch i:472, episode_len:10\r\n",
      "kl:0.01562,lr_multiplier:0.132,loss:[3.0040739],entropy:2.4679245948791504,explained_var_old:0.366,explained_var_new:0.439\r\n",
      "loss :[3.0040739], entropy:2.4679245948791504\r\n",
      "batch i:473, episode_len:32\r\n",
      "kl:0.01017,lr_multiplier:0.132,loss:[2.930649],entropy:2.463813304901123,explained_var_old:0.448,explained_var_new:0.522\r\n",
      "loss :[2.930649], entropy:2.463813304901123\r\n",
      "batch i:474, episode_len:29\r\n",
      "kl:0.01518,lr_multiplier:0.132,loss:[2.946231],entropy:2.4372687339782715,explained_var_old:0.425,explained_var_new:0.485\r\n",
      "loss :[2.946231], entropy:2.4372687339782715\r\n",
      "batch i:475, episode_len:15\r\n",
      "kl:0.01768,lr_multiplier:0.132,loss:[3.0387754],entropy:2.5242350101470947,explained_var_old:0.396,explained_var_new:0.469\r\n",
      "loss :[3.0387754], entropy:2.5242350101470947\r\n",
      "batch i:476, episode_len:10\r\n",
      "kl:0.01351,lr_multiplier:0.132,loss:[2.9882574],entropy:2.472151517868042,explained_var_old:0.395,explained_var_new:0.471\r\n",
      "loss :[2.9882574], entropy:2.472151517868042\r\n",
      "batch i:477, episode_len:22\r\n",
      "kl:0.01973,lr_multiplier:0.132,loss:[2.9226902],entropy:2.4029736518859863,explained_var_old:0.452,explained_var_new:0.527\r\n",
      "loss :[2.9226902], entropy:2.4029736518859863\r\n",
      "batch i:478, episode_len:16\r\n",
      "kl:0.01474,lr_multiplier:0.132,loss:[2.8852265],entropy:2.425551414489746,explained_var_old:0.450,explained_var_new:0.540\r\n",
      "loss :[2.8852265], entropy:2.425551414489746\r\n",
      "batch i:479, episode_len:13\r\n",
      "kl:0.02655,lr_multiplier:0.132,loss:[2.9462643],entropy:2.4048707485198975,explained_var_old:0.400,explained_var_new:0.459\r\n",
      "loss :[2.9462643], entropy:2.4048707485198975\r\n",
      "batch i:480, episode_len:15\r\n",
      "kl:0.02033,lr_multiplier:0.132,loss:[2.9941974],entropy:2.4703381061553955,explained_var_old:0.340,explained_var_new:0.430\r\n",
      "loss :[2.9941974], entropy:2.4703381061553955\r\n",
      "batch i:481, episode_len:22\r\n",
      "kl:0.01827,lr_multiplier:0.132,loss:[3.0112443],entropy:2.4830808639526367,explained_var_old:0.379,explained_var_new:0.459\r\n",
      "loss :[3.0112443], entropy:2.4830808639526367\r\n",
      "batch i:482, episode_len:18\r\n",
      "kl:0.01681,lr_multiplier:0.132,loss:[2.973936],entropy:2.4101061820983887,explained_var_old:0.390,explained_var_new:0.475\r\n",
      "loss :[2.973936], entropy:2.4101061820983887\r\n",
      "batch i:483, episode_len:22\r\n",
      "kl:0.01478,lr_multiplier:0.132,loss:[2.9215274],entropy:2.4092659950256348,explained_var_old:0.397,explained_var_new:0.475\r\n",
      "loss :[2.9215274], entropy:2.4092659950256348\r\n",
      "batch i:484, episode_len:30\r\n",
      "kl:0.01809,lr_multiplier:0.132,loss:[2.990367],entropy:2.4172251224517822,explained_var_old:0.366,explained_var_new:0.440\r\n",
      "loss :[2.990367], entropy:2.4172251224517822\r\n",
      "batch i:485, episode_len:12\r\n",
      "kl:0.02287,lr_multiplier:0.132,loss:[2.9855332],entropy:2.4753403663635254,explained_var_old:0.400,explained_var_new:0.495\r\n",
      "loss :[2.9855332], entropy:2.4753403663635254\r\n",
      "batch i:486, episode_len:24\r\n",
      "kl:0.01630,lr_multiplier:0.132,loss:[2.9103565],entropy:2.470527172088623,explained_var_old:0.432,explained_var_new:0.494\r\n",
      "loss :[2.9103565], entropy:2.470527172088623\r\n",
      "batch i:487, episode_len:16\r\n",
      "kl:0.01857,lr_multiplier:0.132,loss:[2.90656],entropy:2.4604930877685547,explained_var_old:0.430,explained_var_new:0.506\r\n",
      "loss :[2.90656], entropy:2.4604930877685547\r\n",
      "batch i:488, episode_len:22\r\n",
      "kl:0.02192,lr_multiplier:0.132,loss:[2.8226106],entropy:2.4183669090270996,explained_var_old:0.500,explained_var_new:0.566\r\n",
      "loss :[2.8226106], entropy:2.4183669090270996\r\n",
      "batch i:489, episode_len:16\r\n",
      "kl:0.02391,lr_multiplier:0.132,loss:[2.9127347],entropy:2.394723892211914,explained_var_old:0.454,explained_var_new:0.529\r\n",
      "loss :[2.9127347], entropy:2.394723892211914\r\n",
      "batch i:490, episode_len:22\r\n",
      "kl:0.01906,lr_multiplier:0.132,loss:[2.8676612],entropy:2.395693778991699,explained_var_old:0.464,explained_var_new:0.536\r\n",
      "loss :[2.8676612], entropy:2.395693778991699\r\n",
      "batch i:491, episode_len:9\r\n",
      "kl:0.02261,lr_multiplier:0.132,loss:[2.941587],entropy:2.4808425903320312,explained_var_old:0.463,explained_var_new:0.522\r\n",
      "loss :[2.941587], entropy:2.4808425903320312\r\n",
      "batch i:492, episode_len:24\r\n",
      "kl:0.02176,lr_multiplier:0.132,loss:[2.8691192],entropy:2.438418388366699,explained_var_old:0.514,explained_var_new:0.581\r\n",
      "loss :[2.8691192], entropy:2.438418388366699\r\n",
      "batch i:493, episode_len:18\r\n",
      "kl:0.01509,lr_multiplier:0.132,loss:[2.885495],entropy:2.435591220855713,explained_var_old:0.437,explained_var_new:0.535\r\n",
      "loss :[2.885495], entropy:2.435591220855713\r\n",
      "batch i:494, episode_len:16\r\n",
      "kl:0.01626,lr_multiplier:0.132,loss:[2.9254842],entropy:2.425938606262207,explained_var_old:0.440,explained_var_new:0.522\r\n",
      "loss :[2.9254842], entropy:2.425938606262207\r\n",
      "batch i:495, episode_len:23\r\n",
      "kl:0.01355,lr_multiplier:0.132,loss:[2.8246334],entropy:2.423705577850342,explained_var_old:0.516,explained_var_new:0.563\r\n",
      "loss :[2.8246334], entropy:2.423705577850342\r\n",
      "batch i:496, episode_len:28\r\n",
      "kl:0.01579,lr_multiplier:0.132,loss:[2.8519475],entropy:2.424809217453003,explained_var_old:0.483,explained_var_new:0.554\r\n",
      "loss :[2.8519475], entropy:2.424809217453003\r\n",
      "batch i:497, episode_len:16\r\n",
      "kl:0.02200,lr_multiplier:0.132,loss:[2.8755],entropy:2.3990042209625244,explained_var_old:0.455,explained_var_new:0.511\r\n",
      "loss :[2.8755], entropy:2.3990042209625244\r\n",
      "batch i:498, episode_len:22\r\n",
      "kl:0.01680,lr_multiplier:0.132,loss:[2.809492],entropy:2.3843331336975098,explained_var_old:0.495,explained_var_new:0.558\r\n",
      "loss :[2.809492], entropy:2.3843331336975098\r\n",
      "batch i:499, episode_len:13\r\n",
      "kl:0.01864,lr_multiplier:0.132,loss:[2.8944635],entropy:2.3795437812805176,explained_var_old:0.429,explained_var_new:0.520\r\n",
      "loss :[2.8944635], entropy:2.3795437812805176\r\n",
      "batch i:500, episode_len:26\r\n",
      "kl:0.02063,lr_multiplier:0.132,loss:[2.9387207],entropy:2.427647829055786,explained_var_old:0.434,explained_var_new:0.486\r\n",
      "loss :[2.9387207], entropy:2.427647829055786\r\n",
      "current self-play batch: 500\r\n",
      "num_playouts:1000, win: 7, lose: 3, tie:0\r\n",
      "New best policy!!!!!!!!\r\n",
      "batch i:501, episode_len:25\r\n",
      "kl:0.02682,lr_multiplier:0.132,loss:[2.9653223],entropy:2.372123956680298,explained_var_old:0.400,explained_var_new:0.480\r\n",
      "loss :[2.9653223], entropy:2.372123956680298\r\n",
      "batch i:502, episode_len:9\r\n",
      "kl:0.01758,lr_multiplier:0.132,loss:[2.9223537],entropy:2.4618654251098633,explained_var_old:0.451,explained_var_new:0.519\r\n",
      "loss :[2.9223537], entropy:2.4618654251098633\r\n",
      "batch i:503, episode_len:20\r\n",
      "kl:0.01742,lr_multiplier:0.132,loss:[2.9973643],entropy:2.4635043144226074,explained_var_old:0.377,explained_var_new:0.434\r\n",
      "loss :[2.9973643], entropy:2.4635043144226074\r\n",
      "batch i:504, episode_len:13\r\n",
      "kl:0.01648,lr_multiplier:0.132,loss:[2.931833],entropy:2.465667486190796,explained_var_old:0.417,explained_var_new:0.500\r\n",
      "loss :[2.931833], entropy:2.465667486190796\r\n",
      "batch i:505, episode_len:16\r\n",
      "kl:0.02243,lr_multiplier:0.132,loss:[2.9569483],entropy:2.4833035469055176,explained_var_old:0.398,explained_var_new:0.471\r\n",
      "loss :[2.9569483], entropy:2.4833035469055176\r\n",
      "batch i:506, episode_len:32\r\n",
      "kl:0.02066,lr_multiplier:0.132,loss:[2.8180096],entropy:2.4010894298553467,explained_var_old:0.525,explained_var_new:0.578\r\n",
      "loss :[2.8180096], entropy:2.4010894298553467\r\n",
      "batch i:507, episode_len:22\r\n",
      "kl:0.01285,lr_multiplier:0.132,loss:[2.90025],entropy:2.445307731628418,explained_var_old:0.477,explained_var_new:0.539\r\n",
      "loss :[2.90025], entropy:2.445307731628418\r\n",
      "batch i:508, episode_len:9\r\n",
      "kl:0.01037,lr_multiplier:0.132,loss:[2.871237],entropy:2.4220592975616455,explained_var_old:0.468,explained_var_new:0.535\r\n",
      "loss :[2.871237], entropy:2.4220592975616455\r\n",
      "batch i:509, episode_len:22\r\n",
      "kl:0.01742,lr_multiplier:0.132,loss:[2.9338472],entropy:2.47566819190979,explained_var_old:0.465,explained_var_new:0.546\r\n",
      "loss :[2.9338472], entropy:2.47566819190979\r\n",
      "batch i:510, episode_len:16\r\n",
      "kl:0.01173,lr_multiplier:0.132,loss:[2.8983245],entropy:2.418541669845581,explained_var_old:0.400,explained_var_new:0.461\r\n",
      "loss :[2.8983245], entropy:2.418541669845581\r\n",
      "batch i:511, episode_len:15\r\n",
      "kl:0.01899,lr_multiplier:0.132,loss:[2.874127],entropy:2.4572250843048096,explained_var_old:0.469,explained_var_new:0.545\r\n",
      "loss :[2.874127], entropy:2.4572250843048096\r\n",
      "batch i:512, episode_len:27\r\n",
      "kl:0.01871,lr_multiplier:0.132,loss:[2.9777017],entropy:2.427489757537842,explained_var_old:0.401,explained_var_new:0.462\r\n",
      "loss :[2.9777017], entropy:2.427489757537842\r\n",
      "batch i:513, episode_len:20\r\n",
      "kl:0.02376,lr_multiplier:0.132,loss:[2.8930304],entropy:2.462947368621826,explained_var_old:0.463,explained_var_new:0.527\r\n",
      "loss :[2.8930304], entropy:2.462947368621826\r\n",
      "batch i:514, episode_len:28\r\n",
      "kl:0.01293,lr_multiplier:0.132,loss:[2.9705837],entropy:2.394296646118164,explained_var_old:0.397,explained_var_new:0.439\r\n",
      "loss :[2.9705837], entropy:2.394296646118164\r\n",
      "batch i:515, episode_len:19\r\n",
      "kl:0.01644,lr_multiplier:0.132,loss:[3.0048966],entropy:2.4463815689086914,explained_var_old:0.407,explained_var_new:0.467\r\n",
      "loss :[3.0048966], entropy:2.4463815689086914\r\n",
      "batch i:516, episode_len:38\r\n",
      "kl:0.01615,lr_multiplier:0.132,loss:[2.8671737],entropy:2.421447515487671,explained_var_old:0.457,explained_var_new:0.524\r\n",
      "loss :[2.8671737], entropy:2.421447515487671\r\n",
      "batch i:517, episode_len:18\r\n",
      "kl:0.01968,lr_multiplier:0.132,loss:[2.95982],entropy:2.443340539932251,explained_var_old:0.456,explained_var_new:0.507\r\n",
      "loss :[2.95982], entropy:2.443340539932251\r\n",
      "batch i:518, episode_len:46\r\n",
      "kl:0.01766,lr_multiplier:0.132,loss:[3.0085058],entropy:2.4369235038757324,explained_var_old:0.388,explained_var_new:0.472\r\n",
      "loss :[3.0085058], entropy:2.4369235038757324\r\n",
      "batch i:519, episode_len:33\r\n",
      "kl:0.01609,lr_multiplier:0.132,loss:[2.9594026],entropy:2.499478816986084,explained_var_old:0.443,explained_var_new:0.508\r\n",
      "loss :[2.9594026], entropy:2.499478816986084\r\n",
      "batch i:520, episode_len:31\r\n",
      "kl:0.01928,lr_multiplier:0.132,loss:[3.1205204],entropy:2.4689316749572754,explained_var_old:0.249,explained_var_new:0.373\r\n",
      "loss :[3.1205204], entropy:2.4689316749572754\r\n",
      "batch i:521, episode_len:20\r\n",
      "kl:0.02118,lr_multiplier:0.132,loss:[3.0614834],entropy:2.490222454071045,explained_var_old:0.334,explained_var_new:0.424\r\n",
      "loss :[3.0614834], entropy:2.490222454071045\r\n",
      "batch i:522, episode_len:22\r\n",
      "kl:0.02058,lr_multiplier:0.132,loss:[3.0099425],entropy:2.4962968826293945,explained_var_old:0.348,explained_var_new:0.426\r\n",
      "loss :[3.0099425], entropy:2.4962968826293945\r\n",
      "batch i:523, episode_len:37\r\n",
      "kl:0.02582,lr_multiplier:0.132,loss:[3.0728097],entropy:2.4887380599975586,explained_var_old:0.294,explained_var_new:0.386\r\n",
      "loss :[3.0728097], entropy:2.4887380599975586\r\n",
      "batch i:524, episode_len:22\r\n",
      "kl:0.02300,lr_multiplier:0.132,loss:[3.0703235],entropy:2.4852657318115234,explained_var_old:0.336,explained_var_new:0.408\r\n",
      "loss :[3.0703235], entropy:2.4852657318115234\r\n",
      "batch i:525, episode_len:18\r\n",
      "kl:0.02475,lr_multiplier:0.132,loss:[3.0728312],entropy:2.515556812286377,explained_var_old:0.318,explained_var_new:0.399\r\n",
      "loss :[3.0728312], entropy:2.515556812286377\r\n",
      "batch i:526, episode_len:22\r\n",
      "kl:0.01560,lr_multiplier:0.132,loss:[3.0122285],entropy:2.4803860187530518,explained_var_old:0.340,explained_var_new:0.408\r\n",
      "loss :[3.0122285], entropy:2.4803860187530518\r\n",
      "batch i:527, episode_len:16\r\n",
      "kl:0.01880,lr_multiplier:0.132,loss:[3.0557],entropy:2.4908909797668457,explained_var_old:0.326,explained_var_new:0.419\r\n",
      "loss :[3.0557], entropy:2.4908909797668457\r\n",
      "batch i:528, episode_len:15\r\n",
      "kl:0.02119,lr_multiplier:0.132,loss:[3.1772547],entropy:2.542019844055176,explained_var_old:0.285,explained_var_new:0.361\r\n",
      "loss :[3.1772547], entropy:2.542019844055176\r\n",
      "batch i:529, episode_len:28\r\n",
      "kl:0.02366,lr_multiplier:0.132,loss:[3.0544925],entropy:2.5014688968658447,explained_var_old:0.357,explained_var_new:0.438\r\n",
      "loss :[3.0544925], entropy:2.5014688968658447\r\n",
      "batch i:530, episode_len:15\r\n",
      "kl:0.01677,lr_multiplier:0.132,loss:[3.0787106],entropy:2.5485353469848633,explained_var_old:0.400,explained_var_new:0.451\r\n",
      "loss :[3.0787106], entropy:2.5485353469848633\r\n",
      "batch i:531, episode_len:22\r\n",
      "kl:0.02012,lr_multiplier:0.132,loss:[3.041296],entropy:2.511673927307129,explained_var_old:0.327,explained_var_new:0.402\r\n",
      "loss :[3.041296], entropy:2.511673927307129\r\n",
      "batch i:532, episode_len:26\r\n",
      "kl:0.01863,lr_multiplier:0.132,loss:[3.107262],entropy:2.4522271156311035,explained_var_old:0.281,explained_var_new:0.356\r\n",
      "loss :[3.107262], entropy:2.4522271156311035\r\n",
      "batch i:533, episode_len:30\r\n",
      "kl:0.01791,lr_multiplier:0.132,loss:[3.0421133],entropy:2.519197940826416,explained_var_old:0.335,explained_var_new:0.444\r\n",
      "loss :[3.0421133], entropy:2.519197940826416\r\n",
      "batch i:534, episode_len:22\r\n",
      "kl:0.02051,lr_multiplier:0.132,loss:[3.0652213],entropy:2.5248637199401855,explained_var_old:0.373,explained_var_new:0.437\r\n",
      "loss :[3.0652213], entropy:2.5248637199401855\r\n",
      "batch i:535, episode_len:22\r\n",
      "kl:0.01570,lr_multiplier:0.132,loss:[3.0235577],entropy:2.511471748352051,explained_var_old:0.380,explained_var_new:0.446\r\n",
      "loss :[3.0235577], entropy:2.511471748352051\r\n",
      "batch i:536, episode_len:17\r\n",
      "kl:0.02116,lr_multiplier:0.132,loss:[3.0671983],entropy:2.4697060585021973,explained_var_old:0.378,explained_var_new:0.424\r\n",
      "loss :[3.0671983], entropy:2.4697060585021973\r\n",
      "batch i:537, episode_len:12\r\n",
      "kl:0.01644,lr_multiplier:0.132,loss:[3.0308142],entropy:2.511073589324951,explained_var_old:0.403,explained_var_new:0.464\r\n",
      "loss :[3.0308142], entropy:2.511073589324951\r\n",
      "batch i:538, episode_len:18\r\n",
      "kl:0.01943,lr_multiplier:0.132,loss:[2.9777627],entropy:2.5230021476745605,explained_var_old:0.424,explained_var_new:0.471\r\n",
      "loss :[2.9777627], entropy:2.5230021476745605\r\n",
      "batch i:539, episode_len:21\r\n",
      "kl:0.01935,lr_multiplier:0.132,loss:[3.017667],entropy:2.4769654273986816,explained_var_old:0.371,explained_var_new:0.432\r\n",
      "loss :[3.017667], entropy:2.4769654273986816\r\n",
      "batch i:540, episode_len:34\r\n",
      "kl:0.02095,lr_multiplier:0.132,loss:[3.0313394],entropy:2.5365967750549316,explained_var_old:0.398,explained_var_new:0.445\r\n",
      "loss :[3.0313394], entropy:2.5365967750549316\r\n",
      "batch i:541, episode_len:22\r\n",
      "kl:0.02359,lr_multiplier:0.132,loss:[3.1124616],entropy:2.558955430984497,explained_var_old:0.378,explained_var_new:0.444\r\n",
      "loss :[3.1124616], entropy:2.558955430984497\r\n",
      "batch i:542, episode_len:18\r\n",
      "kl:0.01449,lr_multiplier:0.132,loss:[3.0602574],entropy:2.4987545013427734,explained_var_old:0.346,explained_var_new:0.411\r\n",
      "loss :[3.0602574], entropy:2.4987545013427734\r\n",
      "batch i:543, episode_len:27\r\n",
      "kl:0.01591,lr_multiplier:0.132,loss:[3.074155],entropy:2.5313892364501953,explained_var_old:0.363,explained_var_new:0.422\r\n",
      "loss :[3.074155], entropy:2.5313892364501953\r\n",
      "batch i:544, episode_len:16\r\n",
      "kl:0.01660,lr_multiplier:0.132,loss:[2.9856253],entropy:2.512942314147949,explained_var_old:0.394,explained_var_new:0.464\r\n",
      "loss :[2.9856253], entropy:2.512942314147949\r\n",
      "batch i:545, episode_len:18\r\n",
      "kl:0.01454,lr_multiplier:0.132,loss:[3.1058283],entropy:2.4945132732391357,explained_var_old:0.340,explained_var_new:0.409\r\n",
      "loss :[3.1058283], entropy:2.4945132732391357\r\n",
      "batch i:546, episode_len:20\r\n",
      "kl:0.02134,lr_multiplier:0.132,loss:[3.0800734],entropy:2.5284929275512695,explained_var_old:0.383,explained_var_new:0.436\r\n",
      "loss :[3.0800734], entropy:2.5284929275512695\r\n",
      "batch i:547, episode_len:26\r\n",
      "kl:0.01836,lr_multiplier:0.132,loss:[2.9819834],entropy:2.5181069374084473,explained_var_old:0.354,explained_var_new:0.469\r\n",
      "loss :[2.9819834], entropy:2.5181069374084473\r\n",
      "batch i:548, episode_len:16\r\n",
      "kl:0.01953,lr_multiplier:0.132,loss:[3.0319366],entropy:2.527907371520996,explained_var_old:0.434,explained_var_new:0.493\r\n",
      "loss :[3.0319366], entropy:2.527907371520996\r\n",
      "batch i:549, episode_len:26\r\n",
      "kl:0.02697,lr_multiplier:0.132,loss:[3.1239653],entropy:2.4895553588867188,explained_var_old:0.335,explained_var_new:0.415\r\n",
      "loss :[3.1239653], entropy:2.4895553588867188\r\n",
      "batch i:550, episode_len:13\r\n",
      "kl:0.01766,lr_multiplier:0.132,loss:[3.000223],entropy:2.516655206680298,explained_var_old:0.489,explained_var_new:0.545\r\n",
      "loss :[3.000223], entropy:2.516655206680298\r\n",
      "batch i:551, episode_len:16\r\n",
      "kl:0.02210,lr_multiplier:0.132,loss:[2.9944108],entropy:2.5631158351898193,explained_var_old:0.488,explained_var_new:0.535\r\n",
      "loss :[2.9944108], entropy:2.5631158351898193\r\n",
      "batch i:552, episode_len:26\r\n",
      "kl:0.01778,lr_multiplier:0.132,loss:[2.9395094],entropy:2.504220485687256,explained_var_old:0.501,explained_var_new:0.567\r\n",
      "loss :[2.9395094], entropy:2.504220485687256\r\n",
      "batch i:553, episode_len:16\r\n",
      "kl:0.01701,lr_multiplier:0.132,loss:[2.9627614],entropy:2.5746922492980957,explained_var_old:0.494,explained_var_new:0.547\r\n",
      "loss :[2.9627614], entropy:2.5746922492980957\r\n",
      "batch i:554, episode_len:17\r\n",
      "kl:0.01940,lr_multiplier:0.132,loss:[2.9700234],entropy:2.512502670288086,explained_var_old:0.462,explained_var_new:0.502\r\n",
      "loss :[2.9700234], entropy:2.512502670288086\r\n",
      "batch i:555, episode_len:23\r\n",
      "kl:0.01723,lr_multiplier:0.132,loss:[3.0808246],entropy:2.5202860832214355,explained_var_old:0.372,explained_var_new:0.459\r\n",
      "loss :[3.0808246], entropy:2.5202860832214355\r\n",
      "batch i:556, episode_len:17\r\n",
      "kl:0.02624,lr_multiplier:0.132,loss:[3.0500407],entropy:2.5179147720336914,explained_var_old:0.425,explained_var_new:0.484\r\n",
      "loss :[3.0500407], entropy:2.5179147720336914\r\n",
      "batch i:557, episode_len:30\r\n",
      "kl:0.02215,lr_multiplier:0.132,loss:[3.0080938],entropy:2.529775381088257,explained_var_old:0.429,explained_var_new:0.500\r\n",
      "loss :[3.0080938], entropy:2.529775381088257\r\n",
      "batch i:558, episode_len:17\r\n",
      "kl:0.02326,lr_multiplier:0.132,loss:[3.0284247],entropy:2.546893358230591,explained_var_old:0.432,explained_var_new:0.494\r\n",
      "loss :[3.0284247], entropy:2.546893358230591\r\n",
      "batch i:559, episode_len:19\r\n",
      "kl:0.02012,lr_multiplier:0.132,loss:[3.0364428],entropy:2.5631461143493652,explained_var_old:0.389,explained_var_new:0.455\r\n",
      "loss :[3.0364428], entropy:2.5631461143493652\r\n",
      "batch i:560, episode_len:28\r\n",
      "kl:0.01643,lr_multiplier:0.132,loss:[3.0591178],entropy:2.5584425926208496,explained_var_old:0.423,explained_var_new:0.489\r\n",
      "loss :[3.0591178], entropy:2.5584425926208496\r\n",
      "batch i:561, episode_len:14\r\n",
      "kl:0.01522,lr_multiplier:0.132,loss:[3.0782351],entropy:2.5233287811279297,explained_var_old:0.392,explained_var_new:0.459\r\n",
      "loss :[3.0782351], entropy:2.5233287811279297\r\n",
      "batch i:562, episode_len:20\r\n",
      "kl:0.01505,lr_multiplier:0.132,loss:[2.9804919],entropy:2.517544984817505,explained_var_old:0.425,explained_var_new:0.510\r\n",
      "loss :[2.9804919], entropy:2.517544984817505\r\n",
      "batch i:563, episode_len:26\r\n",
      "kl:0.01363,lr_multiplier:0.132,loss:[3.006286],entropy:2.5614466667175293,explained_var_old:0.407,explained_var_new:0.479\r\n",
      "loss :[3.006286], entropy:2.5614466667175293\r\n",
      "batch i:564, episode_len:32\r\n",
      "kl:0.01565,lr_multiplier:0.132,loss:[2.961727],entropy:2.534520149230957,explained_var_old:0.492,explained_var_new:0.551\r\n",
      "loss :[2.961727], entropy:2.534520149230957\r\n",
      "batch i:565, episode_len:48\r\n",
      "kl:0.02131,lr_multiplier:0.132,loss:[3.0224025],entropy:2.5538506507873535,explained_var_old:0.503,explained_var_new:0.560\r\n",
      "loss :[3.0224025], entropy:2.5538506507873535\r\n",
      "batch i:566, episode_len:11\r\n",
      "kl:0.01920,lr_multiplier:0.132,loss:[2.9506524],entropy:2.5260605812072754,explained_var_old:0.521,explained_var_new:0.584\r\n",
      "loss :[2.9506524], entropy:2.5260605812072754\r\n",
      "batch i:567, episode_len:27\r\n",
      "kl:0.02636,lr_multiplier:0.132,loss:[2.9201794],entropy:2.5233845710754395,explained_var_old:0.519,explained_var_new:0.587\r\n",
      "loss :[2.9201794], entropy:2.5233845710754395\r\n",
      "batch i:568, episode_len:18\r\n",
      "kl:0.01764,lr_multiplier:0.132,loss:[2.9707296],entropy:2.5217318534851074,explained_var_old:0.468,explained_var_new:0.548\r\n",
      "loss :[2.9707296], entropy:2.5217318534851074\r\n",
      "batch i:569, episode_len:20\r\n",
      "kl:0.02056,lr_multiplier:0.132,loss:[3.0170774],entropy:2.5391030311584473,explained_var_old:0.417,explained_var_new:0.502\r\n",
      "loss :[3.0170774], entropy:2.5391030311584473\r\n",
      "batch i:570, episode_len:22\r\n",
      "kl:0.01798,lr_multiplier:0.132,loss:[3.005564],entropy:2.6029231548309326,explained_var_old:0.462,explained_var_new:0.546\r\n",
      "loss :[3.005564], entropy:2.6029231548309326\r\n",
      "batch i:571, episode_len:12\r\n",
      "kl:0.02114,lr_multiplier:0.132,loss:[3.050028],entropy:2.572192430496216,explained_var_old:0.473,explained_var_new:0.563\r\n",
      "loss :[3.050028], entropy:2.572192430496216\r\n",
      "batch i:572, episode_len:30\r\n",
      "kl:0.02054,lr_multiplier:0.132,loss:[2.8921146],entropy:2.512155532836914,explained_var_old:0.448,explained_var_new:0.576\r\n",
      "loss :[2.8921146], entropy:2.512155532836914\r\n",
      "batch i:573, episode_len:23\r\n",
      "kl:0.02226,lr_multiplier:0.132,loss:[2.9621165],entropy:2.521418571472168,explained_var_old:0.515,explained_var_new:0.563\r\n",
      "loss :[2.9621165], entropy:2.521418571472168\r\n",
      "batch i:574, episode_len:12\r\n",
      "kl:0.01582,lr_multiplier:0.132,loss:[3.101984],entropy:2.580970525741577,explained_var_old:0.405,explained_var_new:0.485\r\n",
      "loss :[3.101984], entropy:2.580970525741577\r\n",
      "batch i:575, episode_len:15\r\n",
      "kl:0.01827,lr_multiplier:0.132,loss:[3.0900245],entropy:2.5620808601379395,explained_var_old:0.472,explained_var_new:0.518\r\n",
      "loss :[3.0900245], entropy:2.5620808601379395\r\n",
      "batch i:576, episode_len:18\r\n",
      "kl:0.01501,lr_multiplier:0.132,loss:[3.12302],entropy:2.5504679679870605,explained_var_old:0.367,explained_var_new:0.430\r\n",
      "loss :[3.12302], entropy:2.5504679679870605\r\n",
      "batch i:577, episode_len:13\r\n",
      "kl:0.01421,lr_multiplier:0.132,loss:[3.0814064],entropy:2.617685556411743,explained_var_old:0.442,explained_var_new:0.518\r\n",
      "loss :[3.0814064], entropy:2.617685556411743\r\n",
      "batch i:578, episode_len:21\r\n",
      "kl:0.01131,lr_multiplier:0.132,loss:[3.0597405],entropy:2.5690717697143555,explained_var_old:0.407,explained_var_new:0.495\r\n",
      "loss :[3.0597405], entropy:2.5690717697143555\r\n",
      "batch i:579, episode_len:22\r\n",
      "kl:0.01350,lr_multiplier:0.132,loss:[3.0301318],entropy:2.551835775375366,explained_var_old:0.442,explained_var_new:0.506\r\n",
      "loss :[3.0301318], entropy:2.551835775375366\r\n",
      "batch i:580, episode_len:11\r\n",
      "kl:0.01579,lr_multiplier:0.132,loss:[3.10425],entropy:2.5977210998535156,explained_var_old:0.438,explained_var_new:0.510\r\n",
      "loss :[3.10425], entropy:2.5977210998535156\r\n",
      "batch i:581, episode_len:27\r\n",
      "kl:0.01322,lr_multiplier:0.132,loss:[3.021585],entropy:2.5186233520507812,explained_var_old:0.419,explained_var_new:0.514\r\n",
      "loss :[3.021585], entropy:2.5186233520507812\r\n",
      "batch i:582, episode_len:18\r\n",
      "kl:0.02062,lr_multiplier:0.132,loss:[3.0826945],entropy:2.5534048080444336,explained_var_old:0.418,explained_var_new:0.488\r\n",
      "loss :[3.0826945], entropy:2.5534048080444336\r\n",
      "batch i:583, episode_len:21\r\n",
      "kl:0.02135,lr_multiplier:0.132,loss:[2.9235904],entropy:2.5183072090148926,explained_var_old:0.532,explained_var_new:0.599\r\n",
      "loss :[2.9235904], entropy:2.5183072090148926\r\n",
      "batch i:584, episode_len:25\r\n",
      "kl:0.02197,lr_multiplier:0.132,loss:[2.9421284],entropy:2.4923810958862305,explained_var_old:0.524,explained_var_new:0.598\r\n",
      "loss :[2.9421284], entropy:2.4923810958862305\r\n",
      "batch i:585, episode_len:17\r\n",
      "kl:0.02042,lr_multiplier:0.132,loss:[3.0120516],entropy:2.558501958847046,explained_var_old:0.445,explained_var_new:0.529\r\n",
      "loss :[3.0120516], entropy:2.558501958847046\r\n",
      "batch i:586, episode_len:16\r\n",
      "kl:0.01967,lr_multiplier:0.132,loss:[3.0686674],entropy:2.552441120147705,explained_var_old:0.420,explained_var_new:0.483\r\n",
      "loss :[3.0686674], entropy:2.552441120147705\r\n",
      "batch i:587, episode_len:25\r\n",
      "kl:0.00969,lr_multiplier:0.198,loss:[3.0312958],entropy:2.5796256065368652,explained_var_old:0.457,explained_var_new:0.541\r\n",
      "loss :[3.0312958], entropy:2.5796256065368652\r\n",
      "batch i:588, episode_len:16\r\n",
      "kl:0.02203,lr_multiplier:0.198,loss:[2.9525957],entropy:2.51562237739563,explained_var_old:0.440,explained_var_new:0.539\r\n",
      "loss :[2.9525957], entropy:2.51562237739563\r\n",
      "batch i:589, episode_len:11\r\n",
      "kl:0.02624,lr_multiplier:0.198,loss:[3.0235748],entropy:2.567028760910034,explained_var_old:0.457,explained_var_new:0.563\r\n",
      "loss :[3.0235748], entropy:2.567028760910034\r\n",
      "batch i:590, episode_len:24\r\n",
      "kl:0.03088,lr_multiplier:0.198,loss:[2.9036355],entropy:2.4821536540985107,explained_var_old:0.527,explained_var_new:0.642\r\n",
      "loss :[2.9036355], entropy:2.4821536540985107\r\n",
      "batch i:591, episode_len:24\r\n",
      "kl:0.02853,lr_multiplier:0.198,loss:[3.0838294],entropy:2.5736238956451416,explained_var_old:0.430,explained_var_new:0.546\r\n",
      "loss :[3.0838294], entropy:2.5736238956451416\r\n",
      "batch i:592, episode_len:10\r\n",
      "kl:0.03117,lr_multiplier:0.198,loss:[2.9617486],entropy:2.623645782470703,explained_var_old:0.526,explained_var_new:0.655\r\n",
      "loss :[2.9617486], entropy:2.623645782470703\r\n",
      "batch i:593, episode_len:17\r\n",
      "kl:0.02991,lr_multiplier:0.198,loss:[3.0459952],entropy:2.6255412101745605,explained_var_old:0.445,explained_var_new:0.551\r\n",
      "loss :[3.0459952], entropy:2.6255412101745605\r\n",
      "batch i:594, episode_len:10\r\n",
      "kl:0.03185,lr_multiplier:0.198,loss:[3.022618],entropy:2.586141347885132,explained_var_old:0.466,explained_var_new:0.538\r\n",
      "loss :[3.022618], entropy:2.586141347885132\r\n",
      "batch i:595, episode_len:13\r\n",
      "kl:0.02404,lr_multiplier:0.198,loss:[2.9064572],entropy:2.5409250259399414,explained_var_old:0.524,explained_var_new:0.619\r\n",
      "loss :[2.9064572], entropy:2.5409250259399414\r\n",
      "batch i:596, episode_len:28\r\n",
      "kl:0.03014,lr_multiplier:0.198,loss:[3.0303829],entropy:2.506793737411499,explained_var_old:0.435,explained_var_new:0.535\r\n",
      "loss :[3.0303829], entropy:2.506793737411499\r\n",
      "batch i:597, episode_len:18\r\n",
      "kl:0.02564,lr_multiplier:0.198,loss:[2.9716656],entropy:2.5440053939819336,explained_var_old:0.444,explained_var_new:0.545\r\n",
      "loss :[2.9716656], entropy:2.5440053939819336\r\n",
      "batch i:598, episode_len:32\r\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#  对于五子棋的AlphaZero的训练的实现\n",
    "\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict, deque\n",
    "import paddle\n",
    "\n",
    "\n",
    "class TrainPipeline():\n",
    "    def __init__(self, init_model=None, is_shown = 0):\n",
    "        # 五子棋逻辑和棋盘UI的参数\n",
    "        self.board_width = 9  ###为了更快的验证算法，可以调整棋盘大小为(8x8) ，(6x6)\n",
    "        self.board_height = 9\n",
    "        self.n_in_row = 5\n",
    "        self.board = Board(width=self.board_width,\n",
    "                           height=self.board_height,\n",
    "                           n_in_row=self.n_in_row)\n",
    "        self.is_shown = is_shown\n",
    "        self.game = Game_UI(self.board, is_shown)\n",
    "        # 训练参数\n",
    "        self.learn_rate = 2e-3\n",
    "        self.lr_multiplier = 1.0  # 基于KL自适应地调整学习率\n",
    "        self.temp = 1.0  # 临时变量\n",
    "        self.n_playout = 400  # 每次移动的模拟次数\n",
    "        self.c_puct = 5\n",
    "        self.buffer_size = 10000 #经验池大小 10000\n",
    "        self.batch_size = 512  # 训练的mini-batch大小 512\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
    "        self.play_batch_size = 1\n",
    "        self.epochs = 5  # 每次更新的train_steps数量\n",
    "        self.kl_targ = 0.02\n",
    "        self.check_freq = 100  #评估模型的频率，可以设置大一些比如500\n",
    "        self.game_batch_num = 1500\n",
    "        self.best_win_ratio = 0.0\n",
    "        # 用于纯粹的mcts的模拟数量，用作评估训练策略的对手\n",
    "        self.pure_mcts_playout_num = 1000\n",
    "        if init_model:\n",
    "            # 从初始的策略价值网开始训练\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height,\n",
    "                                                   model_file=init_model)\n",
    "        else:\n",
    "            # 从新的策略价值网络开始训练\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height)\n",
    "        # 定义训练机器人\n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                      c_puct=self.c_puct,\n",
    "                                      n_playout=self.n_playout,\n",
    "                                      is_selfplay=1)\n",
    "\n",
    "    def get_equi_data(self, play_data):\n",
    "        \"\"\"通过旋转和翻转来增加数据集\n",
    "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "        \"\"\"\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            for i in [1, 2, 3, 4]:\n",
    "                # 逆时针旋转\n",
    "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(\n",
    "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "                # 水平翻转\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "        return extend_data\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"收集自我博弈数据进行训练\"\"\"\n",
    "        for i in range(n_games):\n",
    "            winner, play_data = self.game.start_self_play(self.mcts_player, temp=self.temp)\n",
    "            play_data = list(play_data)[:]\n",
    "            self.episode_len = len(play_data)\n",
    "            # 增加数据\n",
    "            play_data = self.get_equi_data(play_data)\n",
    "            self.data_buffer.extend(play_data)\n",
    "\n",
    "    def policy_update(self):\n",
    "        \"\"\"更新策略价值网络\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        \n",
    "        # print(np.array( state_batch).shape )\n",
    "        state_batch= np.array( state_batch).astype(\"float32\")\n",
    "        \n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        mcts_probs_batch= np.array( mcts_probs_batch).astype(\"float32\")\n",
    "        \n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        winner_batch= np.array( winner_batch).astype(\"float32\")\n",
    "        \n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\n",
    "        for i in range(self.epochs):\n",
    "            loss, entropy = self.policy_value_net.train_step(\n",
    "                state_batch,\n",
    "                mcts_probs_batch,\n",
    "                winner_batch,\n",
    "                self.learn_rate * self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                                axis=1)\n",
    "                         )\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # 自适应调节学习率\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "\n",
    "        explained_var_old = (1 -\n",
    "                             np.var(np.array(winner_batch) - old_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        explained_var_new = (1 -\n",
    "                             np.var(np.array(winner_batch) - new_v.flatten()) /\n",
    "                             np.var(np.array(winner_batch)))\n",
    "        print((\"kl:{:.5f},\"\n",
    "               \"lr_multiplier:{:.3f},\"\n",
    "               \"loss:{},\"\n",
    "               \"entropy:{},\"\n",
    "               \"explained_var_old:{:.3f},\"\n",
    "               \"explained_var_new:{:.3f}\"\n",
    "               ).format(kl,\n",
    "                        self.lr_multiplier,\n",
    "                        loss,\n",
    "                        entropy,\n",
    "                        explained_var_old,\n",
    "                        explained_var_new))\n",
    "        return loss, entropy\n",
    "\n",
    "    def policy_evaluate(self, n_games=10):\n",
    "        \"\"\"\n",
    "        通过与纯的MCTS算法对抗来评估训练的策略\n",
    "        注意：这仅用于监控训练进度\n",
    "        \"\"\"\n",
    "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                         c_puct=self.c_puct,\n",
    "                                         n_playout=self.n_playout)\n",
    "        pure_mcts_player = MCTS_Pure(c_puct=5,\n",
    "                                     n_playout=self.pure_mcts_playout_num)\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            winner = self.game.start_play(current_mcts_player,\n",
    "                                          pure_mcts_player,\n",
    "                                          start_player=i % 2)\n",
    "            win_cnt[winner] += 1\n",
    "        win_ratio = 1.0 * (win_cnt[1] + 0.5 * win_cnt[-1]) / n_games\n",
    "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
    "            self.pure_mcts_playout_num,\n",
    "            win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"开始训练\"\"\"\n",
    "        root = os.getcwd()\n",
    "\n",
    "        dst_path = os.path.join(root, 'best_model')\n",
    "\n",
    "        if not os.path.exists(dst_path):\n",
    "            os.makedirs(dst_path)\n",
    "\n",
    "        try:\n",
    "            for i in range(self.game_batch_num):\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                print(\"batch i:{}, episode_len:{}\".format(\n",
    "                    i + 1, self.episode_len))\n",
    "                if len(self.data_buffer) > self.batch_size:\n",
    "                    loss, entropy = self.policy_update()\n",
    "                    print(\"loss :{}, entropy:{}\".format(loss, entropy))\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    self.policy_value_net.save_model(os.path.join(dst_path, 'current_policy_step.model'))\n",
    "                # 检查当前模型的性能，保存模型的参数\n",
    "                if (i + 1) % self.check_freq == 0:\n",
    "                    print(\"current self-play batch: {}\".format(i + 1))\n",
    "                    win_ratio = self.policy_evaluate()\n",
    "                    self.policy_value_net.save_model(os.path.join(dst_path, 'current_policy.model'))\n",
    "                    if win_ratio > self.best_win_ratio:\n",
    "                        print(\"New best policy!!!!!!!!\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        # 更新最好的策略\n",
    "                        self.policy_value_net.save_model(os.path.join(dst_path, 'best_policy.model'))\n",
    "                        if (self.best_win_ratio == 1.0 and\n",
    "                                    self.pure_mcts_playout_num < 8000):\n",
    "                            self.pure_mcts_playout_num += 1000\n",
    "                            self.best_win_ratio = 0.0\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        device = paddle.get_device()               \n",
    "        paddle.set_device(device)\n",
    "        is_shown = 0\n",
    "        # model_path = 'dist/best_policy.model'\n",
    "        # model_path = 'dist/current_policy.model'\n",
    "\n",
    "        # training_pipeline = TrainPipeline(model_path, is_shown)\n",
    "        training_pipeline = TrainPipeline(None, is_shown)\n",
    "        training_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. 训练结果与展示：\n",
    "\n",
    "### 训练过程\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/54709767590b487c916a31eb595c293a13ee0393b6d54b4f87f0400eb4eae5b2)\n",
    "\n",
    "### 最终下棋的效果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/225bd09c070c4177814a408689cb45c0ac8ca042f4e74e81897fc41cb79bc833)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}