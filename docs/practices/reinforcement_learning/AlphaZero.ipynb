{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 用飞桨框架2.0造一个会下五子棋的AI模型\n",
    "\n",
    "作者信息：yangguohao (https://github.com/yangguohao/)\n",
    "\n",
    "更新日期：2023 年 2 月 14 日\n",
    "\n",
    "## 1. 简要介绍\n",
    "\n",
    "### AlphaZero\n",
    "\n",
    "* [AlphaZero](https://arxiv.org/abs/1712.01815) 是由 Alphabet 旗下的子公司 DeepMind 研发的强化学习模型。 相较于之前的两代版本 [AlphaGo](https://www.nature.com/articles/nature16961) 和 [AlphaGoZero](https://www.nature.com/articles/nature24270)，AlphaZero 完全无需人工特征、无需任何人类棋谱、甚至无需任何特定的策略和算法。\n",
    "\n",
    "* 作为一个通用模型，AlphaZero 不只针对围棋，而是同时学习了三种棋类-日本将棋、国际象棋以及围棋。从零开始，经过短时间的训练，AlphaZero 完胜各个领域里的最强AI。包括国际象棋的Stockfish、将棋的Elmo，以及围棋的前辈AlphaGo Zero。\n",
    "\n",
    "* 其核心主要为 蒙特卡洛树搜索(MCTS) 和 策略价值深度网络。通俗地说，蒙特卡洛树搜索能让 AlphaZero 多想几步棋，看的更远。而策略价值网络能让 AlphaZero 更准确地评估当前的棋局，提升蒙特卡洛树搜索的精度。两者相辅相成从而决定每一步的落子。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/abb79e88765242ccb9df65f2cf4877fc83be041546bf47e293dcae1b98e7e8bb)\n",
    "\n",
    "\n",
    "### 本项目简介\n",
    "\n",
    "* 本项目是AlphaZero算法的一个实现（使用PaddlePaddle框架），用于玩简单的棋盘游戏Gomoku（也称为五子棋），使用纯粹的自我博弈的方式开始训练。\n",
    "\n",
    "* Gomoku游戏比围棋或象棋简单得多，因此我们可以专注于AlphaZero的训练，在一台PC机上几个小时内就可以获得一个让你不可忽视的AI模型。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/d510e461a8d84be3a1d0952874099910f4ac4da475e2424d862251d20f23c0f3)\n",
    "\n",
    "\n",
    "* 因为和围棋相比，五子棋的规则较为简单，落子空间也比较小，因此没有用到AlphaGo Zero中大量使用的残差网络，只使用了卷积层和全连接层。本项目的网路简单，无需使用大量的计算就可以进行运行训练。使用的 Paddle 版本为 2.4.0。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 环境配置\n",
    "本教程基于 PaddlePaddle 2.4.0 编写，如果你的环境不是本版本，请先参考官网安装 PaddlePaddle 2.4.0。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pygame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn \n",
    "import paddle.nn.functional as F\n",
    "import pygame\n",
    "from pygame.locals import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 游戏环境\n",
    "\n",
    "初始化游戏环境，可以跳过该内容。\n",
    "\n",
    "主要是针对五子棋棋盘大小，走子以及游戏规则的设定，以及棋盘棋子等可视化的 UI 设定。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class Board(object):\n",
    "    \"\"\"棋盘游戏逻辑控制\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.width = int(kwargs.get('width', 15))  # 棋盘宽度\n",
    "        self.height = int(kwargs.get('height', 15))  # 棋盘高度\n",
    "        self.states = {}    # 棋盘状态为一个字典,键: 移动步数,值: 玩家的棋子类型\n",
    "        self.n_in_row = int(kwargs.get('n_in_row', 5))  # 5个棋子一条线则获胜\n",
    "        self.players = [1, 2]  # 玩家1,2\n",
    "\n",
    "    def init_board(self, start_player=0):\n",
    "        # 初始化棋盘\n",
    "\n",
    "        # 当前棋盘的宽高小于5时,抛出异常(因为是五子棋)\n",
    "        if self.width < self.n_in_row or self.height < self.n_in_row:\n",
    "            raise Exception('棋盘的长宽不能少于{}'.format(self.n_in_row))\n",
    "        self.current_player = self.players[start_player]  # 先手玩家\n",
    "        self.availables = list(range(self.width * self.height)) # 初始化可用的位置列表\n",
    "        self.states = {}  # 初始化棋盘状态\n",
    "        self.last_move = -1  # 初始化最后一次的移动位置\n",
    "\n",
    "    def current_state(self):\n",
    "        \"\"\"\n",
    "        从当前玩家的角度返回棋盘状态。\n",
    "        状态形式: 4 * 宽 * 高\n",
    "        \"\"\"\n",
    "        # 使用4个15x15的二值特征平面来描述当前的局面\n",
    "        # 前两个平面分别表示当前player的棋子位置和对手player的棋子位置，有棋子的位置是1，没棋子的位置是0\n",
    "        # 第三个平面表示对手player最近一步的落子位置，也就是整个平面只有一个位置是1，其余全部是0\n",
    "        # 第四个平面表示的是当前player是不是先手player，如果是先手player则整个平面全部为1，否则全部为0\n",
    "        square_state = np.zeros((4, self.width, self.height))\n",
    "        if self.states:\n",
    "            moves, players = np.array(list(zip(*self.states.items())))\n",
    "            move_curr = moves[players == self.current_player]   # 获取棋盘状态上属于当前玩家的所有移动值\n",
    "            move_oppo = moves[players != self.current_player]   # 获取棋盘状态上属于对方玩家的所有移动值\n",
    "            square_state[0][move_curr // self.width,            # 对第一个特征平面填充值(当前玩家)\n",
    "                            move_curr % self.height] = 1.0\n",
    "            square_state[1][move_oppo // self.width,            # 对第二个特征平面填充值(对方玩家)\n",
    "                            move_oppo % self.height] = 1.0\n",
    "            # 指出最后一个移动位置\n",
    "            square_state[2][self.last_move // self.width,       # 对第三个特征平面填充值(对手最近一次的落子位置)\n",
    "                            self.last_move % self.height] = 1.0\n",
    "        if len(self.states) % 2 == 0:   # 对第四个特征平面填充值,当前玩家是先手,则填充全1,否则为全0\n",
    "            square_state[3][:, :] = 1.0\n",
    "        # 将每个平面棋盘状态按行逆序转换(第一行换到最后一行,第二行换到倒数第二行..)\n",
    "        return square_state[:, ::-1, :]\n",
    "\n",
    "    def do_move(self, move):\n",
    "        # 根据移动的数据更新各参数\n",
    "        self.states[move] = self.current_player  # 将当前的参数存入棋盘状态中\n",
    "        self.availables.remove(move)  # 从可用的棋盘列表移除当前移动的位置\n",
    "        self.current_player = (\n",
    "            self.players[0] if self.current_player == self.players[1]\n",
    "            else self.players[1]\n",
    "        )  # 改变当前玩家\n",
    "        self.last_move = move  # 记录最后一次的移动位置\n",
    "\n",
    "    def has_a_winner(self):\n",
    "        # 是否产生赢家\n",
    "        width = self.width  # 棋盘宽度\n",
    "        height = self.height  # 棋盘高度\n",
    "        states = self.states  # 状态\n",
    "        n = self.n_in_row  # 获胜需要的棋子数量\n",
    "\n",
    "        # 当前棋盘上所有的落子位置\n",
    "        moved = list(set(range(width * height)) - set(self.availables))\n",
    "        if len(moved) < self.n_in_row + 2:\n",
    "            # 当前棋盘落子数在7个以上时会产生赢家,落子数低于7个时,直接返回没有赢家\n",
    "            return False, -1\n",
    "\n",
    "        # 遍历落子数\n",
    "        for m in moved:\n",
    "            h = m // width\n",
    "            w = m % width  # 获得棋子的坐标\n",
    "            player = states[m]  # 根据移动的点确认玩家\n",
    "\n",
    "            # 判断各种赢棋的情况\n",
    "            # 横向5个\n",
    "            if (w in range(width - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 纵向5个\n",
    "            if (h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * width, width))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 左上到右下斜向5个\n",
    "            if (w in range(width - n + 1) and h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * (width + 1), width + 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "            # 右上到左下斜向5个\n",
    "            if (w in range(n - 1, width) and h in range(height - n + 1) and\n",
    "                    len(set(states.get(i, -1) for i in range(m, m + n * (width - 1), width - 1))) == 1):\n",
    "                return True, player\n",
    "\n",
    "        # 当前都没有赢家,返回False\n",
    "        return False, -1\n",
    "\n",
    "    def game_end(self):\n",
    "        \"\"\"检查当前棋局是否结束\"\"\"\n",
    "        win, winner = self.has_a_winner()\n",
    "        if win:\n",
    "            return True, winner\n",
    "        elif not len(self.availables):\n",
    "            # 棋局布满,没有赢家\n",
    "            return True, -1\n",
    "        return False, -1\n",
    "\n",
    "    def get_current_player(self):\n",
    "        return self.current_player\n",
    "\n",
    "\n",
    "# 加上UI的布局的训练方式\n",
    "class Game_UI(object):\n",
    "    \"\"\"游戏控制区域\"\"\"\n",
    "\n",
    "    def __init__(self, board, **kwargs):\n",
    "        self.board = board  # 加载棋盘控制类\n",
    "\n",
    "        # 初始化 pygame\n",
    "        pygame.init()\n",
    "\n",
    "    def start_play_evaluate(self, player1, player2, start_player=0):\n",
    "        \"\"\"开始一局游戏，评估当前的价值策略网络的胜率\"\"\"\n",
    "        if start_player not in (0, 1):\n",
    "            # 如果玩家不在玩家1,玩家2之间,抛出异常\n",
    "            raise Exception('开始的玩家必须为0(玩家1)或1(玩家2)')\n",
    "        self.board.init_board(start_player)  # 初始化棋盘\n",
    "        p1, p2 = self.board.players  # 加载玩家1,玩家2\n",
    "        player1.set_player_ind(p1)  # 设置玩家1\n",
    "        player2.set_player_ind(p2)  # 设置玩家2\n",
    "        players = {p1: player1, p2: player2}\n",
    "        \n",
    "        while True:\n",
    "\n",
    "            current_player = self.board.current_player  # 获取当前玩家\n",
    "            player_in_turn = players[current_player]  # 当前玩家的信息\n",
    "            move = player_in_turn.get_action(self.board)  # 基于MCTS的AI下一步落子\n",
    "            self.board.do_move(move)  # 根据下一步落子的状态更新棋盘各参数\n",
    "        \n",
    "            # 判断当前棋局是否结束\n",
    "            end, winner = self.board.game_end()\n",
    "            # 结束\n",
    "            if end:\n",
    "                win = winner\n",
    "                break\n",
    "        \n",
    "        return win\n",
    "\n",
    "    def start_play_train(self, player, temp=1e-3):\n",
    "        \"\"\" \n",
    "        开始自我博弈，使用MCTS玩家开始自己玩游戏,重新使用搜索树并存储自己玩游戏的数据\n",
    "        (state, mcts_probs, z) 提供训练\n",
    "        \"\"\"\n",
    "        self.board.init_board()  # 初始化棋盘\n",
    "        states, mcts_probs, current_players = [], [], []  # 状态,mcts的行为概率,当前玩家\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # 根据当前棋盘状态返回可能得行为,及行为对应的概率\n",
    "            move, move_probs = player.get_action(self.board,\n",
    "                                                 temp=temp,\n",
    "                                                 return_prob=1)\n",
    "            # 存储数据\n",
    "            states.append(self.board.current_state())  # 存储状态数据\n",
    "            mcts_probs.append(move_probs)  # 存储行为概率数据\n",
    "            current_players.append(self.board.current_player)  # 存储当前玩家\n",
    "            # 执行一个移动\n",
    "            self.board.do_move(move)\n",
    "\n",
    "            # 判断该局游戏是否终止\n",
    "            end, winner = self.board.game_end()\n",
    "            if end:\n",
    "                # 从每个状态的当时的玩家的角度看待赢家\n",
    "                winners_z = np.zeros(len(current_players))\n",
    "                if winner != -1:\n",
    "                    # 没有赢家时\n",
    "                    winners_z[np.array(current_players) == winner] = 1.0\n",
    "                    winners_z[np.array(current_players) != winner] = -1.0\n",
    "                # 重置MSCT的根节点\n",
    "                player.reset_player()\n",
    "                return winner, zip(states, mcts_probs, winners_z)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 价值策略网络\n",
    "\n",
    "* 原论文中的策略价值网络的结构是一个 CNN 组成的神经网络，初始游戏状态的张量在经过一个基本的卷积后，使用了19层或者39层的深度残差网络，最后输出价值和策略两个部分。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ac1bbf4b83b04a1d8f7e0abee8ae51fc91725b3526ac440da643a2f8d82b28f9)\n",
    "\n",
    "* 而这里为了演示算法，网络并不复杂，深度较浅。但是整体的逻辑与 AlphaZero 相似，由公共网络层、行动策略网络层和状态价值网络层构成。公共网络层使用卷积网络对棋盘上的状态进行特征提取，而行动策略层用以输出每个可落子点的落子概率，状态价值层用以输出可落子点的价值的评分。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Net(paddle.nn.Layer):\n",
    "    def __init__(self,board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        # 公共网络层\n",
    "        self.conv1 = nn.Conv2D(in_channels=4,out_channels=32,kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2D(in_channels=32,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.conv3 = nn.Conv2D(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        # 行动策略网络层\n",
    "        self.act_conv1 = nn.Conv2D(in_channels=128,out_channels=4,kernel_size=1,padding=0)\n",
    "        self.act_fc1 = nn.Linear(4*self.board_width*self.board_height,\n",
    "                                 self.board_width*self.board_height)\n",
    "        self.val_conv1 = nn.Conv2D(in_channels=128,out_channels=2,kernel_size=1,padding=0)\n",
    "        self.val_fc1 = nn.Linear(2*self.board_width*self.board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 公共网络层 \n",
    "        x = F.relu(self.conv1(inputs))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 行动策略网络层\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = paddle.reshape(\n",
    "                x_act, [-1, 4 * self.board_height * self.board_width])\n",
    "        \n",
    "        x_act  = F.log_softmax(self.act_fc1(x_act))        \n",
    "        # 状态价值网络层\n",
    "        x_val  = F.relu(self.val_conv1(x))\n",
    "        x_val = paddle.reshape(\n",
    "                x_val, [-1, 2 * self.board_height * self.board_width])\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "\n",
    "        return x_act,x_val"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 在定义好策略和价值网络的基础上，接下来实现PolicyValueNet类，该类主要定义：policy_value_fn()方法，主要用于蒙特卡洛树搜索时评估叶子节点对应局面评分、该局所有可行动作及对应概率；另一个方法train_step()，主要用于更新自我对弈收集数据上策略价值网络的参数。\n",
    "\n",
    "* 在训练神经网络阶段，我们使用自我对战学习阶段得到的样本集合 (s,π,z) 来训练我们神经网络的模型参数。训练的目的是对于每个输入的棋盘状态 s, 神经网络输出的概率 p 和价值 v 和我们训练样本中的 π,z 差距尽可能的少。\n",
    "\n",
    "* 损失函数由三部分组成，\n",
    "第一部分是均方误差损失函数，对应代码中 value_loss，用于评估神经网络预测的胜负结果和真实结果之间的差异。\n",
    "第二部分是交叉熵损失函数，对应 policy_loss，用于评估神经网络的输出策略和我们 MCTS 输出的策略的差异。\n",
    "第三部分是L2正则化项，对应优化器 self.optimizer 中的 weight_decay, 用于控制网络模型的复杂度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class PolicyValueNet():\n",
    "    \"\"\"策略&值网络 \"\"\"\n",
    "    def __init__(self, board_width, board_height,\n",
    "                 model_file=None, use_gpu=True):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-3  # coef of l2 penalty\n",
    "        \n",
    "\n",
    "        self.policy_value_net = Net(self.board_width, self.board_height)        \n",
    "        \n",
    "        self.optimizer  = paddle.optimizer.Adam(learning_rate=0.02,\n",
    "                                parameters=self.policy_value_net.parameters(), weight_decay=self.l2_const)\n",
    "                                     \n",
    "\n",
    "        if model_file:\n",
    "            net_params = paddle.load(model_file)\n",
    "            self.policy_value_net.set_state_dict(net_params)\n",
    "            \n",
    "    def policy_value_evaluate(self, state_batch):\n",
    "        \"\"\"\n",
    "        评估函数\n",
    "        Args:\n",
    "            input: 一组棋盘状态\n",
    "            output: 根据棋盘状态输出对应的动作概率及价值\n",
    "        \"\"\"\n",
    "        state_batch = paddle.to_tensor(state_batch)\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        act_probs = np.exp(log_act_probs.numpy())\n",
    "        return act_probs, value.numpy()\n",
    "\n",
    "    def policy_value_fn(self, board):\n",
    "        \"\"\"\n",
    "        评估场面局势，给出每个位置的概率及价值\n",
    "        Args:\n",
    "            input: 棋盘状态\n",
    "            output: 返回一组列表，包含棋盘每个可下的点的动作概率以及价值得分。\n",
    "        \"\"\"\n",
    "        legal_positions = board.availables\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(\n",
    "                -1, 4, self.board_width, self.board_height)).astype(\"float32\")\n",
    "\n",
    "        act_probs, value = self.policy_value_evaluate(current_state)\n",
    "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
    "        return act_probs, value\n",
    "\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr=0.002):\n",
    "        \"\"\"用采样得到的样本集合对策略价值网络进行一次训练\"\"\"\n",
    "        # wrap in Tensor\n",
    "        state_batch = paddle.to_tensor(state_batch)\n",
    "        mcts_probs = paddle.to_tensor(mcts_probs)\n",
    "        winner_batch = paddle.to_tensor(winner_batch)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        self.optimizer.clear_gradients()\n",
    "        # set learning rate\n",
    "        self.optimizer.set_lr(lr)\n",
    "\n",
    "        # forward\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # define the loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        # Note: the L2 penalty is incorporated in optimizer\n",
    "        value = paddle.reshape(x=value, shape=[-1])\n",
    "        value_loss = F.mse_loss(input=value, label=winner_batch)\n",
    "        policy_loss = -paddle.mean(paddle.sum(mcts_probs*log_act_probs, axis=1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # backward and optimize\n",
    "        loss.backward()\n",
    "        self.optimizer.minimize(loss)\n",
    "        return loss.numpy()\n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        paddle.save(net_params, model_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 蒙特卡洛树搜索（MCTS）\n",
    "\n",
    "传统的AI博弈树搜索算法效率都很低，因为这些算法在做出最终选择前需要穷尽每一种走法。即便很少的分支因子的游戏，其每一步的搜索空间也会爆炸式增长。分支因子就是所有可能的走法的数量，这个数量会随着游戏的进行不断变化。因此，你可以试着计算一个游戏的平均分支因子数，国际象棋的平均分支因子是35，而围棋则是250。这意味着，在国际象棋中，仅走两步就有1,225（35²）种可能的棋面，而在围棋中，这个数字会变成62,500（250²）。因此，上述的价值策略神经网络将指导并告诉我们哪些博弈路径值得探索，从而避免被许多无用的搜索路径所淹没。再结合蒙特卡洛树选择最佳的走法。\n",
    "\n",
    "## 棋类游戏的蒙特卡洛树搜索（MCTS）\n",
    "使用MCTS的具体做法是这样的，给定一个棋面，MCTS共进行N次模拟。主要的搜索阶段有4个：选择，扩展，仿真和回溯\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/73384055df364b44a49e7e206a9015790be7b3c0aa1942d0a4e57aa617fad087)\n",
    "\n",
    "* 第一步是选择(Selection)，这一步会从根节点开始，每次都选一个“最值得搜索的子节点”，一般使用上限置信区间算法 (Upper Confidence Bound Apply to Tree, UCT) 选择分数最高的节点，直到来到一个“存在未扩展的子节点”的节点\n",
    "\n",
    "* 第二步是扩展(Expansion)，在这个搜索到的“存在未扩展的子节点”之上，加上一个没有历史记录的子节点并初始化该子节点\n",
    "\n",
    "* 第三步是仿真(simulation)，从上面这个没有试过的着法开始，用一个简单策略比如快速走子策略 (Rollout policy) 走到底，得到一个胜负结果。快速走子策略虽然不是很精确，但是速度较快，在这里具有优势。因为如果这个策略走得慢，结果虽然会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。\n",
    "\n",
    "* 第四步是回溯 (backpropagation), 将我们最后得到的胜负结果回溯加到MCTS树结构上。注意除了之前的MCTS树要回溯外，新加入的节点也要加上一次胜负历史记录。\n",
    "\n",
    "以上就是MCTS搜索的整个过程。这4步一般是通用的，但是MCTS树结构上保存的内容而一般根据要解决的问题和建模的复杂度而不同。\n",
    "## 基于神经网络的蒙特卡洛树搜索（MCTS）\n",
    "N(s,a) :记录边的访问次数；\n",
    "W(s,a):  合计行动价值；\n",
    "Q(s,a) :平均行动价值；\n",
    "P(s,a) :选择该条边的先验概率；\n",
    "* 首先是选择(Selection)：在MCTS内部，出现过的局面，我们会使用UCT选择子分支。最终我们会选择Q+U最大的子分支作为搜索分支，一直走到棋局结束，或者走到了没有到终局MCTS的叶子节点。$c_{puct}$是决定探索程度的一个系数\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/a3edc34d8d554068becbfb21b7f6a5f7fc0b43f804eb45ed9c92a27d38478fdd)\n",
    "\n",
    "* 然后是扩展(Expansion)&&仿真(simulation)：对于叶子节点状态s，会利用神经网络对叶子节点做预测，得到当前叶子节点的各个可能的子节点位置sL落子的概率p和对应的价值v,对于这些可能的新节点我们在MCTS中创建出来，初始化其分支上保存的信息为\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/aed60f9babbb4c208f19d480fd25558b903c4836a2e5438bb858e6ddcaa218c9)\n",
    "\n",
    "\n",
    "* 最后是回溯(backpropagation)：将新叶子节点分支的信息回溯累加到祖先节点分支上去。这个回溯的逻辑也是很简单的，从每个叶子节点L依次向根节点回溯，并依次更新上层分支数据结构如下：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/b39e2bd4ab0f42a691e6a239443bc57f0dfac929a03b428b8a06b0416d4340c3)\n",
    "\n",
    "MCTS搜索完毕后，模型就可以在MCTS的根节点s基于以下公式选择行棋的MCTS分支了:\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3a14cd6be857468b9bcbcdee61d6ecdb325a864649284df4a8aa5b1d2b7605a0)\n",
    "\n",
    "τ是用来控制探索的程度，τ的取值介于(0,1]之间，当τ越接近于1时，神经网络的采样越接近于MCTS的原始采样，当τ越接近于0时，神经网络的采样越接近于贪婪策略，即选择最大访问次数N所对应的动作。\n",
    "因为在τ很小的情况下，直接计算访问次数N的τ次方根可能会导致数值异常，为了避免这种情况，在计算行动概率时，先将访问次数N加上一个非常小的数值（本项目是1e-10），取自然对数后乘上1/τ，再用一个简化的softmax函数将输出还原为概率，这和原始公式在数学上基本上是等效的。\n",
    "\n",
    "**关键点是什么？**\n",
    "* 通过每一次模拟，MCTS依靠神经网络， 使用累计价值 (Q)、神经网络给出的走法先验概率 (P) 以及访问对应节点的频率这些数字的组合，沿着最有希望获胜的路径（换句话说，也就是具有最高置信区间上界的路径）进行探索。\n",
    "\n",
    "* 在每一次模拟中，MCTS会尽可能向纵深进行探索直至遇到它从未见过的盘面状态，在这种情况下，它会通过神经网络来评估该盘面状态的优劣。\n",
    "\n",
    "* 巧妙了使用MCTS搜索树和神经网络一起，通过MCTS搜索树优化神经网络参数，反过来又通过优化的神经网络指导MCTS搜索。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    probs = np.exp(x - np.max(x))\n",
    "    probs /= np.sum(probs)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def policy_value_fn(board):\n",
    "    \"\"\"\n",
    "    接受状态并输出（动作，概率）列表的函数元组和状态的分数\"\"\"\n",
    "    # 返回统一概率和0分的纯MCTS\n",
    "    action_probs = np.ones(len(board.availables)) / len(board.availables)\n",
    "    return zip(board.availables, action_probs), 0\n",
    "\n",
    "\n",
    "class TreeNode(object):\n",
    "    \"\"\"MCTS树中的节点。\n",
    "\n",
    "    每个节点跟踪其自身的值Q，先验概率P及其访问次数调整的先前得分u。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent, prior_p):\n",
    "        self._parent = parent\n",
    "        self._children = {}  # 从动作到TreeNode的映射\n",
    "        self._n_visits = 0\n",
    "        self._Q = 0\n",
    "        self._u = 0\n",
    "        self._P = prior_p\n",
    "\n",
    "    def expand(self, action_priors):\n",
    "        \"\"\"通过创建新子项来展开树。\n",
    "     action_priors：一系列动作元组及其先验概率根据策略函数.\n",
    "        \"\"\"\n",
    "        for action, prob in action_priors:\n",
    "            if action not in self._children:\n",
    "                self._children[action] = TreeNode(self, prob)\n",
    "\n",
    "    def select(self, c_puct):\n",
    "        \"\"\"在子节点中选择能够提供最大行动价值Q的行动加上奖金u（P）。\n",
    "     return：（action，next_node）的元组\n",
    "        \"\"\"\n",
    "        return max(self._children.items(),\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\n",
    "\n",
    "    def update(self, leaf_value):\n",
    "        \"\"\"从叶节点评估中更新节点值\n",
    "        leaf_value: 这个子树的评估值来自从当前玩家的视角\n",
    "        \"\"\"\n",
    "        # 统计访问次数\n",
    "        self._n_visits += 1\n",
    "        # 更新Q值,取对于所有访问次数的平均数\n",
    "        self._Q += 1.0 * (leaf_value - self._Q) / self._n_visits\n",
    "\n",
    "    def update_recursive(self, leaf_value):\n",
    "        \"\"\"就像调用update（）一样，但是对所有祖先进行递归应用。\n",
    "        \"\"\"\n",
    "        # 如果它不是根节点，则应首先更新此节点的父节点。\n",
    "        if self._parent:\n",
    "            self._parent.update_recursive(-leaf_value)\n",
    "        self.update(leaf_value)\n",
    "\n",
    "    def get_value(self, c_puct):\n",
    "        \"\"\"计算并返回此节点的值。它是叶评估Q和此节点的先验的组合\n",
    "     调整了访问次数，u。\n",
    "     c_puct：控制相对影响的（0，inf）中的数字，该节点得分的值Q和先验概率P.\n",
    "        \"\"\"\n",
    "        self._u = (c_puct * self._P *\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\n",
    "        return self._Q + self._u\n",
    "\n",
    "    def is_leaf(self):\n",
    "        \"\"\"检查叶节点（即没有扩展的节点）。\"\"\"\n",
    "        return self._children == {}\n",
    "\n",
    "    def is_root(self):\n",
    "        return self._parent is None\n",
    "\n",
    "\n",
    "class MCTS(object):\n",
    "    \"\"\"对蒙特卡罗树搜索的一个简单实现\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=10000, mode='train'):\n",
    "        \"\"\"\n",
    "        policy_value_fn：一个接收板状态和输出的函数（动作，概率）元组列表以及[-1,1]中的分数\n",
    "             （即来自当前的最终比赛得分的预期值玩家的观点）对于当前的玩家。\n",
    "        c_puct：（0，inf）中的数字，用于控制探索的速度收敛于最大值政策。 更高的价值意味着\n",
    "                 依靠先前的更多。\n",
    "        \"\"\"\n",
    "        self._root = TreeNode(None, 1.0)\n",
    "        self._policy = policy_value_fn\n",
    "        self._c_puct = c_puct\n",
    "        self._n_playout = n_playout\n",
    "        self.mode = mode\n",
    "\n",
    "    def _playout(self, state):\n",
    "        \"\"\"从根到叶子运行单个播出，获取值\n",
    "         叶子并通过它的父母传播回来。\n",
    "         State已就地修改，因此必须提供副本。\n",
    "        \"\"\"\n",
    "        node = self._root\n",
    "        while True:\n",
    "            if node.is_leaf():\n",
    "                break\n",
    "            # 贪心算法选择下一步行动\n",
    "            action, node = node.select(self._c_puct)\n",
    "            state.do_move(action)\n",
    "\n",
    "        # 使用网络评估叶子，该网络输出（动作，概率）元组p的列表以及当前玩家的[-1,1]中的分数v。\n",
    "        action_probs, leaf_value = self._policy(state)\n",
    "        # 查看游戏是否结束\n",
    "        end, winner = state.game_end()\n",
    "        if not end:\n",
    "            node.expand(action_probs)\n",
    "        if self.mode == 'train':\n",
    "            if end:\n",
    "                # 对于结束状态,将叶子节点的值换成\"true\"\n",
    "                if winner == -1:  # tie\n",
    "                    leaf_value = 0.0\n",
    "                else:\n",
    "                    leaf_value = (\n",
    "                        1.0 if winner == state.get_current_player() else -1.0\n",
    "                    )\n",
    "        else:\n",
    "            # 通过随机的rollout评估叶子结点\n",
    "            leaf_value = self._evaluate_rollout(state)\n",
    "        # 在本次遍历中更新节点的值和访问次数\n",
    "        node.update_recursive(-leaf_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def _evaluate_rollout(state, limit=1000):\n",
    "        \"\"\"使用推出策略直到游戏结束，\n",
    "        如果当前玩家获胜则返回+1，如果对手获胜则返回-1，\n",
    "        如果是平局则为0。\n",
    "        \"\"\"\n",
    "        player = state.get_current_player()\n",
    "        winner = -1\n",
    "        for i in range(limit):\n",
    "            end, winner = state.game_end()\n",
    "            if end:\n",
    "                break\n",
    "            max_action = np.random.choice(board.availables)\n",
    "            state.do_move(max_action)\n",
    "        else:\n",
    "            # 如果没有从循环中断，请发出警告。\n",
    "            print(\"WARNING: rollout reached move limit\")\n",
    "        if winner == -1:  # tie\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 if winner == player else -1\n",
    "\n",
    "    def get_move(self, state, temp=1e-3):\n",
    "        \"\"\"\n",
    "        如果 prob 为 True，则按顺序运行所有播出并返回可用的操作及其相应的概率。\n",
    "        否则按顺序运行所有播出并返回访问量最大的操作。\n",
    "        \"\"\"\n",
    "        for n in range(self._n_playout):\n",
    "            state_copy = copy.deepcopy(state)\n",
    "            self._playout(state_copy)\n",
    "        if self.mode == 'train':\n",
    "            # 根据根节点处的访问计数来计算移动概率\n",
    "            act_visits = [(act, node._n_visits)\n",
    "                          for act, node in self._root._children.items()]\n",
    "            acts, visits = zip(*act_visits)\n",
    "            act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))\n",
    "\n",
    "            return acts, act_probs\n",
    "\n",
    "        return max(self._root._children.items(),\n",
    "                   key=lambda act_node: act_node[1]._n_visits)[0]\n",
    "\n",
    "    def update_with_move(self, last_move):\n",
    "        \"\"\"保留我们已经知道的关于子树的信息\n",
    "        \"\"\"\n",
    "        if last_move in self._root._children:\n",
    "            self._root = self._root._children[last_move]\n",
    "            self._root._parent = None\n",
    "        else:\n",
    "            self._root = TreeNode(None, 1.0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS\"\n",
    "\n",
    "\n",
    "class MCTSPlayer(object):\n",
    "    \"\"\"基于MCTS的AI玩家\"\"\"\n",
    "\n",
    "    def __init__(self, policy_value_function=policy_value_fn,\n",
    "                 c_puct=5, n_playout=2000, is_selfplay=0, mode='train'):\n",
    "        self.mcts = MCTS(policy_value_function, c_puct, n_playout, mode)\n",
    "        self._is_selfplay = is_selfplay\n",
    "\n",
    "    def set_player_ind(self, p):\n",
    "        self.player = p\n",
    "\n",
    "    def reset_player(self):\n",
    "        self.mcts.update_with_move(-1)\n",
    "\n",
    "    def get_action(self, board, temp=1e-3, return_prob=0):\n",
    "\n",
    "        sensible_moves = board.availables\n",
    "        # 像alphaGo Zero论文一样使用MCTS算法返回的pi向量\n",
    "        move_probs = np.zeros(board.width * board.height)\n",
    "        if len(sensible_moves) > 0:\n",
    "            if self.mcts.mode == 'train':\n",
    "                acts, probs = self.mcts.get_move(board, temp)\n",
    "                move_probs[list(acts)] = probs\n",
    "                if self._is_selfplay:\n",
    "                    # 添加Dirichlet Noise进行探索（自我训练所需）\n",
    "                    move = np.random.choice(\n",
    "                        acts,\n",
    "                        p=0.75 * probs + 0.25 * np.random.dirichlet(0.3 * np.ones(len(probs)))\n",
    "                    )\n",
    "                    # 更新根节点并重用搜索树\n",
    "                    self.mcts.update_with_move(move)\n",
    "                else:\n",
    "                    # 使用默认的temp = 1e-3，它几乎相当于选择具有最高概率的移动\n",
    "                    move = np.random.choice(acts, p=probs)\n",
    "                    # 重置根节点\n",
    "                    self.mcts.update_with_move(-1)\n",
    "\n",
    "                if return_prob:\n",
    "                    return move, move_probs\n",
    "                else:\n",
    "                    return move\n",
    "            else:\n",
    "                move = self.mcts.get_move(board)\n",
    "                self.mcts.update_with_move(-1)\n",
    "                return move\n",
    "        else:\n",
    "            print(\"棋盘已满\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"MCTS {}\".format(self.player)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. 模型训练\n",
    "\n",
    "* AlphaZero的算法流程，概括来说就是通过自我对弈收集数据，并用于更新策略价值网络，更新后的策略价值网络又会被用于后续的自我对弈过程中，从而产生高质量的自我对弈数据，这样相互促进、不断迭代，实现稳定的学习和提升。\n",
    "\n",
    "* 为了加快训练设置棋盘大小为 6×6，自我对弈500轮。\n",
    "\n",
    "### 模型训练过程以及伪代码\n",
    "\n",
    "每轮训练过程中，首先使用上文中提到的蒙特卡洛树搜索及策略价值网络进行一场自我对弈。\n",
    "\n",
    "在一场自我对弈中，每一手棋都通过蒙特卡洛树的四个步骤搜索走法，并得到最终落子的策略概率 Π。 在自我对弈最终结束时，标记胜负，并将其中每一手棋的状态概率以及胜负放入经验池中。\n",
    "\n",
    "最后通过对经验池采样，训练更新策略价值网络。\n",
    "\n",
    "下面给出图示及论文中的伪代码\n",
    "\n",
    "#### 图示\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0841d0771e704488bdbb989e1bdabbe2f82610f471a84a83b48826c35c5da23f)\n",
    "\n",
    "\n",
    "#### 伪代码\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f306ff233cd746208896826bc0ca2f3e2d442b7187fd4fb5815f6b5e688f74ff)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-30T01:20:46.858489Z",
     "iopub.status.busy": "2022-12-30T01:20:46.857980Z",
     "iopub.status.idle": "2022-12-30T04:19:33.342924Z",
     "shell.execute_reply": "2022-12-30T04:19:33.341753Z",
     "shell.execute_reply.started": "2022-12-30T01:20:46.858457Z"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch i:50, episode_len:18, loss :[3.2428253], entropy:2.993258476257324\r\n",
      "batch i:100, episode_len:23, loss :[3.1140473], entropy:2.7185049057006836\r\n",
      "batch i:150, episode_len:14, loss :[3.0530043], entropy:2.62568998336792\r\n",
      "batch i:200, episode_len:16, loss :[2.872697], entropy:2.4276740550994873\r\n",
      "current self-play batch: 200\r\n",
      "New best policy!!!!!!!!\r\n",
      "batch i:250, episode_len:17, loss :[2.8419676], entropy:2.324467182159424\r\n",
      "batch i:300, episode_len:12, loss :[2.8092303], entropy:2.369798183441162\r\n",
      "batch i:350, episode_len:36, loss :[2.61333], entropy:2.25146222114563\r\n",
      "batch i:400, episode_len:16, loss :[2.6112113], entropy:2.2541027069091797\r\n",
      "current self-play batch: 400\r\n",
      "New best policy!!!!!!!!\r\n",
      "batch i:450, episode_len:19, loss :[2.668013], entropy:2.2586355209350586\r\n",
      "batch i:500, episode_len:11, loss :[2.594069], entropy:2.1295700073242188\r\n"
     ]
    }
   ],
   "source": [
    "#  对于五子棋的AlphaZero的训练的实现\n",
    "\n",
    "class TrainPipeline():\n",
    "    def __init__(self, init_model=None, file_path='test'):\n",
    "        # 五子棋逻辑和棋盘UI的参数\n",
    "        self.board_width = 6  ###为了更快的验证算法，可以调整棋盘大小为(8x8) ，(6x6)\n",
    "        self.board_height = 6\n",
    "        self.n_in_row = 5\n",
    "        self.board = Board(width=self.board_width,\n",
    "                           height=self.board_height,\n",
    "                           n_in_row=self.n_in_row)\n",
    "        self.game = Game_UI(self.board)\n",
    "        # 训练参数\n",
    "        self.learn_rate = 2e-3\n",
    "        self.lr_multiplier = 1.0  # 基于KL自适应地调整学习率\n",
    "        self.temp = 1.0  # 临时变量\n",
    "        self.n_playout = 400  # 每次移动的模拟次数\n",
    "        self.c_puct = 5\n",
    "        self.buffer_size = 10000 #经验池大小 10000\n",
    "        self.batch_size = 512  # 训练的mini-batch大小 512\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)\n",
    "        self.play_batch_size = 1\n",
    "        self.epochs = 5  # 每次更新的train_steps数量\n",
    "        self.kl_targ = 0.02\n",
    "        self.check_freq = 200  #评估模型的频率，可以设置大一些比如500\n",
    "        self.game_batch_num = 500\n",
    "        self.best_win_ratio = 0.0\n",
    "        # 用于纯粹的mcts的模拟数量，用作评估训练策略的对手\n",
    "        self.pure_mcts_playout_num = 1000\n",
    "        if init_model:\n",
    "            # 从初始的策略价值网开始训练\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height,\n",
    "                                                   model_file=init_model)\n",
    "        else:\n",
    "            # 从新的策略价值网络开始训练\n",
    "            self.policy_value_net = PolicyValueNet(self.board_width,\n",
    "                                                   self.board_height)\n",
    "        # 定义训练机器人\n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                      c_puct=self.c_puct,\n",
    "                                      n_playout=self.n_playout,\n",
    "                                      is_selfplay=1)\n",
    "        self.file_path = file_path  # 存储训练参数文件位置\n",
    "        self.episode_len = 0\n",
    "\n",
    "    def get_equi_data(self, play_data):\n",
    "        \"\"\"通过旋转和翻转来增加数据集\n",
    "        play_data: [(state, mcts_prob, winner_z), ..., ...]\n",
    "        \"\"\"\n",
    "        extend_data = []\n",
    "        for state, mcts_porb, winner in play_data:\n",
    "            for i in [1, 2, 3, 4]:\n",
    "                # 逆时针旋转\n",
    "                equi_state = np.array([np.rot90(s, i) for s in state])\n",
    "                equi_mcts_prob = np.rot90(np.flipud(\n",
    "                    mcts_porb.reshape(self.board_height, self.board_width)), i)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "                # 水平翻转\n",
    "                equi_state = np.array([np.fliplr(s) for s in equi_state])\n",
    "                equi_mcts_prob = np.fliplr(equi_mcts_prob)\n",
    "                extend_data.append((equi_state,\n",
    "                                    np.flipud(equi_mcts_prob).flatten(),\n",
    "                                    winner))\n",
    "        return extend_data\n",
    "\n",
    "    def collect_selfplay_data(self, n_games=1):\n",
    "        \"\"\"收集自我博弈数据进行训练\"\"\"\n",
    "        for i in range(n_games):\n",
    "            winner, play_data = self.game.start_play_train(self.mcts_player, temp=self.temp)\n",
    "            play_data = list(play_data)\n",
    "            self.episode_len = len(play_data)\n",
    "            # 增加数据\n",
    "            play_data = self.get_equi_data(play_data)\n",
    "            self.data_buffer.extend(play_data)\n",
    "\n",
    "    def update_policy_value_net(self):\n",
    "        \"\"\"更新策略价值网络\"\"\"\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\n",
    "        state_batch = [data[0] for data in mini_batch]\n",
    "        \n",
    "        state_batch= np.array( state_batch).astype(\"float32\")\n",
    "        \n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\n",
    "        mcts_probs_batch= np.array( mcts_probs_batch).astype(\"float32\")\n",
    "        \n",
    "        winner_batch = [data[2] for data in mini_batch]\n",
    "        winner_batch= np.array( winner_batch).astype(\"float32\")\n",
    "        \n",
    "        old_probs, old_v = self.policy_value_net.policy_value_evaluate(state_batch)\n",
    "        loss = kl = 0\n",
    "        for i in range(self.epochs):\n",
    "            loss = self.policy_value_net.train_step(\n",
    "                state_batch,\n",
    "                mcts_probs_batch,\n",
    "                winner_batch,\n",
    "                self.learn_rate * self.lr_multiplier)\n",
    "            new_probs, new_v = self.policy_value_net.policy_value_evaluate(state_batch)\n",
    "            kl = np.mean(np.sum(old_probs * (\n",
    "                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\n",
    "                                axis=1)\n",
    "                         )\n",
    "            if kl > self.kl_targ * 4:  # early stopping if D_KL diverges badly\n",
    "                break\n",
    "        # 自适应调节学习率\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\n",
    "            self.lr_multiplier /= 1.5\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\n",
    "            self.lr_multiplier *= 1.5\n",
    "        return loss\n",
    "\n",
    "    def evaluate_policy_value_net(self, n_games=10):\n",
    "        \"\"\"\n",
    "        通过与纯的MCTS算法对抗来评估训练的策略\n",
    "        注意：这仅用于监控训练进度\n",
    "        \"\"\"\n",
    "        current_mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\n",
    "                                         c_puct=self.c_puct,\n",
    "                                         n_playout=self.n_playout)\n",
    "        pure_mcts_player = MCTSPlayer(c_puct=5,\n",
    "                                     n_playout=self.pure_mcts_playout_num,\n",
    "                                     mode='eval')\n",
    "        win_cnt = defaultdict(int)\n",
    "        for i in range(n_games):\n",
    "            winner = self.game.start_play_evaluate(current_mcts_player,\n",
    "                                          pure_mcts_player,\n",
    "                                          start_player=i % 2)\n",
    "            win_cnt[winner] += 1\n",
    "        win_ratio = 1.0 * (win_cnt[1] + 0.5 * win_cnt[-1]) / n_games\n",
    "        print(\"num_playouts:{}, win: {}, lose: {}, tie:{}\".format(\n",
    "            self.pure_mcts_playout_num,\n",
    "            win_cnt[1], win_cnt[2], win_cnt[-1]))\n",
    "        return win_ratio\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"开始训练\"\"\"\n",
    "        root = os.getcwd()\n",
    "\n",
    "        dst_path = os.path.join(root, self.file_path)\n",
    "\n",
    "        if not os.path.exists(dst_path):\n",
    "            os.makedirs(dst_path)\n",
    "\n",
    "        loss_list = []\n",
    "        try:\n",
    "            for i in range(self.game_batch_num):\n",
    "                self.collect_selfplay_data(self.play_batch_size)\n",
    "                if len(self.data_buffer) > self.batch_size:\n",
    "                    loss = self.update_policy_value_net()\n",
    "                    loss_list.append(loss)\n",
    "\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(\"batch i:{}, episode_len:{}, loss :{},\".format(i + 1, self.episode_len, loss,))\n",
    "                    self.policy_value_net.save_model(os.path.join(dst_path, 'current_policy_step.model'))\n",
    "                # 检查当前模型的性能，保存模型的参数\n",
    "                if (i + 1) % self.check_freq == 0:\n",
    "                    print(\"current self-play batch: {}\".format(i + 1))\n",
    "                    win_ratio = self.evaluate_policy_value_net()\n",
    "                    self.policy_value_net.save_model(os.path.join(dst_path, 'current_policy.model'))\n",
    "                    if win_ratio > self.best_win_ratio:\n",
    "                        print(\"New best policy!!!!!!!!\")\n",
    "                        self.best_win_ratio = win_ratio\n",
    "                        # 更新最好的策略\n",
    "                        self.policy_value_net.save_model(os.path.join(dst_path, 'best_policy.model'))\n",
    "                        if (self.best_win_ratio == 1.0 and\n",
    "                                    self.pure_mcts_playout_num < 8000):\n",
    "                            self.pure_mcts_playout_num += 1000\n",
    "                            self.best_win_ratio = 0.0\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\n\\rquit')\n",
    "        finally:\n",
    "            return loss_list\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        device = paddle.get_device()               \n",
    "        paddle.set_device(device)\n",
    "        # model_path = 'model_ygh/best_policy.model'\n",
    "        # model_path = 'dist/current_policy.model'\n",
    "\n",
    "        # training_pipeline = TrainPipeline(model_path)\n",
    "        training_pipeline = TrainPipeline(None)\n",
    "        loss_list = training_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. 训练结果与展示：\n",
    "\n",
    "### 训练过程\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/870e8e440d4744c69ca10a3c80db77684227c25ae1c14d2e9ae88b25ffea32be)\n",
    "\n",
    "\n",
    "### 最终下棋的效果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/225bd09c070c4177814a408689cb45c0ac8ca042f4e74e81897fc41cb79bc833)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}