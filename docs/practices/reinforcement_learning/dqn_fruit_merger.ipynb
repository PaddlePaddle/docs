{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **强化学习——DQN玩合成大西瓜**\n",
    "**作者：** [莱可可](https://github.com/RedContritio)<br>\n",
    "**日期：** 2022.11 <br>\n",
    "**摘要：** 使用 DQN 算法训练 AI 玩合成大西瓜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **一、介绍**\n",
    "本案例展示了使用DQN玩合成大西瓜的一个实现。\n",
    "\n",
    "### Deep Q-Network (DQN 算法)\n",
    "\n",
    "DQN 是基于深度学习的 Q 学习算法，主要结合了值函数近似和神经网络技术，并采用目标网络和经验回放的方法进行训练。\n",
    "- 目标网络：使用一个固定的 Q 神经网络来计算目标值，这样可以减少训练时的波动。\n",
    "- 经验回放：将经验存储在一个缓冲区中，然后从缓冲区中随机采样，这样可以减少相关性，使得训练更加稳定。\n",
    "\n",
    "\n",
    "### 合成大西瓜\n",
    "合成大西瓜玩法与《2048》《俄罗斯方块》类似，两个相同类别的水果碰撞后，合成成为更大的新水果。其中共有11种水果，最终目标是合成大西瓜。\n",
    "agent需要学会将水果从合适的位置放下，使水果能顺利合成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **二、环境配置**\n",
    "\n",
    "本教程依赖的非 python 官方包如下：\n",
    "\n",
    "- `pymunk`: python 2D物理引擎，用于处理物体碰撞等运动\n",
    "- `opencv-python`: 图形库，用于绘制界面与交互\n",
    "- `numpy`: 数值计算库，主要用于图像处理和模型数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install paddlepaddle pymunk opencv-python numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import typing\n",
    "import enum\n",
    "import cv2\n",
    "import collections\n",
    "import pymunk\n",
    "from threading import Lock\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "from paddle import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **三、实现游戏本体**\n",
    "\n",
    "### 3.1 事件系统\n",
    "\n",
    "事件系统是对游戏与用户交互的抽象，包括鼠标点击与鼠标移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EventType(enum.Enum):\n",
    "    LBUTTONDOWN = (1,)\n",
    "    MOUSEMOVE = (2,)\n",
    "    RBUTTONDOWN = (3,)\n",
    "\n",
    "\n",
    "class Event:\n",
    "    def __init__(self, type: EventType):\n",
    "        self.type = type\n",
    "\n",
    "\n",
    "class MouseEvent(Event):\n",
    "    def __init__(self, type: EventType, pos: typing.Tuple[int, int]):\n",
    "        self.pos = pos\n",
    "        super().__init__(type)\n",
    "\n",
    "\n",
    "# 事件系统基类\n",
    "class GameEventBase:\n",
    "    def __init__(self):\n",
    "        self.__events = []\n",
    "\n",
    "    def add_event(self, event: Event):\n",
    "        self.__events.append(event)\n",
    "\n",
    "    @property\n",
    "    def events(self) -> typing.List[Event]:\n",
    "        _events = self.__events[:]\n",
    "        self.__events.clear()\n",
    "        return _events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 图像渲染\n",
    "\n",
    "游戏的图形界面绘制需要一些辅助函数，如下：\n",
    "\n",
    "- `mix`: 将 `background` 和 `foreground` 基于透明度混合（并给 `foreground` 额外乘算 `alpha` 的透明度），直接在 `background` 上修改\n",
    "- `cover`: 将 `foreground` 基于透明度覆盖到 `background` 上（并给 `foreground` 额外乘算 `alpha` 的透明度），直接在 `background` 上修改\n",
    "- `intersectRect`: 计算两个矩形框 `(x, y, w, h)` 的相交矩形框 `(ix, iy, iw, ih)`\n",
    "- `putText2`: 在 `image` 上以 `center` 为中心，绘制文字 `text` （用法同 `cv2.putText` ）\n",
    "- `putInverseColorText`: 在 `image` 上基于 `pos` 点，调用 `putTextFunc` 绘制文字 `text`，文字与 `image` 反色"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix(\n",
    "    background: np.ndarray, foreground: np.ndarray, alpha: float = 1.0\n",
    ") -> None:\n",
    "    alpha_back = background[:, :, 3] / 255.0\n",
    "    alpha_fore = (foreground[:, :, 3] / 255.0) * alpha\n",
    "\n",
    "    for c in range(3):\n",
    "        background[:, :, c] = np.ubyte(\n",
    "            alpha_fore * foreground[:, :, c]\n",
    "            + alpha_back * background[:, :, c] * (1 - alpha_fore)\n",
    "        )\n",
    "    background[:, :, 3] = np.ubyte(\n",
    "        (1 - (1 - alpha_fore) * (1 - alpha_back)) * 255\n",
    "    )\n",
    "\n",
    "\n",
    "def cover(\n",
    "    background: np.ndarray, foreground: np.ndarray, alpha: float = 1.0\n",
    ") -> None:\n",
    "    foreground_colors = foreground[:, :, :3]\n",
    "    alpha_channel = (foreground[:, :, 3] / 255.0) * alpha\n",
    "    alpha_mask = np.dstack((alpha_channel, alpha_channel, alpha_channel))\n",
    "\n",
    "    background[:, :, :3] = (\n",
    "        background[:, :, :3] * (1 - alpha_mask) + foreground_colors * alpha_mask\n",
    "    )\n",
    "\n",
    "\n",
    "def intersectRect(\n",
    "    rect1: typing.List[int], rect2: typing.List[int]\n",
    ") -> typing.List[int]:\n",
    "    l1, t1, b1, h1 = rect1\n",
    "    r1, b1 = l1 + b1, t1 + h1\n",
    "    l2, t2, b2, h2 = rect2\n",
    "    r2, b2 = l2 + b2, t2 + h2\n",
    "\n",
    "    l, r = max(l1, l2), min(r1, r2)\n",
    "    t, b = max(t1, t2), min(b1, b2)\n",
    "\n",
    "    return [l, t, max(0, r - l), max(0, b - t)]\n",
    "\n",
    "\n",
    "def putText2(\n",
    "    image: np.ndarray,\n",
    "    text: str,\n",
    "    center: typing.List[int],\n",
    "    font_face: int = 0,\n",
    "    font_scale: float = 1.0,\n",
    "    color: typing.List[int] = (255, 255, 255),\n",
    "    thickness: int = 1,\n",
    ") -> None:\n",
    "    INNER_LINE_MARGIN = 5\n",
    "    x, y = center\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    sizes = [\n",
    "        cv2.getTextSize(line, font_face, font_scale, thickness)[0]\n",
    "        for line in lines\n",
    "    ]\n",
    "\n",
    "    h_sum = (\n",
    "        sum([size[1] for size in sizes]) + (len(sizes) - 1) * INNER_LINE_MARGIN\n",
    "    )\n",
    "    w_max = max([size[0] for size in sizes])\n",
    "\n",
    "    y_base = y - h_sum // 2\n",
    "\n",
    "    for i, (w, h) in enumerate(sizes):\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            lines[i],\n",
    "            (x - w // 2, y_base),\n",
    "            font_face,\n",
    "            font_scale,\n",
    "            color,\n",
    "            thickness,\n",
    "        )\n",
    "        y_base += h + INNER_LINE_MARGIN\n",
    "\n",
    "\n",
    "def putInverseColorText(\n",
    "    image: np.ndarray,\n",
    "    text: str,\n",
    "    pos: typing.List[int],\n",
    "    font_face: int = 0,\n",
    "    font_scale: float = 1.0,\n",
    "    thickness: int = 1,\n",
    "    putTextFunc: typing.Callable = putText2,\n",
    ") -> None:\n",
    "    mask = np.zeros((*image.shape[:2], 3), dtype=np.uint8)\n",
    "    putTextFunc(\n",
    "        mask, text, pos, font_face, font_scale, (255, 255, 255), thickness\n",
    "    )\n",
    "\n",
    "    image[:, :, :3] = mask + (2 * (0.5 - mask / 255.0)) * image[:, :, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 实现水果类\n",
    "\n",
    "#### 3.3.1 水果参数\n",
    "\n",
    "定义各种水果的半径、尺寸与对应图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list[0] is nonsense for type 0\n",
    "FRUIT_RADIUS = [\n",
    "    int(1.5 * r) for r in [-1, 10, 15, 21, 23, 29, 35, 37, 50, 59, 60, 78]\n",
    "]\n",
    "FRUIT_SIZES = [(2 * r, 2 * r) for r in FRUIT_RADIUS]\n",
    "\n",
    "FRUIT_IMAGE_PATHS = [f\"res/{i:02d}.png\" for i in range(11)]\n",
    "FRUIT_RAW_IMAGES = [\n",
    "    cv2.imread(FRUIT_IMAGE_PATHS[i], -1) if i > 0 else None for i in range(11)\n",
    "]\n",
    "\n",
    "FRUIT_IMAGES = [\n",
    "    None if img is None else cv2.resize(img, FRUIT_SIZES[i])\n",
    "    for i, img in enumerate(FRUIT_RAW_IMAGES)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 水果类定义\n",
    "\n",
    "定义所有水果的类 `class Fruit`。\n",
    "\n",
    "每个水果对应一个该类型实例，主要用于描述水果对象的位置。\n",
    "\n",
    "其中，水果的种类 `type` 为 `[1, 11]`，其中 `1` 是葡萄，`11` 是大西瓜。\n",
    "\n",
    "当两个种类为 `x` 的水果碰撞时，合成出一个种类为 `x+1` 的新水果，并在游戏中获得 `x+1` 分；\n",
    "\n",
    "特别的，当 `x` 为 `10` 时，合成出的新水果为 `11` （大西瓜），并在游戏中获得 `100` 分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fruit:\n",
    "    def __init__(self, type: int, x: int, y: int) -> None:\n",
    "        self.type = type\n",
    "        self.r = FRUIT_RADIUS[self.type]\n",
    "        self.size = FRUIT_SIZES[self.type]\n",
    "\n",
    "        self.x, self.y = x, y\n",
    "\n",
    "    def update_position(self, x: int, y: int) -> None:\n",
    "        self.x, self.y = x, y\n",
    "\n",
    "    def draw(self, screen: np.ndarray) -> None:\n",
    "        Fruit.paint(screen, self.type, self.x, self.y)\n",
    "\n",
    "    def paint(\n",
    "        screen: np.ndarray, type: int, x: int, y: int, alpha: float = 1.0\n",
    "    ) -> None:\n",
    "        assert type > 0 and type <= 11\n",
    "        l, t = (x - FRUIT_RADIUS[type], y - FRUIT_RADIUS[type])\n",
    "        w, h = FRUIT_SIZES[type]\n",
    "\n",
    "        l, t, w, h = [int(v) for v in (l, t, w, h)]\n",
    "\n",
    "        il, it, iw, ih = [\n",
    "            int(v)\n",
    "            for v in intersectRect((l, t, w, h), (0, 0, *screen.shape[1::-1]))\n",
    "        ]\n",
    "\n",
    "        cover(\n",
    "            screen[it : it + ih, il : il + iw],\n",
    "            FRUIT_IMAGES[type][it - t : it - t + ih, il - l : il - l + iw],\n",
    "            alpha=alpha,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 游戏本体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 物理引擎重力\n",
    "GRAVITY = (0, 800)\n",
    "# 游戏场景大小\n",
    "GAME_RESOLUTION = GAME_WIDTH, GAME_HEIGHT = 300, 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameCore(GameEventBase):\n",
    "    def __init__(self, gravity: typing.Tuple[int, int] = GRAVITY) -> None:\n",
    "        self.resolution = self.width, self.height = GAME_WIDTH, GAME_HEIGHT\n",
    "        self.init_x = int(self.width / 2)\n",
    "        self.init_y = int(0.15 * self.height)\n",
    "\n",
    "        self.score = 0\n",
    "        self.recent_score_delta = 0\n",
    "\n",
    "        self.fruits: typing.List[Fruit] = []\n",
    "        self.balls: typing.List[pymunk.Shape] = []\n",
    "\n",
    "        self.background_color = (0xE1, 0x69, 0x41, 0)\n",
    "        self.preset_background = np.zeros(\n",
    "            (self.height, self.width, 4), dtype=np.uint8\n",
    "        )\n",
    "        self.preset_background[:, :] = self.background_color\n",
    "        self.preset_redline_screen = self.preset_background.copy()\n",
    "        cv2.line(\n",
    "            self.preset_redline_screen,\n",
    "            (0, self.init_y),\n",
    "            (self.width, self.init_y),\n",
    "            (0, 0, 255),\n",
    "            2,\n",
    "        )\n",
    "        self.__screen = self.preset_background.copy()\n",
    "\n",
    "        self.lock = Lock()\n",
    "        self.render_lock = Lock()\n",
    "\n",
    "        self.stable_frame_threshold = 10\n",
    "        self.current_frame_id = 0\n",
    "        self.stable_frame_id = (\n",
    "            self.current_frame_id - self.stable_frame_threshold\n",
    "        )\n",
    "        self.clickable = False\n",
    "\n",
    "        self.largest_fruit_type = 1\n",
    "        self.current_fruit_type = self.create_random_fruit_type()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        self.space = pymunk.Space()\n",
    "        self.space.gravity = gravity\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "        self.init_segment()\n",
    "        self.setup_collision_handler()\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    # 重启游戏\n",
    "    def reset(self):\n",
    "        for ball in self.balls:\n",
    "            self.space.remove(ball, ball.body)\n",
    "\n",
    "        self.prev_score, self.score = 0, 0\n",
    "\n",
    "        self.fruits.clear()\n",
    "        self.balls.clear()\n",
    "\n",
    "        self.current_frame_id = 0\n",
    "        self.stable_frame_id = (\n",
    "            self.current_frame_id - self.stable_frame_threshold\n",
    "        )\n",
    "        self.prev_stable_frame_id = self.stable_frame_id\n",
    "        self.clickable = False\n",
    "\n",
    "        self.largest_fruit_type = 1\n",
    "        self.current_fruit_type = self.create_random_fruit_type()\n",
    "\n",
    "        self.alive = True\n",
    "\n",
    "    # 初始化游戏场地边界（物理），防止球体掉出游戏场地\n",
    "    def init_segment(self, thinkness: float = 20, friction: float = 0.6):\n",
    "        l, t = 0 - thinkness, 0 - thinkness - self.height // 2\n",
    "        r, b = self.width + thinkness, self.height + thinkness\n",
    "\n",
    "        def create_segment(\n",
    "            p1: typing.Tuple[int, int], p2: typing.Tuple[int, int]\n",
    "        ) -> pymunk.Segment:\n",
    "            s = pymunk.Segment(self.space.static_body, p1, p2, thinkness)\n",
    "            s.friction = friction\n",
    "            return s\n",
    "\n",
    "        self.space.add(create_segment((l, t), (l, b)))\n",
    "        self.space.add(create_segment((r, t), (r, b)))\n",
    "        # no top wall\n",
    "        # self.space.add(create_segment((l, t), (r, t)))\n",
    "        self.space.add(create_segment((l, b), (r, b)))\n",
    "\n",
    "    # 初始化碰撞检测\n",
    "    def setup_collision_handler(self):\n",
    "        def collision_post_solve(\n",
    "            arbiter: pymunk.Arbiter, space: pymunk.Space, _data\n",
    "        ):\n",
    "            with self.lock:\n",
    "                s0, s1 = arbiter.shapes[:2]\n",
    "                new_type = s0.collision_type + 1\n",
    "                x1, y1 = s0.body.position\n",
    "                x2, y2 = s1.body.position\n",
    "                x, y = (x1, y1) if y1 > y2 else (x2, y2)\n",
    "\n",
    "                if s0 in self.balls and s1 in self.balls:\n",
    "                    self.remove_ball(space, s0)\n",
    "                    self.remove_ball(space, s1)\n",
    "\n",
    "                    fruit = Fruit(new_type, x, self.init_y)\n",
    "                    self.fruits.append(fruit)\n",
    "\n",
    "                    ball = self.create_ball(\n",
    "                        self.space, x, y, fruit.r // 10, fruit.r - 1, new_type\n",
    "                    )\n",
    "                    self.balls.append(ball)\n",
    "\n",
    "                    self.largest_fruit_type = max(\n",
    "                        self.largest_fruit_type, new_type\n",
    "                    )\n",
    "                    self.recent_score_delta = new_type if new_type < 11 else 100\n",
    "                    self.score += self.recent_score_delta\n",
    "\n",
    "        for collision_type in range(1, 11):\n",
    "            self.space.add_collision_handler(\n",
    "                collision_type, collision_type\n",
    "            ).post_solve = collision_post_solve\n",
    "\n",
    "    # 创建随机水果类型\n",
    "    def create_random_fruit_type(self) -> int:\n",
    "        return random.randint(1, min(self.largest_fruit_type, 5))\n",
    "\n",
    "    # 创建水果\n",
    "    def create_fruit(self, type: int, x: int) -> Fruit:\n",
    "        return Fruit(type, x, self.init_y - FRUIT_RADIUS[type])\n",
    "\n",
    "    # 创建球体（物理）\n",
    "    def create_ball(\n",
    "        self,\n",
    "        space: pymunk.Space,\n",
    "        x: int,\n",
    "        y: int,\n",
    "        mass: int = 1,\n",
    "        radius: int = 7,\n",
    "        type: int = 1,\n",
    "    ) -> pymunk.Shape:\n",
    "        ball_moment = pymunk.moment_for_circle(mass, 0, radius)\n",
    "        ball_body = pymunk.Body(mass, ball_moment)\n",
    "        ball_body.position = x, y\n",
    "        ball_shape = pymunk.Circle(ball_body, radius)\n",
    "        ball_shape.elasticity = 0.3\n",
    "        ball_shape.friction = 0.6\n",
    "        ball_shape.collision_type = type\n",
    "        space.add(ball_body, ball_shape)\n",
    "        return ball_shape\n",
    "\n",
    "    # 移除球体（物理）\n",
    "    def remove_ball(self, space: pymunk.Space, ball: pymunk.Circle):\n",
    "        p = self.balls.index(ball)\n",
    "\n",
    "        space.remove(ball, ball.body)\n",
    "\n",
    "        self.balls.pop(p)\n",
    "        self.fruits.pop(p)\n",
    "\n",
    "    # 保存游戏画面\n",
    "    def save_screen(self, path: str = \"screenshot.png\") -> bool:\n",
    "        rgb_img = cv2.cvtColor(self.screen, cv2.COLOR_BGRA2BGR)\n",
    "        return cv2.imwrite(path, rgb_img)\n",
    "\n",
    "    # 绘制游戏画面\n",
    "    def draw(self) -> np.ndarray:\n",
    "        backbuffer = self.preset_background.copy()\n",
    "\n",
    "        # if self.clickable:\n",
    "        if self.current_fruit_type > 0:\n",
    "            y = self.init_y - FRUIT_RADIUS[self.current_fruit_type]\n",
    "            Fruit.paint(\n",
    "                backbuffer,\n",
    "                self.current_fruit_type,\n",
    "                self.init_x,\n",
    "                y,\n",
    "                1 if self.clickable else 0.5,\n",
    "            )\n",
    "\n",
    "        for f in self.fruits:\n",
    "            f.draw(backbuffer)\n",
    "\n",
    "        cv2.addWeighted(\n",
    "            backbuffer, 1, self.preset_redline_screen, 0.5, 0, backbuffer\n",
    "        )\n",
    "\n",
    "        putInverseColorText(\n",
    "            backbuffer,\n",
    "            f\"Score: {self.score}\",\n",
    "            (0, 20),\n",
    "            font_scale=0.7,\n",
    "            thickness=1,\n",
    "            putTextFunc=cv2.putText,\n",
    "        )\n",
    "\n",
    "        if not self.alive:\n",
    "            putInverseColorText(\n",
    "                backbuffer,\n",
    "                f\"Failed\\nClick RButton to Restart\",\n",
    "                (int(self.width / 2), int(self.height / 2)),\n",
    "                font_scale=0.7,\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        with self.render_lock:\n",
    "            self.__screen[:, :, :] = backbuffer\n",
    "            return self.__screen\n",
    "\n",
    "    # 获取游戏画面\n",
    "    @property\n",
    "    def screen(self) -> np.ndarray:\n",
    "        with self.render_lock:\n",
    "            return self.__screen\n",
    "\n",
    "    # 获取特征，用于神经网络输入\n",
    "    def get_features(\n",
    "        self, class_count: int = 11, memory_size: int = 15\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        params:\n",
    "            - class_count: default 11, 1 ~ 11\n",
    "            - memory_size: default 15\n",
    "\n",
    "        return:\n",
    "            - features: (memory_size + 1, class_count + 2)\n",
    "        \"\"\"\n",
    "        feature_current = np.zeros((1, class_count + 2))\n",
    "        feature_current[0, self.current_fruit_type] = 1\n",
    "        feature_current[0, 0] = 0.5\n",
    "\n",
    "        feature_fruits = np.zeros((memory_size, class_count + 2))\n",
    "\n",
    "        recent_fruit = [(f.x, f.y, f.type) for f in self.fruits]\n",
    "\n",
    "        recent_fruit.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i, (x, y, t) in enumerate(recent_fruit[:memory_size]):\n",
    "            feature_fruits[i, t] = 1\n",
    "            feature_fruits[i, 0] = x / self.width\n",
    "            feature_fruits[i, -1] = y / self.height\n",
    "\n",
    "        feature = np.concatenate((feature_current, feature_fruits), axis=0)\n",
    "        return feature\n",
    "\n",
    "    # 更新至下一可操作帧或游戏结束\n",
    "    def update_until_stable(self, fps: float = 60, max_seconds: int = 5):\n",
    "        self.set_unstable()\n",
    "\n",
    "        max_steps = int(fps * max_seconds)\n",
    "        step = 0\n",
    "\n",
    "        while (\n",
    "            self.current_frame_id\n",
    "            <= self.stable_frame_id + self.stable_frame_threshold\n",
    "            and step < max_steps\n",
    "        ):\n",
    "            self.update(1.0 / fps)\n",
    "            step += 1\n",
    "\n",
    "        if step == max_steps:\n",
    "            self.clickable = True\n",
    "\n",
    "    # 更新一帧\n",
    "    def update(self, time_delta: float):\n",
    "        self.current_frame_id += 1\n",
    "        self.space.step(time_delta)\n",
    "\n",
    "        stable = self.check_stable()\n",
    "        if not stable:\n",
    "            self.set_unstable()\n",
    "\n",
    "        self.alive = self.alive and self.check_alive()\n",
    "        if not self.alive:\n",
    "            for event in self.events:\n",
    "                if event.type == EventType.RBUTTONDOWN:\n",
    "                    self.reset()\n",
    "                    break\n",
    "            return\n",
    "\n",
    "        if (\n",
    "            not self.clickable\n",
    "            and self.current_frame_id\n",
    "            > self.stable_frame_id + self.stable_frame_threshold\n",
    "        ):\n",
    "            self.prev_stable_frame_id = self.stable_frame_id\n",
    "            self.clickable = True\n",
    "\n",
    "        for event in self.events:\n",
    "            if event.type == EventType.LBUTTONDOWN and self.clickable:\n",
    "                x, _y = event.pos\n",
    "\n",
    "                fruit = self.create_fruit(self.current_fruit_type, x)\n",
    "                self.fruits.append(fruit)\n",
    "\n",
    "                y = self.init_y - fruit.r\n",
    "                ball = self.create_ball(\n",
    "                    self.space,\n",
    "                    x,\n",
    "                    y,\n",
    "                    (fruit.r // 10) ** 2,\n",
    "                    fruit.r - 1,\n",
    "                    self.current_fruit_type,\n",
    "                )\n",
    "                self.balls.append(ball)\n",
    "\n",
    "                self.current_fruit_type = self.create_random_fruit_type()\n",
    "                self.set_unstable()\n",
    "                self.clickable = False\n",
    "\n",
    "            elif event.type == EventType.MOUSEMOVE:\n",
    "                self.init_x, _y = event.pos\n",
    "                self.init_x = max(\n",
    "                    self.init_x, 0 + FRUIT_RADIUS[self.current_fruit_type]\n",
    "                )\n",
    "                self.init_x = min(\n",
    "                    self.init_x,\n",
    "                    self.width - FRUIT_RADIUS[self.current_fruit_type],\n",
    "                )\n",
    "\n",
    "        assert not self.lock.locked()\n",
    "\n",
    "        with self.lock:\n",
    "            for i, ball in enumerate(self.balls):\n",
    "                x, y = ball.body.position\n",
    "\n",
    "                self.fruits[i].update_position(x, y)\n",
    "\n",
    "    # 获取所有球体最大的 y 方向速度\n",
    "    @property\n",
    "    def max_balls_velocity_y(self) -> float:\n",
    "        return (\n",
    "            max([abs(ball.body.velocity.y) for ball in self.balls])\n",
    "            if len(self.balls) > 0\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "    # 设置为不稳定状态\n",
    "    def set_unstable(self) -> None:\n",
    "        self.stable_frame_id = self.current_frame_id + 1\n",
    "\n",
    "    # 检查是否稳定\n",
    "    def check_stable(self) -> bool:\n",
    "        return self.max_balls_velocity_y < 20\n",
    "\n",
    "    # 检查是否存活\n",
    "    def check_alive(self) -> bool:\n",
    "        if (\n",
    "            self.current_frame_id\n",
    "            > self.stable_frame_id + self.stable_frame_threshold\n",
    "        ):\n",
    "            for f in self.fruits:\n",
    "                if f.y < self.init_y:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    # 左键单击，加入事件队列\n",
    "    def click(self, pos: typing.Tuple[int, int]):\n",
    "        self.add_event(MouseEvent(EventType.LBUTTONDOWN, pos))\n",
    "\n",
    "    # 鼠标移动，加入事件队列\n",
    "    def move(self, pos: typing.Tuple[int, int]):\n",
    "        self.add_event(MouseEvent(EventType.MOUSEMOVE, pos))\n",
    "\n",
    "    # 右键单击，加入事件队列\n",
    "    def rclick(self, pos: typing.Tuple[int, int]):\n",
    "        self.add_event(MouseEvent(EventType.RBUTTONDOWN, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 游戏接口\n",
    "\n",
    "封装接口，提供强化学习环境\n",
    "\n",
    "提供以下接口：\n",
    "\n",
    "- `reset`: 重启游戏\n",
    "- `simulate_until_stable`: 运行游戏，直到游戏结束或者可以进行下一次操作\n",
    "- `next`: 输入 `action`，进行一次模拟，并返回 `(feature, reward, alive)` 三元组\n",
    "\n",
    "其中，`reward` 定义为，一次动作 `action` 后，能带来的 `score` 提升。\n",
    "\n",
    "特别的，当 `score` 未改变时，该动作由于减少了空间，其 `reward` 设置为负数 `-fruit.type` （记本次动作放下的水果为 `fruit`） \n",
    "\n",
    "定义模拟时，每秒帧数为 `SIMULATE_FPS = 60` 帧；可进行的动作种类为 `ACTION_NUM = 16` 种（在16个均匀分布的水平坐标处放下水果）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameInterface:\n",
    "    ACTION_NUM = 16\n",
    "    SIMULATE_FPS = 60\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.game = GameCore()\n",
    "        self.action_num = GameInterface.ACTION_NUM\n",
    "        self.action_segment_len = self.game.width / GameInterface.ACTION_NUM\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.game.reset()\n",
    "\n",
    "    def simulate_until_stable(self) -> None:\n",
    "        self.game.update_until_stable(GameInterface.SIMULATE_FPS)\n",
    "\n",
    "    def decode_action(self, action: int) -> typing.Tuple[int, int]:\n",
    "        x = int((action + 0.5) * self.action_segment_len)\n",
    "\n",
    "        return (x, 0)\n",
    "\n",
    "    def next(self, action: int) -> typing.Tuple[np.ndarray, int, bool]:\n",
    "        self.simulate_until_stable()\n",
    "\n",
    "        feature = self.game.get_features()\n",
    "        current_fruit = self.game.current_fruit_type\n",
    "\n",
    "        score_1 = self.game.score\n",
    "\n",
    "        self.game.click(self.decode_action(action))\n",
    "        self.simulate_until_stable()\n",
    "\n",
    "        score_2 = self.game.score\n",
    "\n",
    "        score, reward, alive = (\n",
    "            self.game.score,\n",
    "            score_2 - score_1,\n",
    "            self.game.alive,\n",
    "        )\n",
    "\n",
    "        reward = reward if reward > 0 else -current_fruit\n",
    "\n",
    "        flatten_feature = np.expand_dims(feature.flatten(), axis=0).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "        return flatten_feature, reward, alive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **四、实现智能体与经验池**\n",
    "\n",
    "### 4.1 构建网络\n",
    "\n",
    "定义函数 `build_model`，调用后获得构建的全连接网络。\n",
    "\n",
    "该网络含有两个全连接层，每层 `256` 个节点，并采用 `ReLU` 进行激活。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size: int, output_size: int) -> nn.Layer:\n",
    "    model_prototype = nn.Sequential(\n",
    "        nn.Linear(in_features=input_size, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=256, out_features=output_size),\n",
    "    )\n",
    "\n",
    "    return model_prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 构建经验池\n",
    "\n",
    "经验池可以用来持久化 `experience` （经验），并消除各个 `experience` 之间的相关性。\n",
    "\n",
    "每个 `experience` 主要用于记录 `state` （状态）、`action` （动作） 和 `reward` （奖励）的关联，在强化学习中，通常使用 `(state, action, new_state, reward)` ，以表示状态转移与动作、奖励的关联。\n",
    "\n",
    "在该项目中，使用 `feature` 表示 `state`，并加入 `alive` （存活与否）表示游戏状态。\n",
    "\n",
    "`experience` 的结构为：\n",
    "\n",
    "`experience`:\n",
    "- `feature`: 动作前的状态（特征）\n",
    "- `action`: 进行的动作\n",
    "- `reward`: 动作获得的奖励\n",
    "- `next_feature`: 动作后的状态（特征）\n",
    "- `alive`: 游戏是否仍能进行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 50000\n",
    "MEMORY_WARMUP_SIZE = 1000\n",
    "\n",
    "\n",
    "class ReplayMemory(collections.deque):\n",
    "    def __init__(self, max_size: int = MEMORY_SIZE) -> None:\n",
    "        super().__init__(maxlen=max_size)\n",
    "\n",
    "    def sample(\n",
    "        self, batch_size: int\n",
    "    ) -> typing.Tuple[\n",
    "        np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray\n",
    "    ]:\n",
    "        mini_batch = random.sample(self, batch_size)\n",
    "\n",
    "        # feature_batch, action_batch, reward_batch, next_feature_batch, alive_batch = experiences\n",
    "        experiences = list(zip(*mini_batch))\n",
    "\n",
    "        return tuple([np.array(exp) for exp in experiences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 构建 Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 定义 Agent 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 构建随机 Agent\n",
    "\n",
    "构建一个随机动作的 Agent 作为 baseline，用于效果检验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_num: int) -> None:\n",
    "        self.action_num = action_num\n",
    "\n",
    "    def sample(self, _feature: np.ndarray) -> int:\n",
    "        return np.random.randint(0, self.action_num, size=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 构建DQN Agent\n",
    "\n",
    "DQN 使用两个结构相同、参数不同的神经网络来训练，`policy_net` 用于学习，每次训练都更新，而 `target_net` 在训练过程中比较固定，定期更新，负责产生目标。\n",
    "\n",
    "优化目标为 $Q^\\pi(s_t, a_t) = r_t + Q^\\pi(s_{t+1}, \\pi(s_{t+1}))$\n",
    "\n",
    "其中，左侧 $Q^\\pi(s_t, a_t)$ 为模型输出，即 `policy_net`，\n",
    "右侧 $r_t + Q^\\pi(s_{t+1}, \\pi(s_{t+1}))$ 则为目标 `target`，即 `target_net`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        build_model: typing.Callable,\n",
    "        feature_dim: int,\n",
    "        action_num: int,\n",
    "        e_greed: float = 0.1,\n",
    "        e_greed_decrement: float = 1e-6,\n",
    "        learning_rate: float = LEARNING_RATE,\n",
    "        loss_func: typing.Callable[\n",
    "            [paddle.Tensor, paddle.Tensor], paddle.Tensor\n",
    "        ] = nn.MSELoss(\"mean\"),\n",
    "    ) -> None:\n",
    "        self.policy_net = build_model(feature_dim, action_num)\n",
    "        self.target_net = build_model(feature_dim, action_num)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.action_num = action_num\n",
    "        self.e_greed = e_greed\n",
    "        self.e_greed_decrement = e_greed_decrement\n",
    "\n",
    "        self.loss_func = loss_func\n",
    "        self.optimizer = paddle.optimizer.Adam(\n",
    "            parameters=self.policy_net.parameters(), learning_rate=learning_rate\n",
    "        )\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.update_target_steps = 200\n",
    "\n",
    "    def sample(self, feature: np.ndarray) -> int:\n",
    "        if np.random.uniform() < self.e_greed:\n",
    "            action = np.random.randint(0, self.action_num, size=(1))\n",
    "        else:\n",
    "            action = self.predict(feature)\n",
    "\n",
    "        self.e_greed = max(0, self.e_greed - self.e_greed_decrement)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def predict(self, feature: np.ndarray) -> np.ndarray:\n",
    "        with paddle.no_grad():\n",
    "            action = self.policy_net(paddle.to_tensor(feature)).argmax()\n",
    "        return action.numpy()\n",
    "\n",
    "    def learn(\n",
    "        self,\n",
    "        feature: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        next_feature: np.ndarray,\n",
    "        alive: bool,\n",
    "    ):\n",
    "        if self.global_step % self.update_target_steps == 0:\n",
    "            self.target_net.load_dict(self.policy_net.state_dict())\n",
    "            pass\n",
    "\n",
    "        self.global_step += 1\n",
    "\n",
    "        feature_batch = paddle.to_tensor(feature, dtype=\"float32\")\n",
    "        action_batch = paddle.to_tensor(action, dtype=\"int32\")\n",
    "        reward_batch = paddle.to_tensor(reward, dtype=\"float32\")\n",
    "        next_feature_batch = paddle.to_tensor(next_feature, dtype=\"float32\")\n",
    "        alive_batch = paddle.to_tensor(alive, dtype=\"float32\")\n",
    "\n",
    "        output_policy = paddle.squeeze(self.policy_net(feature_batch))\n",
    "        action_batch = paddle.squeeze(action_batch)\n",
    "        # print(action_batch, self.action_num)\n",
    "        action_batch_onehot = nn.functional.one_hot(\n",
    "            action_batch, self.action_num\n",
    "        )\n",
    "\n",
    "        # print(paddle.multiply(output_policy, action_batch_onehot).shape)\n",
    "        policy_q_value = paddle.sum(\n",
    "            paddle.multiply(output_policy, action_batch_onehot), axis=1\n",
    "        )\n",
    "\n",
    "        with paddle.no_grad():\n",
    "            output_target_next = paddle.squeeze(\n",
    "                self.target_net(next_feature_batch)\n",
    "            )\n",
    "            target_next_q_value = paddle.max(output_target_next, axis=1)\n",
    "\n",
    "        target_q_value = paddle.squeeze(reward_batch) + GAMMA * paddle.squeeze(\n",
    "            target_next_q_value\n",
    "        ) * paddle.squeeze(alive_batch)\n",
    "\n",
    "        # print(policy_q_value.shape, target_q_value.shape)\n",
    "        loss = self.loss_func(policy_q_value, target_q_value)\n",
    "\n",
    "        self.optimizer.clear_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **五、实现训练方法**\n",
    "\n",
    "### 5.1 定义训练超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_FREQUENCY = 5\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 运行一局\n",
    "\n",
    "基于传入的 `env` （游戏环境）、`agent` （智能体） 进行一局游戏，直至游戏结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env: GameInterface, agent: Agent, memory: ReplayMemory):\n",
    "    env.reset()\n",
    "\n",
    "    step, rewards_sum = 0, 0\n",
    "    action = np.random.randint(0, env.action_num)\n",
    "    feature, _, alive = env.next(action)\n",
    "\n",
    "    assert alive\n",
    "\n",
    "    while alive:\n",
    "        step += 1\n",
    "\n",
    "        action = agent.sample(feature)\n",
    "        next_feature, reward, alive = env.next(action)\n",
    "\n",
    "        memory.append((feature, action, reward, next_feature, alive))\n",
    "\n",
    "        if (\n",
    "            len(memory) >= MEMORY_WARMUP_SIZE\n",
    "            and agent.global_step % LEARN_FREQUENCY == 0\n",
    "        ):\n",
    "            (\n",
    "                feature_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "                next_feature_batch,\n",
    "                alive_batch,\n",
    "            ) = memory.sample(BATCH_SIZE)\n",
    "\n",
    "            _loss = agent.learn(\n",
    "                feature_batch,\n",
    "                action_batch,\n",
    "                reward_batch,\n",
    "                next_feature_batch,\n",
    "                alive_batch,\n",
    "            )\n",
    "\n",
    "        reward_sum = np.sum(reward)\n",
    "        rewards_sum += reward_sum\n",
    "\n",
    "        feature = next_feature\n",
    "    return rewards_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **六、开始训练**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 初始化训练环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = 11  # 水果种类\n",
    "memory_size = 15  # 特征记录的水果个数\n",
    "\n",
    "action_dim = GameInterface.ACTION_NUM  # 动作种类数\n",
    "feature_dim = (class_count + 2) * (memory_size + 1)  ## 特征维度\n",
    "e_greed = 0.4  # 贪心策略的概率\n",
    "e_greed_decrement = 4e-6  # 贪心策略的概率衰减\n",
    "\n",
    "env = GameInterface()  # 创建游戏环境\n",
    "\n",
    "memory = ReplayMemory(MEMORY_SIZE)  # 创建经验池\n",
    "\n",
    "agent = Agent(\n",
    "    build_model, feature_dim, action_dim, e_greed, e_greed_decrement\n",
    ")  # 创建智能体\n",
    "\n",
    "FINAL_PARAM_PATH = \"final.pdparams\"  # 模型保存路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 尝试加载已有模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(FINAL_PARAM_PATH):\n",
    "    agent.policy_net.set_state_dict(paddle.load(FINAL_PARAM_PATH))\n",
    "    print(\"Loaded final param.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 预热经验池\n",
    "\n",
    "先在经验池中填充经验，避免前期训练时选取经验相关度过高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(memory) < MEMORY_WARMUP_SIZE:\n",
    "    run_episode(env, agent, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode = 20000\n",
    "episode_per_log = max_episode // 20\n",
    "\n",
    "for episode_id in range(1, max_episode + 1):\n",
    "    total_reward = run_episode(env, agent, memory)\n",
    "\n",
    "    if episode_id % episode_per_log == 0:\n",
    "        print(\n",
    "            f\"episode: {episode_id}, reward: {total_reward}, e_greed: {agent.e_greed}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paddle.save(agent.policy_net.state_dict(), FINAL_PARAM_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **七、评估模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 评估训练结果\n",
    "\n",
    "基于给定的 `env` 和 `agent`，进行一定数量局数的游戏。\n",
    "\n",
    "在游戏结束后，返回每局的平均得分和平均总奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATE_TIMES = 50\n",
    "\n",
    "\n",
    "def evaluate(env: GameInterface, agent: Agent) -> typing.Tuple[float, float]:\n",
    "    scores, rewards_sums = [], []\n",
    "    for _ in range(EVALUATE_TIMES):\n",
    "        env.reset()\n",
    "        action = np.random.randint(0, env.action_num)\n",
    "        feature, _, alive = env.next(action)\n",
    "        rewards_sum = 0\n",
    "\n",
    "        while alive:\n",
    "            action = agent.sample(feature)\n",
    "            feature, reward, alive = env.next(action)\n",
    "\n",
    "            reward_sum = np.sum(reward)\n",
    "            rewards_sum += reward_sum\n",
    "\n",
    "        scores.append(env.game.score)\n",
    "        rewards_sums.append(rewards_sum)\n",
    "\n",
    "    return np.mean(scores), np.mean(rewards_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 与随机 Agent 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_random(\n",
    "    env: GameInterface, agent: Agent, action_count: int\n",
    ") -> None:\n",
    "    mean_score, mean_reward = evaluate(env, agent)\n",
    "    print(\"DQN Agent:\")\n",
    "    print(f\"mean_score: {mean_score}, mean_reward: {mean_reward}\")\n",
    "\n",
    "    random_agent = RandomAgent(action_count)\n",
    "    mean_score, mean_reward = evaluate(env, random_agent)\n",
    "    print(\"Random Agent:\")\n",
    "    print(f\"mean_score: {mean_score}, mean_reward: {mean_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 开始评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_with_random(env, agent, action_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fa163922eb0b3709bbb5d8082b2465c9de796dbaacca80cbaa600e7fff3e4fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
