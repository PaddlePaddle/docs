é€šè¿‡DCGANå®ç°äººè„¸å›¾åƒç”Ÿæˆ
=========================

| ä½œè€…:`ZMpursue <https://github.com/ZMpursue>`__
| æ—¥æœŸ:2020.10.26

æœ¬æ•™ç¨‹å°†é€šè¿‡ä¸€ä¸ªç¤ºä¾‹å¯¹DCGANè¿›è¡Œä»‹ç»ã€‚åœ¨å‘å…¶å±•ç¤ºè®¸å¤šçœŸå®äººè„¸ç…§ç‰‡ï¼ˆæ•°æ®é›†ï¼š\ `Celeb-A
Face <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__\ ï¼‰åï¼Œæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ¥äº§ç”Ÿæ–°äººè„¸ã€‚æœ¬æ–‡å°†å¯¹è¯¥å®ç°è¿›è¡Œè¯¦å°½çš„è§£é‡Šï¼Œå¹¶é˜æ˜æ­¤æ¨¡å‹çš„å·¥ä½œæ–¹å¼å’ŒåŸå› ã€‚å¹¶ä¸éœ€è¦è¿‡å¤šä¸“ä¸šçŸ¥è¯†ï¼Œä½†æ˜¯å¯èƒ½éœ€è¦æ–°æ‰‹èŠ±ä¸€äº›æ—¶é—´æ¥ç†è§£çš„æ¨¡å‹è®­ç»ƒçš„å®é™…æƒ…å†µã€‚ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œè¯·å°½é‡é€‰æ‹©GPUè¿›è¡Œè®­ç»ƒã€‚

1 ç®€ä»‹
------

æœ¬é¡¹ç›®åŸºäºpaddlepaddleï¼Œç»“åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆDCGANï¼‰,é€šè¿‡å¼±ç›‘ç£å­¦ä¹ çš„æ–¹å¼ï¼Œè®­ç»ƒç”ŸæˆçœŸå®äººè„¸ç…§ç‰‡

1.1 ä»€ä¹ˆæ˜¯GANï¼Ÿ
~~~~~~~~~~~~~~~

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGenerative Adversarial Network
[1]ï¼Œç®€ç§°GANï¼‰æ˜¯éç›‘ç£å¼å­¦ä¹ çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡è®©ä¸¤ä¸ªç¥ç»ç½‘ç»œç›¸äº’åšå¼ˆçš„æ–¹å¼è¿›è¡Œå­¦ä¹ ã€‚è¯¥æ–¹æ³•æœ€åˆç”±
lanÂ·Goodfellow ç­‰äººäº2014å¹´æå‡ºï¼ŒåŸè®ºæ–‡è§ `Generative Adversarial
Network <https://arxiv.org/abs/1406.2661>`__\ ã€‚

ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç”±ä¸€ä¸ªç”Ÿæˆç½‘ç»œä¸ä¸€ä¸ªåˆ¤åˆ«ç½‘ç»œç»„æˆã€‚ç”Ÿæˆç½‘ç»œä»æ½œåœ¨ç©ºé—´ï¼ˆlatent
spaceï¼‰ä¸­éšæœºé‡‡æ ·ä½œä¸ºè¾“å…¥ï¼Œå…¶è¾“å‡ºç»“æœéœ€è¦å°½é‡æ¨¡ä»¿è®­ç»ƒé›†ä¸­çš„çœŸå®æ ·æœ¬ã€‚åˆ¤åˆ«ç½‘ç»œçš„è¾“å…¥ä¸ºçœŸå®æ ·æœ¬æˆ–ç”Ÿæˆç½‘ç»œçš„è¾“å‡ºï¼Œå…¶ç›®çš„æ˜¯å°†å°½å¯èƒ½çš„åˆ†è¾¨è¾“å…¥ä¸ºçœŸå®æ ·æœ¬æˆ–ç”Ÿæˆç½‘ç»œçš„è¾“å‡ºã€‚è€Œç”Ÿæˆç½‘ç»œåˆ™è¦å°½å¯èƒ½åœ°æ¬ºéª—åˆ¤åˆ«ç½‘ç»œã€‚ä¸¤ä¸ªç½‘ç»œç›¸äº’å¯¹æŠ—ã€ä¸æ–­è°ƒæ•´å‚æ•°ã€‚

è®©\ :math:`x`\ æ˜¯ä»£è¡¨å›¾åƒçš„æ•°æ®ã€‚\ :math:`D(x)`\ æ˜¯åˆ¤åˆ«å™¨ç½‘ç»œï¼Œè¾“å‡ºçš„æ¦‚ç‡ä¸º\ :math:`x`\ æ¥è‡ªè®­ç»ƒæ•°æ®è¿˜æ˜¯ç”Ÿæˆå™¨ã€‚å‡è®¾\ :math:`x`\ ä¸ºCHWæ ¼å¼ï¼Œå¤§å°ä¸º3x64x64çš„å›¾åƒæ•°æ®ï¼ŒDä¸ºåˆ¤åˆ«å™¨ç½‘ç»œï¼Œ\ :math:`D(x)`\ ä¸º\ :math:`ğ‘¥`\ æ¥è‡ªè®­ç»ƒæ•°æ®è¿˜æ˜¯ç”Ÿæˆå™¨ã€‚å½“\ :math:`ğ‘¥`\ æ¥è‡ªè®­ç»ƒæ•°æ®æ—¶\ :math:`ğ·(ğ‘¥)`\ å°½é‡æ¥è¿‘1ï¼Œ\ :math:`ğ‘¥`\ æ¥è‡ªç”Ÿæˆå™¨æ—¶\ :math:`ğ·(ğ‘¥)`\ å°½é‡æ¥è¿‘0ã€‚
å› æ­¤ï¼Œ\ :math:`ğ·(ğ‘¥)`\ ä¹Ÿå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¼ ç»Ÿçš„äºŒåˆ†ç±»å™¨ã€‚

å¯¹äºç”Ÿæˆå™¨ç½‘ç»œï¼Œ
å‡è®¾\ :math:`z`\ ä¸ºä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„ç©ºé—´å‘é‡ã€‚\ :math:`G(z)`\ è¡¨ç¤ºç”Ÿæˆå™¨ç½‘ç»œï¼Œè¯¥ç½‘ç»œå°†çŸ¢é‡\ :math:`z`\ æ˜ å°„åˆ°æ•°æ®ç©ºé—´ï¼Œ\ :math:`G(z)`\ è¡¨ç¤ºç”Ÿæˆå™¨ç½‘ç»œè¾“å‡ºçš„å›¾åƒã€‚ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯æ‹Ÿåˆè®­ç»ƒæ•°æ®(:math:`p_{data}`)çš„åˆ†å¸ƒï¼Œä»¥ä¾¿å¯ä»¥ä»è¯¥ä¼°è®¡åˆ†å¸ƒä¸­ç”Ÿæˆå‡æ ·æœ¬(:math:`p_g`)ã€‚

æ‰€ä»¥ï¼Œ\ :math:`D(G(z))`\ æ˜¯ç”Ÿæˆå™¨\ :math:`G`\ è¾“å‡ºæ˜¯çœŸå®çš„å›¾åƒçš„æ¦‚ç‡ã€‚å¦‚Goodfellowçš„è®ºæ–‡æ‰€è¿°ï¼Œ\ :math:`D`\ å’Œ\ :math:`G`\ ç©ä¸€ä¸ªminmaxæ¸¸æˆï¼Œå…¶ä¸­\ :math:`D`\ å°è¯•æœ€å¤§åŒ–å…¶æ­£ç¡®åˆ†ç±»çœŸå‡çš„å¯èƒ½æ€§\ :math:`logD(x)`\ ï¼Œä»¥åŠ\ :math:`G`\ è¯•å›¾æœ€å°åŒ–ä»¥ä¸‹å¯èƒ½æ€§\ :math:`D`\ ä¼šé¢„æµ‹å…¶è¾“å‡ºæ˜¯å‡çš„\ :math:`log(1-D(G(x)))`\ ã€‚

GANçš„æŸå¤±å‡½æ•°å¯è¡¨ç¤ºä¸ºï¼š

   :math:`\underset{G}{\text{min}} \underset{D}{\text{max}}V(D,G) = \mathbb{E}_{x\sim p_{data}(x)}\big[logD(x)\big] + \mathbb{E}_{z\sim p_{z}(z)}\big[log(1-D(G(z)))\big]`

| ä»ç†è®ºä¸Šè®²ï¼Œæ­¤minmaxæ¸¸æˆçš„è§£å†³æ–¹æ¡ˆæ˜¯\ :math:`p_g = p_{data}`\ ï¼Œé‰´åˆ«è€…ä¼šç›²ç›®çŒœæµ‹è¾“å…¥æ˜¯çœŸå®çš„è¿˜æ˜¯å‡çš„ã€‚ä½†æ˜¯ï¼ŒGANçš„æ”¶æ•›ç†è®ºä»åœ¨ç§¯æç ”ç©¶ä¸­ï¼Œå®é™…ä¸ŠGANå¸¸å¸¸ä¼šé‡åˆ°æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ã€‚
| ç”Ÿæˆå¯¹æŠ—ç½‘ç»œå¸¸ç”¨äºç”Ÿæˆä»¥å‡ä¹±çœŸçš„å›¾ç‰‡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•è¿˜è¢«ç”¨äºç”Ÿæˆè§†é¢‘ã€ä¸‰ç»´ç‰©ä½“æ¨¡å‹ç­‰ã€‚

1.2 ä»€ä¹ˆæ˜¯DCGANï¼Ÿ
~~~~~~~~~~~~~~~~~

DCGANæ˜¯æ·±å±‚å·ç§¯ç½‘ç»œä¸GANçš„ç»“åˆï¼Œå…¶åŸºæœ¬åŸç†ä¸GANç›¸åŒï¼Œåªæ˜¯å°†ç”Ÿæˆç½‘ç»œå’Œåˆ¤åˆ«ç½‘ç»œç”¨ä¸¤ä¸ªå·ç§¯ç½‘ç»œï¼ˆCNNï¼‰æ›¿ä»£ã€‚ä¸ºäº†æé«˜ç”Ÿæˆæ ·æœ¬çš„è´¨é‡å’Œç½‘ç»œçš„æ”¶æ•›é€Ÿåº¦ï¼Œè®ºæ–‡ä¸­çš„
DCGAN åœ¨ç½‘ç»œç»“æ„ä¸Šè¿›è¡Œäº†ä¸€äº›æ”¹è¿›ï¼š

-  å–æ¶ˆ pooling å±‚ï¼šåœ¨ç½‘ç»œä¸­ï¼Œæ‰€æœ‰çš„poolingå±‚ä½¿ç”¨æ­¥å¹…å·ç§¯ï¼ˆstrided
   convolutionsï¼‰(åˆ¤åˆ«å™¨)å’Œå¾®æ­¥å¹…åº¦å·ç§¯ï¼ˆfractional-strided
   convolutionsï¼‰(ç”Ÿæˆå™¨)è¿›è¡Œæ›¿æ¢ã€‚
-  åŠ å…¥batchnormï¼šåœ¨ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä¸­å‡åŠ å…¥batchnormã€‚
-  ä½¿ç”¨å…¨å·ç§¯ç½‘ç»œï¼šå»æ‰äº†FCå±‚ï¼Œä»¥å®ç°æ›´æ·±çš„ç½‘ç»œç»“æ„ã€‚
-  æ¿€æ´»å‡½æ•°ï¼šåœ¨ç”Ÿæˆå™¨ï¼ˆGï¼‰ä¸­ï¼Œæœ€åä¸€å±‚ä½¿ç”¨Tanhå‡½æ•°ï¼Œå…¶ä½™å±‚é‡‡ç”¨ReLUå‡½æ•° ;
   åˆ¤åˆ«å™¨ï¼ˆDï¼‰ä¸­éƒ½é‡‡ç”¨LeakyReLUã€‚

2 ç¯å¢ƒè®¾ç½®åŠæ•°æ®é›†
------------------

ç¯å¢ƒï¼špaddlepaddleã€scikit-imageã€numpyã€matplotlib

| åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨\ `Celeb-A
  Faces <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`__\ æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†å¯ä»¥åœ¨é“¾æ¥çš„ç½‘ç«™æˆ–\ `AI
  Studio <https://aistudio.baidu.com/aistudio/datasetdetail/39207>`__\ ä¸­ä¸‹è½½ã€‚æ•°æ®é›†å°†ä¸‹è½½ä¸ºåä¸ºimg_align_celeba.zipçš„æ–‡ä»¶ã€‚ä¸‹è½½åï¼Œå¹¶å°†zipæ–‡ä»¶è§£å‹ç¼©åˆ°è¯¥ç›®å½•ä¸­ã€‚
| img_align_celebaç›®å½•ç»“æ„åº”ä¸ºï¼š

::

   /path/to/project  
       -> img_align_celeba  
           -> 188242.jpg  
           -> 173822.jpg  
           -> 284702.jpg  
           -> 537394.jpg  
           ...

2.1 æ•°æ®é›†é¢„å¤„ç†
~~~~~~~~~~~~~~~~

å¤šçº¿ç¨‹å¤„ç†ï¼Œä»¥è£åˆ‡åæ ‡(0,10)å’Œ(64,74)ï¼Œå°†è¾“å…¥ç½‘ç»œçš„å›¾ç‰‡è£åˆ‡åˆ°64*64ã€‚

.. code:: ipython3

    from PIL import Image
    import os.path
    import os
    import threading
    from PIL import ImageFile
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    
    '''å¤šçº¿ç¨‹å°†å›¾ç‰‡ç¼©æ”¾åå†è£åˆ‡åˆ°64*64åˆ†è¾¨ç‡'''
    #è£åˆ‡å›¾ç‰‡å®½åº¦
    w = 64
    #è£åˆ‡å›¾ç‰‡é«˜åº¦
    h = 64
    #è£åˆ‡ç‚¹æ¨ªåæ ‡(ä»¥å›¾ç‰‡å·¦ä¸Šè§’ä¸ºåŸç‚¹)
    x = 0
    #è£åˆ‡ç‚¹çºµåæ ‡
    y = 20
    
    def cutArray(l, num):
      avg = len(l) / float(num)
      o = []
      last = 0.0
    
      while last < len(l):
        o.append(l[int(last):int(last + avg)])
        last += avg
    
      return o
      
    def convertjpg(jpgfile,outdir,width=w,height=h):
        img=Image.open(jpgfile)
        (l,h) = img.size
        rate = min(l,h) / width
        try:
            img = img.resize((int(l // rate),int(h // rate)),Image.BILINEAR)
            img = img.crop((x,y,width+x,height+y))
            img.save(os.path.join(outdir,os.path.basename(jpgfile)))
        except Exception as e:
            print(e)
    
    class thread(threading.Thread):
        def __init__(self, threadID, inpath, outpath, files):
            threading.Thread.__init__(self)
            self.threadID = threadID
            self.inpath = inpath
            self.outpath = outpath
            self.files = files
        def run(self):
            count = 0
            try:
                for file in self.files:
                    convertjpg(self.inpath + file,self.outpath)
                    count = count + 1
            except Exception as e:
                print(e)
            print('å·²å¤„ç†å›¾ç‰‡æ•°é‡ï¼š' +  str(count))
                
    if __name__ == '__main__':
        inpath = './work/img_align_celeba/'
        outpath = './work/imgs/'
        if not os.path.exists(outpath):
            os.mkdir(outpath)
        files =  os.listdir(inpath)
        files = cutArray(files,8)
        T1 = thread(1, inpath, outpath, files[0])
        T2 = thread(2, inpath, outpath, files[1])
        T3 = thread(3, inpath, outpath, files[2])
        T4 = thread(4, inpath, outpath, files[3])
        T5 = thread(5, inpath, outpath, files[4])
        T6 = thread(6, inpath, outpath, files[5])
        T7 = thread(7, inpath, outpath, files[6])
        T8 = thread(8, inpath, outpath, files[7])
        
        T1.start()
        T2.start()
        T3.start()
        T4.start()
        T5.start()
        T6.start()
        T7.start()
        T8.start()
        
        T1.join()
        T2.join()
        T3.join()
        T4.join()
        T5.join()
        T6.join()
        T7.join()
        T8.join()


3 æ¨¡å‹ç»„ç½‘
----------

3.1 å®šä¹‰æ•°æ®é¢„å¤„ç†å·¥å…·-Paddle.io.Dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

å…·ä½“å‚è€ƒ\ `Paddle.io.Datasetæ•™ç¨‹ <https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc/api/paddle/io/Dataset_cn.html#dataset>`__

.. code:: ipython3

    import os
    import cv2
    import numpy as np
    from skimage import io,color,transform
    import matplotlib.pyplot as plt
    import math
    import time
    import paddle
    from paddle.io import Dataset
    import six
    from PIL import Image as PilImage
    from paddle.static import InputSpec
    paddle.enable_static()
    img_dim = 64
    
    '''å‡†å¤‡æ•°æ®ï¼Œå®šä¹‰Reader()'''
    PATH = 'work/imgs/'
    
    class DataGenerater(Dataset):
        """
        æ•°æ®é›†å®šä¹‰
        """
        def __init__(self,path=PATH):
            """
            æ„é€ å‡½æ•°
            """
            super(DataGenerater, self).__init__()
            self.dir = path
            self.datalist = os.listdir(PATH)
            self.image_size = (img_dim,img_dim)
        
        # æ¯æ¬¡è¿­ä»£æ—¶è¿”å›æ•°æ®å’Œå¯¹åº”çš„æ ‡ç­¾
        def __getitem__(self, idx):
            return self._load_img(self.dir + self.datalist[idx])
    
        # è¿”å›æ•´ä¸ªæ•°æ®é›†çš„æ€»æ•°
        def __len__(self):
            return len(self.datalist)
        
        def _load_img(self, path):
            """
            ç»Ÿä¸€çš„å›¾åƒå¤„ç†æ¥å£å°è£…ï¼Œç”¨äºè§„æ•´å›¾åƒå¤§å°å’Œé€šé“
            """
            try:
                img = io.imread(path)
                img = transform.resize(img,self.image_size)
                img = img.transpose()
                img = img.astype('float32')
            except Exception as e:
                    print(e)
            return img

3.2 æµ‹è¯•Paddle.io.DataLoaderå¹¶è¾“å‡ºå›¾ç‰‡
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    
    train_dataset = DataGenerater()
    img = paddle.static.data(name='img', shape=[None,3,img_dim,img_dim], dtype='float32')
    train_loader = paddle.io.DataLoader(
        train_dataset, 
        places=paddle.CPUPlace(), 
        feed_list = [img],
        batch_size=128, 
        shuffle=True,
        num_workers=2,
        use_buffer_reader=True,
        use_shared_memory=False,
        drop_last=True,
        )
    
    for batch_id, data in enumerate(train_loader()):
        plt.figure(figsize=(15,15))
        try:
            for i in range(100):
                image = np.array(data[0]['img'][i])[0].transpose((2,1,0))
                plt.subplot(10, 10, i + 1)
                plt.imshow(image, vmin=-1, vmax=1)
                plt.axis('off')
                plt.xticks([])
                plt.yticks([])
                plt.subplots_adjust(wspace=0.1, hspace=0.1)
            plt.suptitle('\n Training Images',fontsize=30)
            plt.show()
            break
        except IOError:
            print(IOError)


3.3 æƒé‡åˆå§‹åŒ–
~~~~~~~~~~~~~~

| åœ¨ DCGAN
  è®ºæ–‡ä¸­ï¼Œä½œè€…æŒ‡å®šæ‰€æœ‰æ¨¡å‹æƒé‡åº”ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.02çš„æ­£æ€åˆ†å¸ƒä¸­éšæœºåˆå§‹åŒ–ã€‚
| è°ƒç”¨paddle.nn.initializer.Normalå®ç°initializeè®¾ç½®

.. code:: ipython3

    conv_initializer=paddle.nn.initializer.Normal(mean=0.0, std=0.02)
    bn_initializer=paddle.nn.initializer.Normal(mean=1.0, std=0.02)

3.4 åˆ¤åˆ«å™¨
~~~~~~~~~~

å¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œç”Ÿæˆå™¨\ :math:`D`\ æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶åˆ†ç±»ç½‘ç»œï¼Œå®ƒä»¥å›¾åƒä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºå›¾åƒæ˜¯çœŸå®çš„ï¼ˆç›¸å¯¹åº”\ :math:`G`\ ç”Ÿæˆçš„å‡æ ·æœ¬ï¼‰çš„æ¦‚ç‡ã€‚è¾“å…¥\ :math:`Shape`\ ä¸º[3,64,64]çš„RGBå›¾åƒï¼Œé€šè¿‡ä¸€ç³»åˆ—çš„\ :math:`Conv2d`\ ï¼Œ\ :math:`BatchNorm2d`\ å’Œ\ :math:`LeakyReLU`\ å±‚å¯¹å…¶è¿›è¡Œå¤„ç†ï¼Œç„¶åé€šè¿‡å…¨è¿æ¥å±‚è¾“å‡ºçš„ç¥ç»å…ƒä¸ªæ•°ä¸º2ï¼Œå¯¹åº”ä¸¤ä¸ªæ ‡ç­¾çš„é¢„æµ‹æ¦‚ç‡ã€‚

-  å°†BatchNormæ‰¹å½’ä¸€åŒ–ä¸­momentumå‚æ•°è®¾ç½®ä¸º0.5
-  å°†åˆ¤åˆ«å™¨(D)æ¿€æ´»å‡½æ•°leaky_reluçš„alphaå‚æ•°è®¾ç½®ä¸º0.2

..

   | è¾“å…¥: ä¸ºå¤§å°64x64çš„RGBä¸‰é€šé“å›¾ç‰‡
   | è¾“å‡º: ç»è¿‡ä¸€å±‚å…¨è¿æ¥å±‚æœ€åä¸ºshapeä¸º[batch_size,2]çš„Tensor

.. code:: ipython3

    import paddle
    import paddle.nn as nn
    import paddle.nn.functional as F
    
    class Discriminator(paddle.nn.Layer):
        def __init__(self):
            super(Discriminator, self).__init__()
            self.conv_1 = nn.Conv2D(
                3,64,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="d_conv_weight_1_",initializer=conv_initializer)
                )
            self.conv_2 = nn.Conv2D(
                64,128,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="d_conv_weight_2_",initializer=conv_initializer)
                )
            self.bn_2 = nn.BatchNorm2D(
                128,
                weight_attr=paddle.ParamAttr(name="d_2_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_3 = nn.Conv2D(
                128,256,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="d_conv_weight_3_",initializer=conv_initializer)
                )
            self.bn_3 = nn.BatchNorm2D(
                256,
                weight_attr=paddle.ParamAttr(name="d_3_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_4 = nn.Conv2D(
                256,512,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="d_conv_weight_4_",initializer=conv_initializer)
                )
            self.bn_4 = nn.BatchNorm2D(
                512,
                weight_attr=paddle.ParamAttr(name="d_4_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_5 = nn.Conv2D(
                512,1,4,1,0,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="d_conv_weight_5_",initializer=conv_initializer)
                )
        
        def forward(self, x):
            x = self.conv_1(x)
            x = F.leaky_relu(x,negative_slope=0.2)
            x = self.conv_2(x)
            x = self.bn_2(x)
            x = F.leaky_relu(x,negative_slope=0.2)
            x = self.conv_3(x)
            x = self.bn_3(x)
            x = F.leaky_relu(x,negative_slope=0.2)
            x = self.conv_4(x)
            x = self.bn_4(x)
            x = F.leaky_relu(x,negative_slope=0.2)
            x = self.conv_5(x)
            x = F.sigmoid(x)
            return x

3.5 ç”Ÿæˆå™¨
~~~~~~~~~~

ç”Ÿæˆå™¨\ :math:`G`\ æ—¨åœ¨æ˜ å°„æ½œåœ¨ç©ºé—´çŸ¢é‡\ :math:`z`\ åˆ°æ•°æ®ç©ºé—´ã€‚ç”±äºæˆ‘ä»¬çš„æ•°æ®æ˜¯å›¾åƒï¼Œå› æ­¤è½¬æ¢\ :math:`z`\ åˆ°æ•°æ®ç©ºé—´æ„å‘³ç€æœ€ç»ˆåˆ›å»ºå…·æœ‰ä¸è®­ç»ƒå›¾åƒç›¸åŒå¤§å°[3,64,64]çš„RGBå›¾åƒã€‚åœ¨ç½‘ç»œè®¾è®¡ä¸­ï¼Œè¿™æ˜¯é€šè¿‡ä¸€ç³»åˆ—äºŒç»´å·ç§¯è½¬ç½®å±‚æ¥å®Œæˆçš„ï¼Œæ¯ä¸ªå±‚éƒ½ä¸\ :math:`BatchNorm`\ å±‚å’Œ\ :math:`ReLu`\ æ¿€æ´»å‡½æ•°ã€‚ç”Ÿæˆå™¨çš„è¾“å‡ºé€šè¿‡\ :math:`tanh`\ å‡½æ•°è¾“å‡ºï¼Œä»¥ä½¿å…¶è¿”å›åˆ°è¾“å…¥æ•°æ®èŒƒå›´[âˆ’1,1]ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å·ç§¯è½¬ç½®å±‚ä¹‹åå­˜åœ¨\ :math:`BatchNorm`\ å‡½æ•°ï¼Œå› ä¸ºè¿™æ˜¯DCGANè®ºæ–‡çš„å…³é”®æ”¹è¿›ã€‚è¿™äº›å±‚æœ‰åŠ©äºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦æ›´å¥½åœ°æµåŠ¨ã€‚

| **ç”Ÿæˆå™¨ç½‘ç»œç»“æ„**
| |models|

-  å°†\ :math:`BatchNorm`\ æ‰¹å½’ä¸€åŒ–ä¸­\ :math:`momentum`\ å‚æ•°è®¾ç½®ä¸º0.5

..

   | è¾“å…¥:Tensorçš„Shapeä¸º[batch_size,100]å…¶ä¸­æ¯ä¸ªæ•°å€¼å¤§å°ä¸º0~1ä¹‹é—´çš„float32éšæœºæ•°
   | è¾“å‡º:3x64x64RGBä¸‰é€šé“å›¾ç‰‡

.. |models| image:: ./images/models.png

.. code:: ipython3

    
    class Generator(paddle.nn.Layer):
        def __init__(self):
            super(Generator, self).__init__()
            self.conv_1 = nn.Conv2DTranspose(
                100,512,4,1,0,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="g_dconv_weight_1_",initializer=conv_initializer)
                )
            self.bn_1 = nn.BatchNorm2D(
                512,
                weight_attr=paddle.ParamAttr(name="g_1_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_2 = nn.Conv2DTranspose(
                512,256,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="g_dconv_weight_2_",initializer=conv_initializer)
                )
            self.bn_2 = nn.BatchNorm2D(
                256,
                weight_attr=paddle.ParamAttr(name="g_2_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_3 = nn.Conv2DTranspose(
                256,128,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="g_dconv_weight_3_",initializer=conv_initializer)
                )
            self.bn_3 = nn.BatchNorm2D(
                128,
                weight_attr=paddle.ParamAttr(name="g_3_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_4 = nn.Conv2DTranspose(
                128,64,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="g_dconv_weight_4_",initializer=conv_initializer)
                )
            self.bn_4 = nn.BatchNorm2D(
                64,
                weight_attr=paddle.ParamAttr(name="g_4_bn_weight_",initializer=bn_initializer),momentum=0.8
                )
            self.conv_5 = nn.Conv2DTranspose(
                64,3,4,2,1,
                bias_attr=False,weight_attr=paddle.ParamAttr(name="g_dconv_weight_5_",initializer=conv_initializer)
                )
            self.tanh = paddle.nn.Tanh()
        
        def forward(self, x):
            x = self.conv_1(x)
            x = self.bn_1(x)
            x = F.relu(x)
            x = self.conv_2(x)
            x = self.bn_2(x)
            x = F.relu(x)
            x = self.conv_3(x)
            x = self.bn_3(x)
            x = F.relu(x)
            x = self.conv_4(x)
            x = self.bn_4(x)
            x = F.relu(x)
            x = self.conv_5(x)
            x = self.tanh(x)
            return x


3.6 æŸå¤±å‡½æ•°
~~~~~~~~~~~~

é€‰ç”¨BCELoss,å…¬å¼å¦‚ä¸‹:

:math:`Out = -1 * (label * log(input) + (1 - label) * log(1 - input))`

.. code:: ipython3

    ###æŸå¤±å‡½æ•°
    loss = paddle.nn.BCELoss()

4 æ¨¡å‹è®­ç»ƒ
----------

è®¾ç½®çš„è¶…å‚æ•°ä¸ºï¼š \* å­¦ä¹ ç‡ï¼š0.0002 \* è¾“å…¥å›¾ç‰‡é•¿å’Œå®½ï¼š64 \* Epoch: 8 \*
Mini-Batchï¼š128 \* è¾“å…¥Tensoré•¿åº¦ï¼š100 \* Adamï¼šBeta1ï¼š0.5ï¼ŒBeta2ï¼š0.999

è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¯ä¸€æ¬¡è¿­ä»£ï¼Œç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨åˆ†åˆ«è®¾ç½®è‡ªå·±çš„è¿­ä»£æ¬¡æ•°ã€‚ä¸ºäº†é¿å…åˆ¤åˆ«å™¨å¿«é€Ÿæ”¶æ•›åˆ°0ï¼Œæœ¬æ•™ç¨‹é»˜è®¤æ¯è¿­ä»£ä¸€æ¬¡ï¼Œè®­ç»ƒä¸€æ¬¡åˆ¤åˆ«å™¨ï¼Œä¸¤æ¬¡ç”Ÿæˆå™¨ã€‚

.. code:: ipython3

    import IPython.display as display
    import warnings
    import paddle.optimizer as optim
    warnings.filterwarnings('ignore')
    
    img_dim = 64
    lr = 0.0002
    epoch = 5
    output = "work/Output/"
    batch_size = 128
    G_DIMENSION = 100
    beta1=0.5
    beta2=0.999
    output_path = 'work/Output'
    device = paddle.set_device('gpu')
    paddle.disable_static(device)
    
    real_label = 1.
    fake_label = 0.
    
    netD = Discriminator()
    netG = Generator()
    optimizerD = optim.Adam(parameters=netD.parameters(), learning_rate=lr, beta1=beta1, beta2=beta2)
    optimizerG = optim.Adam(parameters=netG.parameters(), learning_rate=lr, beta1=beta1, beta2=beta2)
    
    ###è®­ç»ƒè¿‡ç¨‹
    losses = [[], []]
    #plt.ion()
    now = 0
    for pass_id in range(epoch):
        
        # enumerate()å‡½æ•°å°†ä¸€ä¸ªå¯éå†çš„æ•°æ®å¯¹è±¡ç»„åˆæˆä¸€ä¸ªåºåˆ—åˆ—è¡¨
        for batch_id, data in enumerate(train_loader()):
            #è®­ç»ƒåˆ¤åˆ«å™¨ 
            optimizerD.clear_grad()
            real_cpu = data[0]
            label = paddle.full((batch_size,1,1,1),real_label,dtype='float32')
            output = netD(real_cpu)
            errD_real = loss(output,label)
            errD_real.backward()
            optimizerD.step()
            optimizerD.clear_grad()
    
            noise = paddle.randn([batch_size,G_DIMENSION,1,1],'float32')
            fake = netG(noise)
            label = paddle.full((batch_size,1,1,1),fake_label,dtype='float32')
            output = netD(fake.detach())
            errD_fake = loss(output,label)
            errD_fake.backward()
            optimizerD.step()
            optimizerD.clear_grad()
    
            errD = errD_real + errD_fake
            
            losses[0].append(errD.numpy()[0])
            ###è®­ç»ƒç”Ÿæˆå™¨
            optimizerG.clear_grad()
            noise = paddle.randn([batch_size,G_DIMENSION,1,1],'float32')
            fake = netG(noise)
            label = paddle.full((batch_size,1,1,1),real_label,dtype=np.float32,)
            output = netD(fake)
            errG = loss(output,label)
            errG.backward()
            optimizerG.step()
            optimizerG.clear_grad()
            
            losses[1].append(errG.numpy()[0])
            if batch_id % 100 == 0:
                if not os.path.exists(output_path):
                    os.makedirs(output_path)
                # æ¯è½®çš„ç”Ÿæˆç»“æœ
                generated_image = netG(noise).numpy()
                imgs = []
                plt.figure(figsize=(15,15))
                try:
                    for i in range(100):
                        image = generated_image[i].transpose()
                        image = np.where(image > 0, image, 0)
                        plt.subplot(10, 10, i + 1)
                        plt.imshow(image, vmin=-1, vmax=1)
                        plt.axis('off')
                        plt.xticks([])
                        plt.yticks([])
                        plt.subplots_adjust(wspace=0.1, hspace=0.1)
                    msg = 'Epoch ID={0} Batch ID={1} \n\n D-Loss={2} G-Loss={3}'.format(pass_id, batch_id, errD.numpy()[0], errG.numpy()[0])
                    plt.suptitle(msg,fontsize=20)
                    plt.draw()
                    plt.savefig('{}/{:04d}_{:04d}.png'.format(output_path, pass_id, batch_id),bbox_inches='tight')
                    plt.pause(0.01)
                    display.clear_output(wait=True)
                except IOError:
                    print(IOError)
        paddle.save(netG.state_dict(), "work/generator.params")
    
    plt.close()


.. code:: ipython3

    plt.figure(figsize=(15, 6))
    x = np.arange(len(losses[0]))
    plt.title('Generator and Discriminator Loss During Training')
    plt.xlabel('Number of Batch')
    plt.plot(x,np.array(losses[0]),label='D Loss')
    plt.plot(x,np.array(losses[1]),label='G Loss')
    plt.legend()
    plt.savefig('work/Generator and Discriminator Loss During Training.png')
    plt.show()

5 æ¨¡å‹è¯„ä¼°
----------

ç”Ÿæˆå™¨\ :math:`G`\ å’Œåˆ¤åˆ«å™¨\ :math:`D`\ çš„æŸå¤±ä¸è¿­ä»£å˜åŒ–å›¾
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. figure:: ./images/loss.png
   :alt: loss

   loss

å¯¹æ¯”çœŸå®äººè„¸å›¾åƒï¼ˆå›¾ä¸€ï¼‰å’Œç”Ÿæˆäººè„¸å›¾åƒï¼ˆå›¾äºŒï¼‰
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

å›¾ä¸€
^^^^

|face_image1| ### å›¾äºŒ |face_image2|

.. |face_image1| image:: ./images/face_image1.jpeg
.. |face_image2| image:: ./images/face_image2.jpeg

6 æ¨¡å‹é¢„æµ‹
----------

è¾“å…¥éšæœºæ•°è®©ç”Ÿæˆå™¨\ :math:`G`\ ç”Ÿæˆéšæœºäººè„¸
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

ç”Ÿæˆçš„RGBä¸‰é€šé“64*64çš„å›¾ç‰‡è·¯å¾„ä½äºâ€œworl/Generate/â€

.. code:: ipython3

    device = paddle.set_device('gpu')
    paddle.disable_static(device)
    try:
        generate = Generator()
        state_dict = paddle.load("work/generator.params")
        generate.set_state_dict(state_dict)
        noise = paddle.randn([100,100,1,1],'float32')
        generated_image = generate(noise).numpy()
        for j in range(100):
            image = generated_image[j].transpose()
            plt.figure(figsize=(4,4))
            plt.imshow(image)
            plt.axis('off')
            plt.xticks([])
            plt.yticks([])
            plt.subplots_adjust(wspace=0.1, hspace=0.1)
            plt.savefig('work/Generate/generated_' + str(j + 1), bbox_inches='tight')
            plt.close()
    except IOError:
        print(IOError)

7 é¡¹ç›®æ€»ç»“
----------

ç®€å•ä»‹ç»äº†ä¸€ä¸‹DCGANçš„åŸç†ï¼Œé€šè¿‡å¯¹åŸé¡¹ç›®çš„æ”¹è¿›å’Œä¼˜åŒ–ï¼Œä¸€æ­¥ä¸€æ­¥ä¾æ¬¡å¯¹ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ä»¥åŠè®­ç»ƒè¿‡ç¨‹è¿›è¡Œä»‹ç»ã€‚
DCGANé‡‡ç”¨ä¸€ä¸ªéšæœºå™ªå£°å‘é‡ä½œä¸ºè¾“å…¥ï¼Œè¾“å…¥é€šè¿‡ä¸CNNç±»ä¼¼ä½†æ˜¯ç›¸åçš„ç»“æ„ï¼Œå°†è¾“å…¥æ”¾å¤§æˆäºŒç»´æ•°æ®ã€‚é‡‡ç”¨è¿™ç§ç»“æ„çš„ç”Ÿæˆæ¨¡å‹å’ŒCNNç»“æ„çš„åˆ¤åˆ«æ¨¡å‹ï¼ŒDCGANåœ¨å›¾ç‰‡ç”Ÿæˆä¸Šå¯ä»¥è¾¾åˆ°ç›¸å½“å¯è§‚çš„æ•ˆæœã€‚æœ¬æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨DCGANç”Ÿæˆäº†äººè„¸ç…§ç‰‡ï¼Œæ‚¨å¯ä»¥å°è¯•æ›´æ¢æ•°æ®é›†ç”Ÿæˆç¬¦åˆä¸ªäººéœ€æ±‚çš„å›¾ç‰‡ï¼Œæˆ–å°è¯•ä¿®æ”¹ç½‘ç»œç»“æ„è§‚å¯Ÿä¸ä¸€æ ·çš„ç”Ÿæˆæ•ˆæœã€‚

8 å‚è€ƒæ–‡çŒ®
----------

[1] Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing;
Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua.
Generative Adversarial Networks. 2014. arXiv:1406.2661 [stat.ML].

[2] Andrej Karpathy, Pieter Abbeel, Greg Brockman, Peter Chen, Vicki
Cheung, Rocky Duan, Ian Goodfellow, Durk Kingma, Jonathan Ho, Rein
Houthooft, Tim Salimans, John Schulman, Ilya Sutskever, And Wojciech
Zaremba, Generative Models, OpenAI, [April 7, 2016]

[3] alimans, Tim; Goodfellow, Ian; Zaremba, Wojciech; Cheung, Vicki;
Radford, Alec; Chen, Xi. Improved Techniques for Training GANs. 2016.
arXiv:1606.03498 [cs.LG].

[4] Radford A, Metz L, Chintala S. Unsupervised Representation Learning
with Deep Convolutional Generative Adversarial Networks[J]. Computer
Science, 2015.

[5]Kingma D , Ba J . Adam: A Method for Stochastic Optimization[J].
Computer ence, 2014.
